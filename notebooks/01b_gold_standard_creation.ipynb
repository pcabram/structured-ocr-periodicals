{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Standard Preparation - Stage 1\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "\n",
      "Directories:\n",
      "  Predictions: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n",
      "  Gold raw:    /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/raw\n",
      "  Gold clean:  /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "  Schema:      Stage1PageModel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gold Standard Preparation - Stage 1 OCR\n",
    "\n",
    "Prepares prediction files for manual annotation and validates corrected gold standard.\n",
    "\n",
    "Input:  Predictions from data/predictions/{magazine_name}/\n",
    "Output: Gold standard in data/gold_standard/{raw|cleaned}/{magazine_name}/\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "from typing import Dict, List\n",
    "from collections import Counter\n",
    "\n",
    "# Project root detection\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "print(\"Gold Standard Preparation - Stage 1\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Add schemas to path\n",
    "SCHEMAS_DIR = PROJECT_ROOT / \"schemas\"\n",
    "if str(SCHEMAS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCHEMAS_DIR))\n",
    "\n",
    "# Import schema\n",
    "from stage1_page import Stage1PageModel\n",
    "\n",
    "# Directory structure\n",
    "PRED_ROOT = PROJECT_ROOT / \"data\" / \"predictions\"\n",
    "GOLD_ROOT = PROJECT_ROOT / \"data\" / \"gold_standard\"\n",
    "GOLD_RAW = GOLD_ROOT / \"raw\"\n",
    "GOLD_CLEAN = GOLD_ROOT / \"cleaned\"\n",
    "\n",
    "# Create directories\n",
    "for directory in (GOLD_ROOT, GOLD_RAW, GOLD_CLEAN):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Predictions: {PRED_ROOT}\")\n",
    "print(f\"  Gold raw:    {GOLD_RAW}\")\n",
    "print(f\"  Gold clean:  {GOLD_CLEAN}\")\n",
    "print(f\"  Schema:      {Stage1PageModel.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files for annotation...\n",
      "\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-007.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-004.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-014.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-010.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-011.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-013.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-003.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-005.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-008.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-006.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-002.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-012.json\n",
      "Copied: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-009.json\n",
      "\n",
      "==================================================\n",
      "Copy Summary\n",
      "==================================================\n",
      "Total files found: 14\n",
      "Copied: 14\n",
      "Skipped (already exist): 0\n",
      "Tip: If you need to re-copy, set overwrite=True in the function call above.\n"
     ]
    }
   ],
   "source": [
    "def copy_for_annotation(source_dir: Path, dest_dir: Path, overwrite: bool = False) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Copy JSON files from source to destination for annotation.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Directory with raw extractions\n",
    "        dest_dir: Gold standard directory\n",
    "        overwrite: If True, overwrite existing files (use with caution!)\n",
    "    \n",
    "    Returns:\n",
    "        Stats dict with copied/skipped/total counts\n",
    "    \"\"\"\n",
    "    if not source_dir.exists():\n",
    "        print(f\"Source directory not found: {source_dir}\")\n",
    "        return {\"error\": \"source_not_found\"}\n",
    "    \n",
    "    json_files = list(source_dir.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {source_dir}\")\n",
    "        return {\"total\": 0, \"copied\": 0, \"skipped\": 0}\n",
    "    \n",
    "    stats = {\"total\": len(json_files), \"copied\": 0, \"skipped\": 0}\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        dest_file = dest_dir / json_file.name\n",
    "        \n",
    "        if dest_file.exists() and not overwrite:\n",
    "            stats[\"skipped\"] += 1\n",
    "            print(f\"Skipped (already exists): {json_file.name}\")\n",
    "        else:\n",
    "            shutil.copy2(json_file, dest_file)\n",
    "            stats[\"copied\"] += 1\n",
    "            print(f\"Copied: {json_file.name}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run the copy\n",
    "print(\"Copying files for annotation...\\n\")\n",
    "copy_stats = copy_for_annotation(RAW_DIR, GOLD_DIR_RAW, overwrite=False)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Copy Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total files found: {copy_stats.get('total', 0)}\")\n",
    "print(f\"Copied: {copy_stats.get('copied', 0)}\")\n",
    "print(f\"Skipped (already exist): {copy_stats.get('skipped', 0)}\")\n",
    "print(f\"Tip: If you need to re-copy, set overwrite=True in the function call above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_gold_standard_names():\n",
    "#     \"\"\"\n",
    "#     Rename gold standard files to match the PDF-based naming convention.\n",
    "#     Old: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
    "#     New: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "    \n",
    "#     # The standard name from your PDF\n",
    "#     STANDARD_BASE = \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "    \n",
    "#     for old_path in GOLD_DIR.glob(\"*.json\"):\n",
    "#         # Extract page number\n",
    "#         match = re.search(r'page-(\\d+)\\.json$', old_path.name)\n",
    "#         if not match:\n",
    "#             print(f\"⚠️  Skipping (no page number): {old_path.name}\")\n",
    "#             continue\n",
    "        \n",
    "#         page_num = match.group(1)\n",
    "#         new_name = f\"{STANDARD_BASE}__page-{page_num}.json\"\n",
    "#         new_path = old_path.parent / new_name\n",
    "        \n",
    "#         if old_path.name != new_name:\n",
    "#             print(f\"Renaming: {old_path.name}\")\n",
    "#             print(f\"      →  {new_name}\")\n",
    "#             old_path.rename(new_path)\n",
    "    \n",
    "#     print(\"\\n✓ Gold standard filenames standardized!\")\n",
    "\n",
    "# # Uncomment to run:\n",
    "# standardize_gold_standard_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating against: Stage1PageModel\n",
      "\n",
      "✓ Valid: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "\n",
      "==================================================\n",
      "Validation Summary\n",
      "==================================================\n",
      "Total files: 1\n",
      "Valid: 1 ✓\n",
      "Invalid: 0 x\n"
     ]
    }
   ],
   "source": [
    "def validate_json_files(directory: Path, schema_class) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate all JSON files in directory against a schema.\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to folder with JSON files\n",
    "        schema_class: Pydantic model class (PageWithContinuation or PageNoContinuation)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with validation results and errors\n",
    "    \"\"\"\n",
    "    json_files = list(directory.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        return {\"total\": 0, \"valid\": 0, \"invalid\": 0, \"errors\": {}}\n",
    "    \n",
    "    results = {\n",
    "        \"total\": len(json_files),\n",
    "        \"valid\": 0,\n",
    "        \"invalid\": 0,\n",
    "        \"errors\": {}\n",
    "    }\n",
    "    \n",
    "    for json_file in sorted(json_files):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Validate with Pydantic\n",
    "            schema_class(**data)\n",
    "            results[\"valid\"] += 1\n",
    "            print(f\"✓ Valid: {json_file.name}\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            results[\"invalid\"] += 1\n",
    "            results[\"errors\"][json_file.name] = f\"JSON parse error: {e}\"\n",
    "            print(f\"Invalid JSON: {json_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[\"invalid\"] += 1\n",
    "            results[\"errors\"][json_file.name] = str(e)\n",
    "            print(f\"Schema error: {json_file.name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Choose which schema to validate against\n",
    "# Uncomment the one you're using:\n",
    "\n",
    "SCHEMA = PageWithContinuation  # Schema WITH continuation\n",
    "# SCHEMA = PageNoContinuation  # Schema WITHOUT continuation\n",
    "\n",
    "print(f\"Validating against: {SCHEMA.__name__}\\n\")\n",
    "validation_results = validate_json_files(GOLD_DIR_CLEAN, SCHEMA)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Validation Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total files: {validation_results['total']}\")\n",
    "print(f\"Valid: {validation_results['valid']} ✓\")\n",
    "print(f\"Invalid: {validation_results['invalid']} x\")\n",
    "\n",
    "if validation_results['errors']:\n",
    "    print(f\"Errors found:\")\n",
    "    for filename, error in validation_results['errors'].items():\n",
    "        print(f\"  • {filename}:\")\n",
    "        print(f\"    {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running consistency checks...\n",
      "\n",
      "==================================================\n",
      "📊 Consistency Check Results\n",
      "==================================================\n",
      "\n",
      "✓ All checks passed! No warnings found.\n"
     ]
    }
   ],
   "source": [
    "def consistency_checks(directory: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Run consistency checks on annotated files.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with warnings by category\n",
    "    \"\"\"\n",
    "    json_files = list(directory.glob(\"*.json\"))\n",
    "    \n",
    "    warnings = defaultdict(list)\n",
    "    valid_classes = {\"prose\", \"verse\", \"ad\", \"paratext\", \"unknown\"}\n",
    "    \n",
    "    for json_file in sorted(json_files):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check each item\n",
    "            for i, item in enumerate(data.get('items', [])):\n",
    "                item_id = f\"{json_file.name} - item {i}\"\n",
    "                \n",
    "                # Check 1: Valid item class\n",
    "                item_class = item.get('item_class')\n",
    "                if item_class not in valid_classes:\n",
    "                    warnings['invalid_class'].append(\n",
    "                        f\"{item_id}: '{item_class}' not in {valid_classes}\"\n",
    "                    )\n",
    "                \n",
    "                # Check 2: Empty text\n",
    "                text = item.get('item_text_raw', '').strip()\n",
    "                if not text:\n",
    "                    warnings['empty_text'].append(\n",
    "                        f\"{item_id}: item_text_raw is empty\"\n",
    "                    )\n",
    "                \n",
    "                # Check 3: Continuation logic (if schema has these fields)\n",
    "                is_cont = item.get('is_continuation', False)\n",
    "                continues = item.get('continues_on_next_page', False)\n",
    "                \n",
    "                # Suspicious: same item both starts and ends continuation\n",
    "                if is_cont and continues:\n",
    "                    warnings['continuation_logic'].append(\n",
    "                        f\"{item_id}: both is_continuation AND continues_on_next_page\"\n",
    "                    )\n",
    "                \n",
    "                # Check 4: Very short text (potential OCR error)\n",
    "                if len(text) < 10 and item_class not in ['paratext', 'unknown']:\n",
    "                    warnings['short_text'].append(\n",
    "                        f\"{item_id}: suspiciously short text ({len(text)} chars)\"\n",
    "                    )\n",
    "        \n",
    "        except Exception as e:\n",
    "            warnings['file_error'].append(f\"{json_file.name}: {e}\")\n",
    "    \n",
    "    return dict(warnings)\n",
    "\n",
    "print(\"🔍 Running consistency checks...\\n\")\n",
    "checks = consistency_checks(GOLD_DIR)\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Consistency Check Results\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "if not checks:\n",
    "    print(\"✓ All checks passed! No warnings found.\")\n",
    "else:\n",
    "    for category, warnings in checks.items():\n",
    "        print(f\"{category.upper().replace('_', ' ')} ({len(warnings)}):\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   • {warning}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics...\n",
      "\n",
      "==================================================\n",
      "Gold Standard Statistics\n",
      "==================================================\n",
      "\n",
      "Pages: 1\n",
      "   • Empty pages: 0\n",
      "   • With content: 1\n",
      "\n",
      "Items: 7\n",
      "   • Average per page: 7.0\n",
      "\n",
      "Item Classes:\n",
      "   • paratext: 7 (100.0%)\n",
      "\n",
      "Metadata:\n",
      "   • Items with title: 1\n",
      "   • Items with author: 0\n",
      "\n",
      "Text Length:\n",
      "   • Average: 168 characters\n",
      "   • Min: 38\n",
      "   • Max: 894\n"
     ]
    }
   ],
   "source": [
    "def compute_statistics(directory: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for annotated corpus.\n",
    "    \"\"\"\n",
    "    json_files = list(directory.glob(\"*.json\"))\n",
    "    \n",
    "    stats = {\n",
    "        'total_pages': len(json_files),\n",
    "        'total_items': 0,\n",
    "        'item_classes': Counter(),\n",
    "        'authors': Counter(),\n",
    "        'has_title': 0,\n",
    "        'has_author': 0,\n",
    "        'is_continuation': 0,\n",
    "        'continues_next': 0,\n",
    "        'text_lengths': [],\n",
    "        'empty_pages': 0\n",
    "    }\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            items = data.get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                stats['empty_pages'] += 1\n",
    "                continue\n",
    "            \n",
    "            stats['total_items'] += len(items)\n",
    "            \n",
    "            for item in items:\n",
    "                # Item class\n",
    "                stats['item_classes'][item.get('item_class', 'unknown')] += 1\n",
    "                \n",
    "                # Title/author presence\n",
    "                if item.get('item_title'):\n",
    "                    stats['has_title'] += 1\n",
    "                if item.get('item_author'):\n",
    "                    stats['has_author'] += 1\n",
    "                    stats['authors'][item.get('item_author')] += 1\n",
    "                \n",
    "                # Continuation (if schema has these fields)\n",
    "                if item.get('is_continuation'):\n",
    "                    stats['is_continuation'] += 1\n",
    "                if item.get('continues_on_next_page'):\n",
    "                    stats['continues_next'] += 1\n",
    "                \n",
    "                # Text length\n",
    "                text_len = len(item.get('item_text_raw', ''))\n",
    "                stats['text_lengths'].append(text_len)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file.name}: {e}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    if stats['text_lengths']:\n",
    "        stats['avg_text_length'] = sum(stats['text_lengths']) / len(stats['text_lengths'])\n",
    "    else:\n",
    "        stats['avg_text_length'] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Computing statistics...\\n\")\n",
    "stats = compute_statistics(GOLD_DIR_CLEAN)\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Gold Standard Statistics\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "print(f\"Pages: {stats['total_pages']}\")\n",
    "print(f\"   • Empty pages: {stats['empty_pages']}\")\n",
    "print(f\"   • With content: {stats['total_pages'] - stats['empty_pages']}\")\n",
    "\n",
    "print(f\"\\nItems: {stats['total_items']}\")\n",
    "print(f\"   • Average per page: {stats['total_items'] / max(stats['total_pages'], 1):.1f}\")\n",
    "\n",
    "print(f\"\\nItem Classes:\")\n",
    "for item_class, count in stats['item_classes'].most_common():\n",
    "    percentage = (count / stats['total_items'] * 100) if stats['total_items'] > 0 else 0\n",
    "    print(f\"   • {item_class}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "if stats['is_continuation'] > 0 or stats['continues_next'] > 0:\n",
    "    print(f\"\\nContinuations:\")\n",
    "    print(f\"   • Starts with continuation: {stats['is_continuation']}\")\n",
    "    print(f\"   • Continues on next page: {stats['continues_next']}\")\n",
    "\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"   • Items with title: {stats['has_title']}\")\n",
    "print(f\"   • Items with author: {stats['has_author']}\")\n",
    "\n",
    "if stats['authors']:\n",
    "    print(f\"\\nTop Authors:\")\n",
    "    for author, count in stats['authors'].most_common(5):\n",
    "        print(f\"   • {author}: {count}\")\n",
    "\n",
    "print(f\"\\nText Length:\")\n",
    "print(f\"   • Average: {stats['avg_text_length']:.0f} characters\")\n",
    "if stats['text_lengths']:\n",
    "    print(f\"   • Min: {min(stats['text_lengths'])}\")\n",
    "    print(f\"   • Max: {max(stats['text_lengths'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Annotation Progress Checklist\n",
      "==================================================\n",
      "\n",
      "Total files: 1\n",
      "\n",
      "[✓]  1. La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json (9m ago)\n",
      "\n",
      "Legend:\n",
      "   [ ]  Not recently modified\n",
      "   [✓]  Modified within last hour\n",
      "\n",
      "Last check: 2025-10-14 18:26:26\n"
     ]
    }
   ],
   "source": [
    "def track_progress(directory: Path) -> None:\n",
    "    \"\"\"\n",
    "    Display progress tracking checklist.\n",
    "    \"\"\"\n",
    "    json_files = sorted(directory.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found to track.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Annotation Progress Checklist\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    print(f\"Total files: {len(json_files)}\\n\")\n",
    "    \n",
    "    now = datetime.now()\n",
    "    \n",
    "    for i, json_file in enumerate(json_files, 1):\n",
    "        # Get modification time\n",
    "        mod_time = datetime.fromtimestamp(json_file.stat().st_mtime)\n",
    "        time_ago = now - mod_time\n",
    "        \n",
    "        # Format time ago\n",
    "        if time_ago.days > 0:\n",
    "            time_str = f\"{time_ago.days}d ago\"\n",
    "        elif time_ago.seconds > 3600:\n",
    "            time_str = f\"{time_ago.seconds // 3600}h ago\"\n",
    "        elif time_ago.seconds > 60:\n",
    "            time_str = f\"{time_ago.seconds // 60}m ago\"\n",
    "        else:\n",
    "            time_str = \"just now\"\n",
    "        \n",
    "        # Check if file has been modified recently (within last hour)\n",
    "        recently_modified = time_ago.seconds < 3600 and time_ago.days == 0\n",
    "        indicator = \"[✓]\" if recently_modified else \"[ ]\"\n",
    "        \n",
    "        print(f\"{indicator} {i:2d}. {json_file.name:<40} ({time_str})\")\n",
    "    \n",
    "    print(f\"\\nLegend:\")\n",
    "    print(f\"   [ ]  Not recently modified\")\n",
    "    print(f\"   [✓]  Modified within last hour\")\n",
    "    print(f\"\\nLast check: {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "track_progress(GOLD_DIR_CLEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FULL VALIDATION REPORT\n",
      "============================================================\n",
      "\n",
      "[1] Schema Validation...\n",
      "✓ Valid: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "    1/1 files valid\n",
      "\n",
      "[2] Consistency Checks...\n",
      "    No warnings found\n",
      "\n",
      "[3] Computing Statistics...\n",
      "    • 1 pages\n",
      "    • 7 items\n",
      "    • 1 unique item classes\n",
      "\n",
      "============================================================\n",
      "[✓] GOLD STANDARD READY!\n",
      "    All files validated successfully.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def full_validation_report(directory: Path, schema_class) -> None:\n",
    "    \"\"\"\n",
    "    Run complete validation suite and print summary report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FULL VALIDATION REPORT\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # 1. Schema validation\n",
    "    print(\"[1] Schema Validation...\")\n",
    "    validation = validate_json_files(directory, schema_class)\n",
    "    print(f\"    {validation['valid']}/{validation['total']} files valid\\n\")\n",
    "    \n",
    "    # 2. Consistency checks\n",
    "    print(\"[2] Consistency Checks...\")\n",
    "    checks = consistency_checks(directory)\n",
    "    total_warnings = sum(len(w) for w in checks.values())\n",
    "    if total_warnings == 0:\n",
    "        print(\"    No warnings found\\n\")\n",
    "    else:\n",
    "        print(f\"    {total_warnings} warnings (see details above)\\n\")\n",
    "    \n",
    "    # 3. Statistics\n",
    "    print(\"[3] Computing Statistics...\")\n",
    "    stats = compute_statistics(directory)\n",
    "    print(f\"    • {stats['total_pages']} pages\")\n",
    "    print(f\"    • {stats['total_items']} items\")\n",
    "    print(f\"    • {len(stats['item_classes'])} unique item classes\\n\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"=\"*60)\n",
    "    if validation['invalid'] == 0 and total_warnings == 0:\n",
    "        print(\"[✓] GOLD STANDARD READY!\")\n",
    "        print(\"    All files validated successfully.\")\n",
    "    elif validation['invalid'] == 0:\n",
    "        print(\"[!] GOLD STANDARD HAS WARNINGS\")\n",
    "        print(\"    Schema valid but consistency issues found.\")\n",
    "    else:\n",
    "        print(\"[X] GOLD STANDARD HAS ERRORS\")\n",
    "        print(\"    Fix schema validation errors before proceeding.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run full validation\n",
    "full_validation_report(GOLD_DIR_CLEAN, SCHEMA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
