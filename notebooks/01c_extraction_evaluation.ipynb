{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Evaluation\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "\n",
      "Directories:\n",
      "  Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "  Predictions:   /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n",
      "  Schema:        Stage1PageModel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 OCR Evaluation\n",
    "\n",
    "Evaluates OCR extraction quality by comparing predictions against gold standard.\n",
    "\n",
    "Input:  Predictions from data/predictions/{magazine_name}/\n",
    "        Gold standard from data/gold_standard/cleaned/{magazine_name}/\n",
    "Output: Evaluation metrics and analysis\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Project root detection\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "# Add root to path \n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Stage 1 OCR Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Add schemas to path\n",
    "SCHEMAS_DIR = PROJECT_ROOT / \"schemas\"\n",
    "if str(SCHEMAS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCHEMAS_DIR))\n",
    "\n",
    "# Import schemas for validation\n",
    "from stage1_page import Stage1PageModel\n",
    "\n",
    "# Paths\n",
    "GOLD_ROOT = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_ROOT = PROJECT_ROOT / \"data\" / \"predictions\"\n",
    "\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Gold standard: {GOLD_ROOT}\")\n",
    "print(f\"  Predictions:   {PRED_ROOT}\")\n",
    "print(f\"  Schema:        {Stage1PageModel.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3780b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Finding Magazine Pairs\n",
      "============================================================\n",
      "\n",
      "Found 2 magazine(s) for evaluation:\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889:\n",
      "  Gold files:      14\n",
      "  Pred files:      14\n",
      "  Matching files:  14\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893:\n",
      "  Gold files:      1\n",
      "  Pred files:      34\n",
      "  Matching files:  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find Magazine Pairs for Evaluation\n",
    "\"\"\"\n",
    "\n",
    "def find_magazine_pairs() -> List[Tuple[str, Path, Path, int]]:\n",
    "    \"\"\"\n",
    "    Find magazines that have both gold standard and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        List of (magazine_name, gold_dir, pred_dir, num_matching_files) tuples\n",
    "    \"\"\"\n",
    "    gold_magazines = {d.name: d for d in GOLD_ROOT.iterdir() if d.is_dir()}\n",
    "    pred_magazines = {d.name: d for d in PRED_ROOT.iterdir() if d.is_dir()}\n",
    "    \n",
    "    common_magazines = set(gold_magazines.keys()) & set(pred_magazines.keys())\n",
    "    \n",
    "    pairs = []\n",
    "    for mag_name in sorted(common_magazines):\n",
    "        gold_dir = gold_magazines[mag_name]\n",
    "        pred_dir = pred_magazines[mag_name]\n",
    "        \n",
    "        gold_files = {f.name for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.name for f in pred_dir.glob(\"*.json\")}\n",
    "        \n",
    "        matching_files = gold_files & pred_files\n",
    "        \n",
    "        if matching_files:\n",
    "            pairs.append((mag_name, gold_dir, pred_dir, len(matching_files)))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Find pairs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Finding Magazine Pairs\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "magazine_pairs = find_magazine_pairs()\n",
    "\n",
    "if not magazine_pairs:\n",
    "    print(\"No matching magazine pairs found.\")\n",
    "    print(\"\\nCheck that:\")\n",
    "    print(\"  1. Gold standard magazines exist in cleaned/\")\n",
    "    print(\"  2. Prediction magazines exist in predictions/\")\n",
    "    print(\"  3. Magazine names match between both directories\")\n",
    "else:\n",
    "    print(f\"Found {len(magazine_pairs)} magazine(s) for evaluation:\\n\")\n",
    "    for mag_name, gold_dir, pred_dir, num_files in magazine_pairs:\n",
    "        print(f\"{mag_name}:\")\n",
    "        print(f\"  Gold files:      {len(list(gold_dir.glob('*.json')))}\")\n",
    "        print(f\"  Pred files:      {len(list(pred_dir.glob('*.json')))}\")\n",
    "        print(f\"  Matching files:  {num_files}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2a9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Item Matching Configuration\n",
      "============================================================\n",
      "Text Similarity threshold: 0.7\n",
      "Metadata similarity threshold: 0.8\n",
      "\n",
      "============================================================\n",
      "Item Matching Test\n",
      "============================================================\n",
      "\n",
      "Test page: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Gold items:     8\n",
      "  Pred items:     1\n",
      "  Matches:        1\n",
      "  Unmatched gold: 7\n",
      "  Unmatched pred: 0\n",
      "  Avg match quality: 79.40%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Match gold items to predicted items using content-based text similarity.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_THRESHOLD = 0.7  # Minimum text similarity to consider a match (0.0-1.0)\n",
    "METADATA_SIMILARITY_THRESHOLD = 0.8  # For title/author matching\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Item Matching Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"Metadata similarity threshold: {METADATA_SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "        \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item, find the best-matching unmatched pred item.\n",
    "        Accept the match if similarity exceeds threshold.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair and match items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "\n",
    "# Test matching on first page\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Item Matching Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_pairs:\n",
    "    # Get first magazine's first page\n",
    "    mag_name, gold_dir, pred_dir, _ = magazine_pairs[0]\n",
    "    gold_files = sorted(gold_dir.glob(\"*.json\"))\n",
    "    pred_files = sorted(pred_dir.glob(\"*.json\"))\n",
    "    \n",
    "    if gold_files and pred_files:\n",
    "        test_result = load_and_match_page(gold_files[0], pred_files[0])\n",
    "        \n",
    "        print(f\"Test page: {test_result['page_name']}\")\n",
    "        print(f\"  Gold items:     {len(test_result['gold_items'])}\")\n",
    "        print(f\"  Pred items:     {len(test_result['pred_items'])}\")\n",
    "        print(f\"  Matches:        {len(test_result['matches'])}\")\n",
    "        print(f\"  Unmatched gold: {len(test_result['unmatched_gold'])}\")\n",
    "        print(f\"  Unmatched pred: {len(test_result['unmatched_pred'])}\")\n",
    "        \n",
    "        if test_result['matches']:\n",
    "            avg_score = sum(score for _, _, score in test_result['matches']) / len(test_result['matches'])\n",
    "            print(f\"  Avg match quality: {avg_score:.2%}\")\n",
    "else:\n",
    "    print(\"No magazine pairs available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89939141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading and Matching All Pages\n",
      "============================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889:\n",
      "  Pages:       14\n",
      "  Gold items:  70\n",
      "  Pred items:  55\n",
      "  Matches:     37\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893:\n",
      "  Pages:       1\n",
      "  Gold items:  3\n",
      "  Pred items:  2\n",
      "  Matches:     2\n",
      "\n",
      "Total pages: 15\n",
      "Total matches: 39\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and Match All Pages\n",
    "\"\"\"\n",
    "\n",
    "def load_all_magazine_pages(magazine_pairs: List[Tuple[str, Path, Path, int]]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load and match all pages for all magazines.\n",
    "    \n",
    "    Args:\n",
    "        magazine_pairs: List from find_magazine_pairs()\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping magazine_name to list of matched page dicts\n",
    "    \"\"\"\n",
    "    all_magazine_data = {}\n",
    "    \n",
    "    for mag_name, gold_dir, pred_dir, _ in magazine_pairs:\n",
    "        gold_files = {f.name: f for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.name: f for f in pred_dir.glob(\"*.json\")}\n",
    "        \n",
    "        common_files = set(gold_files.keys()) & set(pred_files.keys())\n",
    "        \n",
    "        magazine_pages = []\n",
    "        for filename in sorted(common_files):\n",
    "            page_data = load_and_match_page(gold_files[filename], pred_files[filename])\n",
    "            page_data['page_id'] = gold_files[filename].stem\n",
    "            magazine_pages.append(page_data)\n",
    "        \n",
    "        all_magazine_data[mag_name] = magazine_pages\n",
    "    \n",
    "    return all_magazine_data\n",
    "\n",
    "# Load all pages\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading and Matching All Pages\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "all_magazine_data = load_all_magazine_pages(magazine_pairs)\n",
    "\n",
    "# Summary per magazine\n",
    "for mag_name, pages in all_magazine_data.items():\n",
    "    total_gold = sum(len(p['gold_items']) for p in pages)\n",
    "    total_pred = sum(len(p['pred_items']) for p in pages)\n",
    "    total_matches = sum(len(p['matches']) for p in pages)\n",
    "    \n",
    "    print(f\"{mag_name}:\")\n",
    "    print(f\"  Pages:       {len(pages)}\")\n",
    "    print(f\"  Gold items:  {total_gold}\")\n",
    "    print(f\"  Pred items:  {total_pred}\")\n",
    "    print(f\"  Matches:     {total_matches}\")\n",
    "    print()\n",
    "\n",
    "# Flatten for compatibility with existing evaluation code\n",
    "all_pages = []\n",
    "for pages in all_magazine_data.values():\n",
    "    all_pages.extend(pages)\n",
    "\n",
    "print(f\"Total pages: {len(all_pages)}\")\n",
    "print(f\"Total matches: {sum(len(page['matches']) for page in all_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7322404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running diagnostics on all pages...\n",
      "\n",
      "\n",
      "\n",
      "SUMMARY TABLE\n",
      "\n",
      "\n",
      "                                    page_id  gold_items  pred_items  matched  match_rate_%  contrib_match_rate_%  avg_similarity  gold_cont_in  pred_cont_in  gold_cont_out  pred_cont_out                                flags\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-001           8           1        1          12.5                   0.0           0.794             0             0              0              0            LOW_MATCH, COUNT_MISMATCH\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-002           2           2        2         100.0                   0.0           0.905             0             0              0              0                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-003           3           3        1          33.3                   0.0           0.713             0             1              1              1               LOW_MATCH, LOW_CONTRIB\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-004           5           3        1          20.0                  33.3           0.991             1             0              1              0               LOW_MATCH, LOW_CONTRIB\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-005           5           3        1          20.0                  33.3           0.990             1             0              1              1               LOW_MATCH, LOW_CONTRIB\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-006           6           5        3          50.0                  75.0           0.925             1             1              1              1                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-007           2           4        1          50.0                   0.0           1.000             0             0              0              0                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-008           0           0        0           0.0                   0.0           0.000             0             0              0              0  ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-009           7           5        4          57.1                  80.0           0.983             1             0              1              1                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-010           6           4        3          50.0                  75.0           0.989             1             0              1              1                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-011           3           1        0           0.0                   0.0           0.000             1             1              1              1 ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-012           8          10        7          87.5                  75.0           0.981             1             1              0              1                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-013          11          11       10          90.9                   0.0           0.932             0             0              0              0                                     \n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-014           4           3        3          75.0                   0.0           0.939             0             0              0              0                                     \n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001           3           2        2          66.7                 100.0           0.861             0             0              1              1                                     \n",
      "\n",
      "\n",
      "================================================================================\n",
      "DETAILED REPORTS\n",
      "================================================================================\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-001 ===\n",
      "Items: 8 gold, 1 pred\n",
      "Matches: 1 (12.5% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   8 gold, 1 pred, 1 matched (12.5%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.794\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 3, 4, 6, 7]\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: LOW_MATCH, COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-002 ===\n",
      "Items: 2 gold, 2 pred\n",
      "Matches: 2 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 2 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.905\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-003 ===\n",
      "Items: 3 gold, 3 pred\n",
      "Matches: 1 (33.3% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 1 matched (50.0%)\n",
      "  prose      1 gold, 2 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 2 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.713\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [1, 2]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-004 ===\n",
      "Items: 5 gold, 3 pred\n",
      "Matches: 1 (20.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 1 pred, 0 matched (0.0%)\n",
      "  verse      1 gold, 2 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 3 pred, 1 matched (33.3%)\n",
      "Avg similarity: 0.991\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 4]\n",
      "Unmatched pred items: [0, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-005 ===\n",
      "Items: 5 gold, 3 pred\n",
      "Matches: 1 (20.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 1 matched (50.0%)\n",
      "  verse      1 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 3 gold, 3 pred, 1 matched (33.3%)\n",
      "Avg similarity: 0.990\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 3]\n",
      "Unmatched pred items: [0, 1]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-006 ===\n",
      "Items: 6 gold, 5 pred\n",
      "Matches: 3 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 3 pred, 1 matched (50.0%)\n",
      "  verse      2 gold, 2 pred, 2 matched (100.0%)\n",
      "\n",
      "Contributions: 4 gold, 5 pred, 3 matched (75.0%)\n",
      "Avg similarity: 0.925\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 4]\n",
      "Unmatched pred items: [2, 3]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-007 ===\n",
      "Items: 2 gold, 4 pred\n",
      "Matches: 1 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 4 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1]\n",
      "Unmatched pred items: [1, 2, 3]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-008 ===\n",
      "Items: 0 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-009 ===\n",
      "Items: 7 gold, 5 pred\n",
      "Matches: 4 (57.1% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      3 gold, 3 pred, 2 matched (66.7%)\n",
      "\n",
      "Contributions: 5 gold, 5 pred, 4 matched (80.0%)\n",
      "Avg similarity: 0.983\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 4]\n",
      "Unmatched pred items: [2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-010 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 3 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 3 matched (75.0%)\n",
      "Avg similarity: 0.989\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 4]\n",
      "Unmatched pred items: [2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-011 ===\n",
      "Items: 3 gold, 1 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      1 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2]\n",
      "Unmatched pred items: [0]\n",
      "\n",
      "FLAGS: ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-012 ===\n",
      "Items: 8 gold, 10 pred\n",
      "Matches: 7 (87.5% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   4 gold, 5 pred, 4 matched (100.0%)\n",
      "  prose      2 gold, 3 pred, 1 matched (50.0%)\n",
      "  verse      2 gold, 2 pred, 2 matched (100.0%)\n",
      "\n",
      "Contributions: 4 gold, 5 pred, 3 matched (75.0%)\n",
      "Avg similarity: 0.981\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 0 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [5]\n",
      "Unmatched pred items: [3, 4, 7]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-013 ===\n",
      "Items: 11 gold, 11 pred\n",
      "Matches: 10 (90.9% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   11 gold, 11 pred, 10 matched (90.9%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.932\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [7]\n",
      "Unmatched pred items: [7]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-014 ===\n",
      "Items: 4 gold, 3 pred\n",
      "Matches: 3 (75.0% match rate)\n",
      "\n",
      "By class:\n",
      "  ad         1 gold, 0 pred, 1 matched (100.0%)\n",
      "  paratext   3 gold, 3 pred, 2 matched (66.7%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.939\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [2]\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1212187t_15-11-1893__page-001 ===\n",
      "Items: 3 gold, 2 pred\n",
      "Matches: 2 (66.7% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 1 matched (50.0%)\n",
      "  prose      1 gold, 1 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 1 gold, 1 pred, 1 matched (100.0%)\n",
      "Avg similarity: 0.861\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [1]\n",
      "Unmatched pred items: []\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-Level Diagnostics\n",
    "Generate diagnostic metrics for each page based on item matches.\n",
    "\"\"\"\n",
    "\n",
    "def diagnose_page(page_id: str, gold_items: list, pred_items: list, matches: list) -> dict:\n",
    "    \"\"\"\n",
    "    Generate diagnostic metrics for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page_id: Page identifier\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with diagnostic metrics\n",
    "    \"\"\"\n",
    "    # Count items by class\n",
    "    gold_by_class = {}\n",
    "    pred_by_class = {}\n",
    "    \n",
    "    for item in gold_items:\n",
    "        item_class = item['item_class']\n",
    "        gold_by_class[item_class] = gold_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    for item in pred_items:\n",
    "        item_class = item['item_class']\n",
    "        pred_by_class[item_class] = pred_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    # Count contributions (prose + verse)\n",
    "    gold_contrib = gold_by_class.get('prose', 0) + gold_by_class.get('verse', 0)\n",
    "    pred_contrib = pred_by_class.get('prose', 0) + pred_by_class.get('verse', 0)\n",
    "    \n",
    "    # Filter matches by contribution class\n",
    "    contrib_matches = [\n",
    "        (g_idx, p_idx, score) for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in ('prose', 'verse')\n",
    "    ]\n",
    "    \n",
    "    # Calculate match rates\n",
    "    match_rate = (len(matches) / len(gold_items) * 100) if gold_items else 0\n",
    "    contrib_match_rate = (len(contrib_matches) / gold_contrib * 100) if gold_contrib else 0\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    avg_similarity = (sum(score for _, _, score in matches) / len(matches)) if matches else 0\n",
    "    \n",
    "    # Count continuation flags\n",
    "    gold_cont_in = sum(1 for item in gold_items if item.get('is_continuation') is True)\n",
    "    pred_cont_in = sum(1 for item in pred_items if item.get('is_continuation') is True)\n",
    "    gold_cont_out = sum(1 for item in gold_items if item.get('continues_on_next_page') is True)\n",
    "    pred_cont_out = sum(1 for item in pred_items if item.get('continues_on_next_page') is True)\n",
    "    \n",
    "    # Track matched indices\n",
    "    matched_gold = {g_idx for g_idx, _, _ in matches}\n",
    "    matched_pred = {p_idx for _, p_idx, _ in matches}\n",
    "    \n",
    "    unmatched_gold = [i for i in range(len(gold_items)) if i not in matched_gold]\n",
    "    unmatched_pred = [i for i in range(len(pred_items)) if i not in matched_pred]\n",
    "    \n",
    "    # Count matches by class\n",
    "    matches_by_class = {}\n",
    "    for g_idx, p_idx, score in matches:\n",
    "        item_class = gold_items[g_idx]['item_class']\n",
    "        matches_by_class[item_class] = matches_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items),\n",
    "        'matched': len(matches),\n",
    "        'match_rate': match_rate,\n",
    "        'contrib_match_rate': contrib_match_rate,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'gold_cont_in': gold_cont_in,\n",
    "        'pred_cont_in': pred_cont_in,\n",
    "        'gold_cont_out': gold_cont_out,\n",
    "        'pred_cont_out': pred_cont_out,\n",
    "        'gold_by_class': gold_by_class,\n",
    "        'pred_by_class': pred_by_class,\n",
    "        'matches_by_class': matches_by_class,\n",
    "        'gold_contrib': gold_contrib,\n",
    "        'pred_contrib': pred_contrib,\n",
    "        'contrib_matched': len(contrib_matches),\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def flag_page(metrics: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate flags for problematic pages based on metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary from diagnose_page()\n",
    "        \n",
    "    Returns:\n",
    "        Comma-separated string of flags, or empty string if no issues\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    if metrics['pred_items'] == 0:\n",
    "        flags.append('ZERO_PREDS')\n",
    "    \n",
    "    if metrics['matched'] == 0:\n",
    "        flags.append('ZERO_MATCHES')\n",
    "    \n",
    "    if metrics['match_rate'] < 50:\n",
    "        flags.append('LOW_MATCH')\n",
    "    \n",
    "    if metrics['gold_contrib'] > 0 and metrics['contrib_match_rate'] < 60:\n",
    "        flags.append('LOW_CONTRIB')\n",
    "    \n",
    "    if abs(metrics['gold_items'] - metrics['pred_items']) >= 3:\n",
    "        flags.append('COUNT_MISMATCH')\n",
    "    \n",
    "    return ', '.join(flags)\n",
    "\n",
    "\n",
    "def run_diagnostics(all_pages: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run diagnostics on all pages and generate summary table and detailed reports.\n",
    "    \n",
    "    Args:\n",
    "        all_pages: List of page dicts from load_all_magazine_pages()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary metrics for all pages\n",
    "    \"\"\"\n",
    "    print(\"Running diagnostics on all pages...\\n\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for page in all_pages:\n",
    "        page_id = page['page_id']\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        # Generate metrics\n",
    "        metrics = diagnose_page(page_id, gold_items, pred_items, matches)\n",
    "        metrics['flags'] = flag_page(metrics)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for m in all_metrics:\n",
    "        summary_data.append({\n",
    "            'page_id': m['page_id'],\n",
    "            'gold_items': m['gold_items'],\n",
    "            'pred_items': m['pred_items'],\n",
    "            'matched': m['matched'],\n",
    "            'match_rate_%': round(m['match_rate'], 1),\n",
    "            'contrib_match_rate_%': round(m['contrib_match_rate'], 1),\n",
    "            'avg_similarity': round(m['avg_similarity'], 3),\n",
    "            'gold_cont_in': m['gold_cont_in'],\n",
    "            'pred_cont_in': m['pred_cont_in'],\n",
    "            'gold_cont_out': m['gold_cont_out'],\n",
    "            'pred_cont_out': m['pred_cont_out'],\n",
    "            'flags': m['flags']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print detailed reports for all pages\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for m in all_metrics:\n",
    "        print(f\"\\n=== Page {m['page_id']} ===\")\n",
    "        print(f\"Items: {m['gold_items']} gold, {m['pred_items']} pred\")\n",
    "        print(f\"Matches: {m['matched']} ({m['match_rate']:.1f}% match rate)\")\n",
    "        \n",
    "        print(\"\\nBy class:\")\n",
    "        all_classes = sorted(set(m['gold_by_class'].keys()) | set(m['pred_by_class'].keys()))\n",
    "        for cls in all_classes:\n",
    "            gold_count = m['gold_by_class'].get(cls, 0)\n",
    "            pred_count = m['pred_by_class'].get(cls, 0)\n",
    "            matched_count = m['matches_by_class'].get(cls, 0)\n",
    "            match_pct = (matched_count / gold_count * 100) if gold_count > 0 else 0\n",
    "            print(f\"  {cls:10s} {gold_count} gold, {pred_count} pred, {matched_count} matched ({match_pct:.1f}%)\")\n",
    "\n",
    "        \n",
    "        print(f\"\\nContributions: {m['gold_contrib']} gold, {m['pred_contrib']} pred, \"\n",
    "              f\"{m['contrib_matched']} matched ({m['contrib_match_rate']:.1f}%)\")\n",
    "        print(f\"Avg similarity: {m['avg_similarity']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nContinuations:\")\n",
    "        print(f\"  is_continuation: {m['gold_cont_in']} gold, {m['pred_cont_in']} pred\")\n",
    "        print(f\"  continues_on_next_page: {m['gold_cont_out']} gold, {m['pred_cont_out']} pred\")\n",
    "        \n",
    "        print(f\"\\nUnmatched gold items: {m['unmatched_gold']}\")\n",
    "        print(f\"Unmatched pred items: {m['unmatched_pred']}\")\n",
    "        \n",
    "        if m['flags']:\n",
    "            print(f\"\\nFLAGS: {m['flags']}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostic_df = run_diagnostics(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fcf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Helpers\n",
    "Utility functions for filtering matches and loading all pages efficiently.\n",
    "These helpers are used by the evaluation cells that follow.\n",
    "\"\"\"\n",
    "\n",
    "def filter_matches_by_class(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    item_classes: List[str]\n",
    ") -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Filter matches to only include items of specified classes.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        item_classes: List of classes to include (e.g., ['prose', 'verse'])\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of matches\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (g_idx, p_idx, score) \n",
    "        for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in item_classes\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_matched_pairs(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict]\n",
    ") -> List[Tuple[Dict, Dict, float]]:\n",
    "    \"\"\"\n",
    "    Convert match indices to actual item pairs.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_item, pred_item, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (gold_items[g_idx], pred_items[p_idx], score)\n",
    "        for g_idx, p_idx, score in matches\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text quality...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEXT QUALITY SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. ORDER-AGNOSTIC EVALUATION\n",
      "   (Pure OCR quality, reading order irrelevant)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 8.08%  |  WER: 11.62%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 8.08%  |  WER: 11.62%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 7.44%\n",
      "\n",
      "   Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 6.66%  |  WER: 8.31%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 6.66%  |  WER: 8.31%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 6.58%\n",
      "\n",
      "======================================================================\n",
      "2. STRUCTURE-AWARE EVALUATION\n",
      "   (OCR quality on matched content only)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   Matched Content - All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 12.23%  |  WER: 18.84%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 12.05%  |  WER: 18.84%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 11.63%\n",
      "      Coverage: 27,015 chars matched (52.4% of gold)\n",
      "      Unmatched: 24,604 chars (47.7% of gold)\n",
      "\n",
      "   Matched Content - Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 2.53%  |  WER: 8.97%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 2.35%  |  WER: 8.97%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 1.93%\n",
      "      Coverage: 16,930 chars matched (42.0% of gold)\n",
      "      Unmatched: 23,350 chars (58.0% of gold)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION GUIDE:\n",
      "----------------------------------------------------------------------\n",
      "Strict: Most conservative\n",
      "Standard: Fair baseline - normalizes whitespace\n",
      "Letters Only: Most lenient - pure character recognition quality\n",
      "\n",
      "======================================================================\n",
      "\n",
      "KEY INSIGHTS:\n",
      "- Pure OCR quality (standard normalization): 8.08%\n",
      "- Letter recognition quality: 7.44%\n",
      "- Structure failures (unmatched content): 47.7%\n",
      "- Contributions:\n",
      "    Standard CER: 2.35%\n",
      "    Successfully matched: 42.0%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Quality Evaluation\n",
    "Calculate CER and WER using two complementary approaches:\n",
    "1. Order-agnostic: Pure OCR quality regardless of reading order\n",
    "2. Structure-aware: OCR quality on properly aligned content via matching\n",
    "\n",
    "Each approach calculates three normalization levels:\n",
    "- Strict: Preserves all whitespace (including \\n vs \\n\\n differences)\n",
    "- Standard: Normalizes whitespace to single spaces (RECOMMENDED)\n",
    "- Letters Only: Removes all whitespace and punctuation (pure character recognition)\n",
    "\n",
    "References:\n",
    "- Flexible Character Accuracy (FCA) for handling reading order issues:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "- Token sort ratio for order-agnostic OCR comparison:\n",
    "  https://urban-institute.medium.com/choosing-the-right-ocr-service-for-extracting-text-data-d7830399ec5\n",
    "- Unicode normalization and whitespace handling in OCR evaluation:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only,\n",
    "    token_sort_text\n",
    ")\n",
    "\n",
    "from utils.ocr_metrics import character_error_rate, word_error_rate\n",
    "\n",
    "\n",
    "def evaluate_order_agnostic(gold_items: List[Dict], pred_items: List[Dict], \n",
    "                            item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality without considering reading order.\n",
    "    Uses token sort ratio approach - sorts all words before comparison.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        item_classes: If provided, filter to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER for each normalization level, and text statistics\n",
    "    \"\"\"\n",
    "    # Filter by class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items if item['item_class'] in item_classes]\n",
    "        pred_items = [item for item in pred_items if item['item_class'] in item_classes]\n",
    "    \n",
    "    # Concatenate all text\n",
    "    gold_text = ' '.join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = ' '.join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    # Sort tokens for order-agnostic comparison\n",
    "    gold_sorted = token_sort_text(gold_text)\n",
    "    pred_sorted = token_sort_text(pred_text)\n",
    "    \n",
    "    # Calculate for all three normalization levels\n",
    "    results = {\n",
    "        'cer_strict': character_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'wer_strict': word_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'cer_standard': character_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'wer_standard': word_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'cer_letters': character_error_rate(gold_sorted, pred_sorted, 'letters_only'),\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split())\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_structure_aware(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                             matches: List[Tuple[int, int, float]],\n",
    "                             item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality on matched pairs, respecting document structure.\n",
    "    Only compares content that was successfully aligned via matching.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        item_classes: If provided, filter matches to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with matched CER/WER for each normalization level and unmatched content statistics\n",
    "    \"\"\"\n",
    "    # Filter matches by class if specified\n",
    "    if item_classes:\n",
    "        filtered_matches = filter_matches_by_class(matches, gold_items, item_classes)\n",
    "    else:\n",
    "        filtered_matches = matches\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(filtered_matches, gold_items, pred_items)\n",
    "    \n",
    "    # Calculate CER/WER on matched content for all normalization levels\n",
    "    if matched_pairs:\n",
    "        # Concatenate matched texts in gold order\n",
    "        gold_matched_text = ' '.join(gold_item.get('item_text_raw', '') \n",
    "                                     for gold_item, _, _ in matched_pairs)\n",
    "        pred_matched_text = ' '.join(pred_item.get('item_text_raw', '') \n",
    "                                     for _, pred_item, _ in matched_pairs)\n",
    "        \n",
    "        cer_strict = character_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        wer_strict = word_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        cer_standard = character_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        wer_standard = word_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        cer_letters = character_error_rate(gold_matched_text, pred_matched_text, 'letters_only')\n",
    "        \n",
    "        matched_gold_chars = len(gold_matched_text)\n",
    "        matched_pred_chars = len(pred_matched_text)\n",
    "    else:\n",
    "        cer_strict = 0.0\n",
    "        wer_strict = 0.0\n",
    "        cer_standard = 0.0\n",
    "        wer_standard = 0.0\n",
    "        cer_letters = 0.0\n",
    "        matched_gold_chars = 0\n",
    "        matched_pred_chars = 0\n",
    "    \n",
    "    # Calculate unmatched content\n",
    "    matched_gold_indices = {g_idx for g_idx, _, _ in filtered_matches}\n",
    "    matched_pred_indices = {p_idx for _, p_idx, _ in filtered_matches}\n",
    "    \n",
    "    if item_classes:\n",
    "        # Only count unmatched items of the specified classes\n",
    "        unmatched_gold_items = [\n",
    "            gold_items[i] for i in range(len(gold_items))\n",
    "            if i not in matched_gold_indices and gold_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        unmatched_pred_items = [\n",
    "            pred_items[i] for i in range(len(pred_items))\n",
    "            if i not in matched_pred_indices and pred_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                              for item in gold_items if item['item_class'] in item_classes)\n",
    "    else:\n",
    "        unmatched_gold_items = [gold_items[i] for i in range(len(gold_items)) \n",
    "                               if i not in matched_gold_indices]\n",
    "        unmatched_pred_items = [pred_items[i] for i in range(len(pred_items)) \n",
    "                               if i not in matched_pred_indices]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    unmatched_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_gold_items)\n",
    "    unmatched_pred_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_pred_items)\n",
    "    \n",
    "    return {\n",
    "        'cer_strict': cer_strict,\n",
    "        'wer_strict': wer_strict,\n",
    "        'cer_standard': cer_standard,\n",
    "        'wer_standard': wer_standard,\n",
    "        'cer_letters': cer_letters,\n",
    "        'matched_gold_chars': matched_gold_chars,\n",
    "        'matched_pred_chars': matched_pred_chars,\n",
    "        'unmatched_gold_chars': unmatched_gold_chars,\n",
    "        'unmatched_pred_chars': unmatched_pred_chars,\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'matched_percentage': (matched_gold_chars / total_gold_chars * 100) if total_gold_chars else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate text quality across all pages\n",
    "print(\"Evaluating text quality...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "order_agnostic_all = []\n",
    "order_agnostic_contrib = []\n",
    "structure_aware_all = []\n",
    "structure_aware_contrib = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Order-agnostic evaluation\n",
    "    oa_all = evaluate_order_agnostic(gold_items, pred_items)\n",
    "    oa_all['page_id'] = page_id\n",
    "    order_agnostic_all.append(oa_all)\n",
    "    \n",
    "    oa_contrib = evaluate_order_agnostic(gold_items, pred_items, \n",
    "                                         item_classes=['prose', 'verse'])\n",
    "    oa_contrib['page_id'] = page_id\n",
    "    order_agnostic_contrib.append(oa_contrib)\n",
    "    \n",
    "    # Structure-aware evaluation\n",
    "    sa_all = evaluate_structure_aware(gold_items, pred_items, matches)\n",
    "    sa_all['page_id'] = page_id\n",
    "    structure_aware_all.append(sa_all)\n",
    "    \n",
    "    sa_contrib = evaluate_structure_aware(gold_items, pred_items, matches,\n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    sa_contrib['page_id'] = page_id\n",
    "    structure_aware_contrib.append(sa_contrib)\n",
    "\n",
    "# Calculate averages for order-agnostic evaluation\n",
    "avg_oa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in order_agnostic_all) / len(order_agnostic_all)\n",
    "}\n",
    "\n",
    "contrib_with_content = [r for r in order_agnostic_contrib if r['gold_chars'] > 0]\n",
    "avg_oa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in contrib_with_content) / len(contrib_with_content)\n",
    "}\n",
    "\n",
    "# Calculate averages for structure-aware evaluation\n",
    "sa_all_with_matches = [r for r in structure_aware_all if r['matched_gold_chars'] > 0]\n",
    "avg_sa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_all_with_matches) / len(sa_all_with_matches)\n",
    "}\n",
    "\n",
    "sa_contrib_with_matches = [r for r in structure_aware_contrib if r['matched_gold_chars'] > 0]\n",
    "avg_sa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches)\n",
    "}\n",
    "\n",
    "# Calculate total matched percentages\n",
    "total_sa_all_matched = sum(r['matched_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_gold = sum(r['total_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_all)\n",
    "\n",
    "total_sa_contrib_matched = sum(r['matched_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_gold = sum(r['total_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_contrib)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORDER-AGNOSTIC EVALUATION\")\n",
    "print(\"   (Pure OCR quality, reading order irrelevant)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_strict']:.2%}  |  WER: {avg_oa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_standard']:.2%}  |  WER: {avg_oa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_letters']:.2%}\")\n",
    "\n",
    "print(f\"\\n   Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_strict']:.2%}  |  WER: {avg_oa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_standard']:.2%}  |  WER: {avg_oa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_letters']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STRUCTURE-AWARE EVALUATION\")\n",
    "print(\"   (OCR quality on matched content only)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   Matched Content - All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_strict']:.2%}  |  WER: {avg_sa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_standard']:.2%}  |  WER: {avg_sa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_all_matched:,} chars matched \" +\n",
    "      f\"({total_sa_all_matched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_all_unmatched:,} chars \" +\n",
    "      f\"({total_sa_all_unmatched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(f\"\\n   Matched Content - Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_strict']:.2%}  |  WER: {avg_sa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_standard']:.2%}  |  WER: {avg_sa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_contrib_matched:,} chars matched \" +\n",
    "      f\"({total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_contrib_unmatched:,} chars \" +\n",
    "      f\"({total_sa_contrib_unmatched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Strict: Most conservative\")\n",
    "print(\"Standard: Fair baseline - normalizes whitespace\")\n",
    "print(\"Letters Only: Most lenient - pure character recognition quality\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(f\"- Pure OCR quality (standard normalization): {avg_oa_all['cer_standard']:.2%}\")\n",
    "print(f\"- Letter recognition quality: {avg_oa_all['cer_letters']:.2%}\")\n",
    "print(f\"- Structure failures (unmatched content): {total_sa_all_unmatched/total_sa_all_gold*100:.1f}%\")\n",
    "print(f\"- Contributions:\")\n",
    "print(f\"    Standard CER: {avg_sa_contrib['cer_standard']:.2%}\")\n",
    "print(f\"    Successfully matched: {total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb143df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detailed page-by-page diagnostics...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PAGE-BY-PAGE TEXT QUALITY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "--- STRICT NORMALIZATION (preserves all whitespace) ---\n",
      "                                    page_id  cer_%  wer_%  coverage_%  subs_%  dels_%  ins_%  matched_chars  total_errors\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-001  34.04  33.97        74.7   37.28    0.45   0.00            896           338\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-002   2.06   8.75       100.0    1.54    0.61   0.00           2475            53\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-003  76.32  83.33         1.0    0.00    0.00  76.32             38            29\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-004   1.13   1.87        11.9    1.13    0.00   0.00            622             7\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-005   1.16   2.08        41.8    1.06    0.15   0.05           1981            25\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-006   7.48  31.67        35.9    7.24    0.18   0.06           1644           123\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00   0.00        64.2    0.00    0.00   0.00             43             0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-009   3.06   4.72        78.8    3.67    0.14   0.18           2780           111\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-010   1.58  13.04        82.7    0.93    0.93   0.14           4315            86\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-012   2.74   7.48        35.3    1.25    2.03   0.08           1279            43\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-013   4.29  35.25        89.9   41.74   21.41  39.17           5522          5650\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-014  24.20  20.78        84.3    2.37    0.72  21.22            971           236\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001   0.92   1.97        99.5    8.25    0.85   2.97           4449           537\n",
      "\n",
      "\n",
      "--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\n",
      "                                    page_id  cer_%  wer_%  coverage_%  subs_%  dels_%  ins_%  matched_chars  total_errors\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-001  33.56  33.97        74.7   36.61    0.45   0.00            896           332\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-002   2.06   8.75       100.0    1.54    0.61   0.00           2475            53\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-003  76.32  83.33         1.0    0.00    0.00  76.32             38            29\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-004   1.13   1.87        11.9    1.13    0.00   0.00            622             7\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-005   1.11   2.08        41.8    1.06    0.15   0.00           1981            24\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-006   7.55  31.67        35.9    7.24    0.18   0.06           1644           123\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00   0.00        64.2    0.00    0.00   0.00             43             0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-009   1.96   4.72        78.8    1.91    0.14   0.14           2780            61\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-010   1.49  13.04        82.7    0.88    1.71   0.97           4315           154\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-012   2.61   7.48        35.3    1.25    1.88   0.08           1279            41\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-013   4.23  35.25        89.9   83.19   13.33  52.97           5522          8255\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-014  23.65  20.78        84.3    2.37    0.00  21.22            971           229\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001   0.92   1.97        99.5    8.25    0.85   2.97           4449           537\n",
      "\n",
      "\n",
      "--- LETTERS ONLY (no whitespace/punctuation) ---\n",
      "                                    page_id  cer_%  coverage_%  subs_%  dels_%  ins_%\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-001  36.95        74.7   11.61    0.00  17.41\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-002   1.26       100.0    0.16    0.28   0.65\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-003  75.00         1.0    0.00    0.00  63.16\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-004   1.44        11.9    1.13    0.00   0.00\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-005   1.40        41.8    1.06    0.15   0.00\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-006   5.04        35.9    0.73    0.00   3.16\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00        64.2    0.00    0.00   0.00\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00         0.0    0.00    0.00   0.00\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-009   2.17        78.8    1.83    0.04   0.00\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-010   0.61        82.7    0.39    0.70   0.72\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00         0.0    0.00    0.00   0.00\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-012   2.31        35.3    0.55    1.25   0.08\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-013   2.03        89.9    3.24    0.04   0.11\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-014  22.14        84.3    1.75    0.00  14.11\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001   0.84        99.5    0.25    0.09   0.40\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WORST PERFORMING PAGES (Standard Normalization)\n",
      "================================================================================\n",
      "\n",
      "1. La_Plume_bpt6k1185893k_1_10_1889__page-003\n",
      "   CER: 76.32%  |  WER: 83.33%\n",
      "   Coverage: 1.0% of gold text\n",
      "   Errors: 0 subs, 0 dels, 29 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 76.32%):\n",
      "         Gold: \"LA PLUME\n",
      "Revue Littéraire & Artistique\"\n",
      "         Pred: \"LA PLUME\n",
      "Revue Littéraire & Artistique\n",
      "NUMÉRO 10\n",
      "1er SEPTEMBRE 1889\"\n",
      "\n",
      "2. La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "   CER: 33.56%  |  WER: 33.97%\n",
      "   Coverage: 74.7% of gold text\n",
      "   Errors: 328 subs, 4 dels, 0 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 33.56%):\n",
      "         Gold: \"SOMMAIRE :\n",
      "\n",
      "Texte :\n",
      "Léon DESCHAMPS : Léon Vanier. — Paul VERLAINE : Les Ingénus. — Stéphane MALLARMÉ...\"\n",
      "         Pred: \"ABONNEMENTS : 5 FRANCS PAR AN\n",
      "LE NUMÉRO : 25 CENTIMES\n",
      "\n",
      "LA PLUME\n",
      "Revue Littéraire & Artistique\n",
      "BI-MEN...\"\n",
      "\n",
      "3. La_Plume_bpt6k1185893k_1_10_1889__page-014\n",
      "   CER: 23.65%  |  WER: 20.78%\n",
      "   Coverage: 84.3% of gold text\n",
      "   Errors: 23 subs, 0 dels, 206 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 45.18%):\n",
      "         Gold: \"ABONNEMENTS :\n",
      "Un An.... : Cinq francs.\n",
      "Six Mois : Trois francs.\n",
      "\n",
      "Les abonnements partent du 1er de c...\"\n",
      "         Pred: \"ABONNEMENTS :\n",
      "Un An..........: Cinq francs.\n",
      "Six Mois :....: Trois francs.\n",
      "Les abonnements partent du...\"\n",
      "      Item 2 (ad, CER: 7.74%):\n",
      "         Gold: \"En vente aux bureaux de LA PLUME\n",
      "\n",
      "ŒUVRES DE LÉON DESCHAMPS\n",
      "\n",
      "A LA GUEULE DU MONSTRE, poésies, un vol....\"\n",
      "         Pred: \"En vente aux bureaux de LA PLUME\n",
      "ŒUVRES DE LÉON DESCHAMPS\n",
      "A LA GUEULE DU MONSTRE, poésies, un vol. i...\"\n",
      "\n",
      "4. La_Plume_bpt6k1185893k_1_10_1889__page-006\n",
      "   CER: 7.55%  |  WER: 31.67%\n",
      "   Coverage: 35.9% of gold text\n",
      "   Errors: 119 subs, 3 dels, 1 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (verse, CER: 10.68%):\n",
      "         Gold: \"GUITARE\n",
      "\n",
      "Pourquoi dormir, ô ma conquête ?\n",
      "Pourquoi rêver sans ton amant ?\n",
      "Est-ce à l'amour, à quelqu...\"\n",
      "         Pred: \"GUITARE\\n\\nPourquoi dormir, ô ma conquête ?\\nPourquoi rêver sans ton amant ?\\nEst-ce à l'amour, à qu...\"\n",
      "      Item 2 (verse, CER: 6.42%):\n",
      "         Gold: \"APPEL\n",
      "\n",
      "A la Jeunesse.\n",
      "\n",
      "Amis, ne laissez pas énerver vos courages\n",
      "Sous les baisers des voluptés,\n",
      "Cons...\"\n",
      "         Pred: \"APPEL\\n\\nA la Jeunesse.\\n\\nAmis, ne laissez pas énerver vos courage\\nSous les baisers des voluptés,\\...\"\n",
      "\n",
      "5. La_Plume_bpt6k1185893k_1_10_1889__page-013\n",
      "   CER: 4.23%  |  WER: 35.25%\n",
      "   Coverage: 89.9% of gold text\n",
      "   Errors: 4594 subs, 736 dels, 2925 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 6.90%):\n",
      "         Gold: \"Numéro 5. — Rodolphe Salis (Francisque Sarcey) ; Concerto (Albert Tinchant) ; Libération (Léon Gandi...\"\n",
      "         Pred: \"Numéro 5.— Rodolphe Salis (Françoise Sarcey); Concerto (Albert Tinchant); Libération (Léon Gandillot...\"\n",
      "      Item 2 (paratext, CER: 4.66%):\n",
      "         Gold: \"Numéro 9. — Albert Tinchant (Maurice Isabey ; « Circenses » (Edmond Porcher) ; Quatre Sonnets (Alber...\"\n",
      "         Pred: \"Numéro 9.— Albert Tinchant (Maurice Isabey); « Circenses » (Edmond Porcher); Quatre Sonnets (Albert ...\"\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ERROR TYPE DISTRIBUTION (Standard Normalization)\n",
      "================================================================================\n",
      "\n",
      "Total errors across all pages: 9,845\n",
      "   Substitutions: 5,604 (56.9%)\n",
      "   Deletions:     901 (9.2%)\n",
      "   Insertions:    3,340 (33.9%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Average CER (standard): 10.44%\n",
      "- Pages with CER > 20%: 3\n",
      "- Pages with CER < 5%: 11\n",
      "- Most common error type: Substitutions\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-by-Page Text Diagnostics\n",
    "Detailed error analysis for each page with three normalization levels.\n",
    "Shows error type distribution, worst performing pages, and actual text examples.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def get_levenshtein_operations(reference: str, hypothesis: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get detailed Levenshtein operations breakdown.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with counts of substitutions, deletions, insertions\n",
    "    \"\"\"\n",
    "    if not reference and not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': 0, 'total': 0}\n",
    "    \n",
    "    if not reference:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': len(hypothesis), 'total': len(hypothesis)}\n",
    "    \n",
    "    if not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': len(reference), 'insertions': 0, 'total': len(reference)}\n",
    "    \n",
    "    # Use SequenceMatcher to get operations\n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    substitutions = 0\n",
    "    deletions = 0\n",
    "    insertions = 0\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Both strings differ - count as substitutions\n",
    "            substitutions += max(i2 - i1, j2 - j1)\n",
    "        elif tag == 'delete':\n",
    "            # Only in reference\n",
    "            deletions += (i2 - i1)\n",
    "        elif tag == 'insert':\n",
    "            # Only in hypothesis\n",
    "            insertions += (j2 - j1)\n",
    "    \n",
    "    return {\n",
    "        'substitutions': substitutions,\n",
    "        'deletions': deletions,\n",
    "        'insertions': insertions,\n",
    "        'total': substitutions + deletions + insertions\n",
    "    }\n",
    "\n",
    "\n",
    "def diagnose_page_text_quality(page: Dict, normalization: str = 'standard') -> Dict:\n",
    "    \"\"\"\n",
    "    Detailed text quality diagnosis for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page: Page data from all_pages\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detailed metrics and error breakdowns\n",
    "    \"\"\"\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "        return {\n",
    "            'page_id': page_id,\n",
    "            'cer': 0.0,\n",
    "            'wer': 0.0,\n",
    "            'matched_chars': 0,\n",
    "            'total_gold_chars': total_gold_chars,\n",
    "            'match_coverage': 0.0,\n",
    "            'substitutions': 0,\n",
    "            'deletions': 0,\n",
    "            'insertions': 0,\n",
    "            'total_errors': 0,\n",
    "            'items_analyzed': []\n",
    "        }\n",
    "    \n",
    "    # Concatenate matched text\n",
    "    gold_text = ' '.join(gold_item.get('item_text_raw', '') for gold_item, _, _ in matched_pairs)\n",
    "    pred_text = ' '.join(pred_item.get('item_text_raw', '') for _, pred_item, _ in matched_pairs)\n",
    "    \n",
    "    # Calculate CER/WER\n",
    "    cer = character_error_rate(gold_text, pred_text, normalization)\n",
    "    wer = word_error_rate(gold_text, pred_text, normalization)\n",
    "    \n",
    "    # Get error breakdown using normalized text\n",
    "    if normalization == 'strict':\n",
    "        gold_norm = normalize_text_strict(gold_text)\n",
    "        pred_norm = normalize_text_strict(pred_text)\n",
    "    elif normalization == 'standard':\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "    else:  # letters_only\n",
    "        gold_norm = normalize_text_letters_only(gold_text)\n",
    "        pred_norm = normalize_text_letters_only(pred_text)\n",
    "    \n",
    "    ops = get_levenshtein_operations(gold_norm, pred_norm)\n",
    "    \n",
    "    # Analyze individual items\n",
    "    items_analyzed = []\n",
    "    for gold_item, pred_item, similarity in matched_pairs:\n",
    "        gold_item_text = gold_item.get('item_text_raw', '')\n",
    "        pred_item_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        item_cer = character_error_rate(gold_item_text, pred_item_text, normalization)\n",
    "        \n",
    "        items_analyzed.append({\n",
    "            'gold_class': gold_item.get('item_class'),\n",
    "            'cer': item_cer,\n",
    "            'gold_preview': gold_item_text[:100],\n",
    "            'pred_preview': pred_item_text[:100],\n",
    "            'gold_length': len(gold_item_text),\n",
    "            'pred_length': len(pred_item_text)\n",
    "        })\n",
    "    \n",
    "    total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'cer': cer,\n",
    "        'wer': wer,\n",
    "        'matched_chars': len(gold_text),\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'match_coverage': len(gold_text) / total_gold_chars * 100 if total_gold_chars > 0 else 0,\n",
    "        'substitutions': ops['substitutions'],\n",
    "        'deletions': ops['deletions'],\n",
    "        'insertions': ops['insertions'],\n",
    "        'total_errors': ops['total'],\n",
    "        'items_analyzed': items_analyzed\n",
    "    }\n",
    "\n",
    "\n",
    "# Diagnose all pages for all three normalizations\n",
    "print(\"Running detailed page-by-page diagnostics...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_diagnostics_strict = []\n",
    "page_diagnostics_standard = []\n",
    "page_diagnostics_letters = []\n",
    "\n",
    "for page in all_pages:\n",
    "    diag_strict = diagnose_page_text_quality(page, 'strict')\n",
    "    page_diagnostics_strict.append(diag_strict)\n",
    "    \n",
    "    diag_standard = diagnose_page_text_quality(page, 'standard')\n",
    "    page_diagnostics_standard.append(diag_standard)\n",
    "    \n",
    "    diag_letters = diagnose_page_text_quality(page, 'letters_only')\n",
    "    page_diagnostics_letters.append(diag_letters)\n",
    "\n",
    "# Create summary DataFrames\n",
    "def create_summary_df(diagnostics, normalization_name):\n",
    "    \"\"\"Create summary DataFrame from diagnostics.\"\"\"\n",
    "    data = []\n",
    "    for d in diagnostics:\n",
    "        if d['matched_chars'] > 0:\n",
    "            sub_pct = d['substitutions'] / d['matched_chars'] * 100\n",
    "            del_pct = d['deletions'] / d['matched_chars'] * 100\n",
    "            ins_pct = d['insertions'] / d['matched_chars'] * 100\n",
    "        else:\n",
    "            sub_pct = del_pct = ins_pct = 0\n",
    "        \n",
    "        data.append({\n",
    "            'page_id': d['page_id'],\n",
    "            'cer_%': round(d['cer'] * 100, 2),\n",
    "            'wer_%': round(d['wer'] * 100, 2),\n",
    "            'coverage_%': round(d['match_coverage'], 1),\n",
    "            'subs_%': round(sub_pct, 2),\n",
    "            'dels_%': round(del_pct, 2),\n",
    "            'ins_%': round(ins_pct, 2),\n",
    "            'matched_chars': d['matched_chars'],\n",
    "            'total_errors': d['total_errors']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_strict = create_summary_df(page_diagnostics_strict, 'Strict')\n",
    "df_standard = create_summary_df(page_diagnostics_standard, 'Standard')\n",
    "df_letters = create_summary_df(page_diagnostics_letters, 'Letters Only')\n",
    "\n",
    "# Print summary tables\n",
    "print(\"=\"*80)\n",
    "print(\"PAGE-BY-PAGE TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- STRICT NORMALIZATION (preserves all whitespace) ---\")\n",
    "print(df_strict.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\")\n",
    "print(df_standard.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- LETTERS ONLY (no whitespace/punctuation) ---\")\n",
    "print(df_letters[['page_id', 'cer_%', 'coverage_%', 'subs_%', 'dels_%', 'ins_%']].to_string(index=False))\n",
    "\n",
    "# Identify worst pages (using standard normalization)\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"WORST PERFORMING PAGES (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "worst_pages = sorted(page_diagnostics_standard, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "for i, page_diag in enumerate(worst_pages, 1):\n",
    "    print(f\"\\n{i}. {page_diag['page_id']}\")\n",
    "    print(f\"   CER: {page_diag['cer']:.2%}  |  WER: {page_diag['wer']:.2%}\")\n",
    "    print(f\"   Coverage: {page_diag['match_coverage']:.1f}% of gold text\")\n",
    "    print(f\"   Errors: {page_diag['substitutions']} subs, {page_diag['deletions']} dels, {page_diag['insertions']} ins\")\n",
    "    \n",
    "    # Show worst items from this page\n",
    "    if page_diag['items_analyzed']:\n",
    "        worst_items = sorted(page_diag['items_analyzed'], key=lambda x: x['cer'], reverse=True)[:2]\n",
    "        print(f\"\\n   Worst items on this page:\")\n",
    "        for j, item in enumerate(worst_items, 1):\n",
    "            print(f\"      Item {j} ({item['gold_class']}, CER: {item['cer']:.2%}):\")\n",
    "            print(f\"         Gold: \\\"{item['gold_preview']}{'...' if item['gold_length'] > 100 else ''}\\\"\")\n",
    "            print(f\"         Pred: \\\"{item['pred_preview']}{'...' if item['pred_length'] > 100 else ''}\\\"\")\n",
    "\n",
    "# Error distribution analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ERROR TYPE DISTRIBUTION (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_errors = sum(d['total_errors'] for d in page_diagnostics_standard)\n",
    "total_subs = sum(d['substitutions'] for d in page_diagnostics_standard)\n",
    "total_dels = sum(d['deletions'] for d in page_diagnostics_standard)\n",
    "total_ins = sum(d['insertions'] for d in page_diagnostics_standard)\n",
    "\n",
    "print(f\"\\nTotal errors across all pages: {total_errors:,}\")\n",
    "print(f\"   Substitutions: {total_subs:,} ({total_subs/total_errors*100:.1f}%)\")\n",
    "print(f\"   Deletions:     {total_dels:,} ({total_dels/total_errors*100:.1f}%)\")\n",
    "print(f\"   Insertions:    {total_ins:,} ({total_ins/total_errors*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Average CER (standard): {df_standard['cer_%'].mean():.2f}%\")\n",
    "print(f\"- Pages with CER > 20%: {len(df_standard[df_standard['cer_%'] > 20])}\")\n",
    "print(f\"- Pages with CER < 5%: {len(df_standard[df_standard['cer_%'] < 5])}\")\n",
    "print(f\"- Most common error type: \" + \n",
    "      (\"Substitutions\" if total_subs > max(total_dels, total_ins) else \n",
    "       \"Deletions\" if total_dels > total_ins else \"Insertions\"))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b21705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing character-level confusions across all pages...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CHARACTER CONFUSION MATRIX\n",
      "================================================================================\n",
      "\n",
      "Total character substitutions: 154\n",
      "Unique confusion pairs: 89\n",
      "\n",
      "Top 30 Most Common Character Substitutions:\n",
      "Gold → Pred                    Count     \n",
      "--------------------------------------------------------------------------------\n",
      "\" \" → \"\\n\"                     36        \n",
      "\" \" → \"\\n\\n\"                   18        \n",
      "\" ; E\" → \"; É\"                 9         \n",
      "'E' → 'É'                      2         \n",
      "'î' → 'i'                      2         \n",
      "'a' → 'à'                      2         \n",
      "\"t) \" → \"l)\"                   2         \n",
      "\".)\" → \")\"                     2         \n",
      "\"SOMMAIRE :\" → \"ABONNEMENTS : 5 FRANCS PAR AN LE NUMÉRO : 25 CENTIMES LA PLUME Revue Littéraire & Artistique BI-MENSUELLE NUMÉRO EXCEPTIONNEL CONSACRÉ AUX MODERNES Rédacteur en chef de ce numéro : Léon VANIER. SOMMAIRE\" 1         \n",
      "\"ance\" → \"ir\"                  1         \n",
      "'C' → 'G'                      1         \n",
      "\"Edouard MANET et LUNEL.\" → \"Édouard MANET et LUNEL. RÉDACTION ET ADMINISTRATION 36, Boulevard Arago, 36 PARIS Directeur de la Revue : Léon DESCHAMPS\" 1         \n",
      "\"l\" → \"att\"                    1         \n",
      "'ê' → 'é'                      1         \n",
      "\"eus\" → \"èr\"                   1         \n",
      "\"el\" → \"luch\"                  1         \n",
      "\"songe\" → \"chanson\"            1         \n",
      "\"tter\" → \"lté\"                 1         \n",
      "\"oint\" → \"as\"                  1         \n",
      "'û' → 'u'                      1         \n",
      "'t' → 'l'                      1         \n",
      "\"éon\" → \"ÉON\"                  1         \n",
      "\"anier\" → \"ANIER\"              1         \n",
      "\"s \" → \"\\n\"                    1         \n",
      "\"ueux\" → \"nheurs\"              1         \n",
      "\"ustave\" → \"USTAVE\"            1         \n",
      "\"ivet\" → \"IVET\"                1         \n",
      "\"es\" → \"é\"                     1         \n",
      "'ç' → 'c'                      1         \n",
      "'e' → 'o'                      1         \n",
      "\n",
      "\n",
      "================================================================================\n",
      "SYSTEMATIC ERROR PATTERNS\n",
      "================================================================================\n",
      "\n",
      "Total confusions: 154\n",
      "Categorized: 114 (74.0%)\n",
      "Uncategorized: 40 (26.0%)\n",
      "\n",
      "Pattern Breakdown:\n",
      "   Space Issues                 100 ( 64.9%)\n",
      "   Accent Removal                 5 (  3.2%)\n",
      "   Case Errors                    5 (  3.2%)\n",
      "   Punctuation Errors             2 (  1.3%)\n",
      "   Accent Confusion               1 (  0.6%)\n",
      "   Ligature Issues                1 (  0.6%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ACCENT & DIACRITIC ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Accented character confusions: 6\n",
      "\n",
      "Most common accented character errors:\n",
      "   'î' → 'i': 2 times\n",
      "   'ê' → 'é': 1 times\n",
      "   'û' → 'u': 1 times\n",
      "   'ç' → 'c': 1 times\n",
      "   'é' → 'e': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LIGATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Ligature-related confusions: 1\n",
      "\n",
      "Ligature substitutions:\n",
      "   'œ' → 'oe': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CASE SENSITIVITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Case-only differences: 5\n",
      "\n",
      "Most common case errors:\n",
      "   'c' → 'C': 1 times\n",
      "   'P' → 'p': 1 times\n",
      "   'M' → 'm': 1 times\n",
      "   'D' → 'd': 1 times\n",
      "   'F' → 'f': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Based on the error analysis:\n",
      "\n",
      "⚠ CASE SENSITIVITY ISSUES (3.2%)\n",
      "   - Model confusing upper/lowercase\n",
      "   - May indicate line/title detection problems\n",
      "\n",
      "✓ ERROR DIVERSITY IS HIGH (unique ratio: 0.58)\n",
      "   - Errors are diverse, not systematic\n",
      "   - Suggests random OCR noise rather than systematic bias\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cross-Page Error Analysis\n",
    "Character-level confusion matrix and systematic error pattern detection.\n",
    "Analyzes all pages together to identify recurring OCR issues.\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def get_character_confusions(reference: str, hypothesis: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract character-level substitutions from aligned strings.\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_char, pred_char) tuples for substitutions\n",
    "    \"\"\"\n",
    "    confusions = []\n",
    "    \n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Character substitution\n",
    "            gold_substr = reference[i1:i2]\n",
    "            pred_substr = hypothesis[j1:j2]\n",
    "            \n",
    "            # For single character replacements\n",
    "            if len(gold_substr) == 1 and len(pred_substr) == 1:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "            # For multi-character replacements (like œ -> oe)\n",
    "            elif len(gold_substr) > 0 and len(pred_substr) > 0:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "    \n",
    "    return confusions\n",
    "\n",
    "\n",
    "def analyze_character_patterns(confusions: list) -> dict:\n",
    "    \"\"\"\n",
    "    Detect systematic patterns in character confusions.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with pattern names and counts\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'accent_removal': 0,\n",
    "        'accent_confusion': 0,\n",
    "        'ligature_issues': 0,\n",
    "        'case_errors': 0,\n",
    "        'punctuation_errors': 0,\n",
    "        'similar_shape': 0,\n",
    "        'space_issues': 0\n",
    "    }\n",
    "    \n",
    "    accent_chars = 'àáâãäåèéêëìíîïòóôõöùúûüýÿñçÀÁÂÃÄÅÈÉÊËÌÍÎÏÒÓÔÕÖÙÚÛÜÝŸÑÇ'\n",
    "    ligatures = 'œæŒÆ'\n",
    "    \n",
    "    for gold, pred in confusions:\n",
    "        # Accent removal (é -> e, à -> a)\n",
    "        if len(gold) == 1 and len(pred) == 1:\n",
    "            gold_base = unicodedata.normalize('NFD', gold)[0]\n",
    "            pred_normalized = unicodedata.normalize('NFD', pred)[0]\n",
    "            if gold in accent_chars and gold_base == pred:\n",
    "                patterns['accent_removal'] += 1\n",
    "            elif gold in accent_chars and pred in accent_chars and gold != pred:\n",
    "                patterns['accent_confusion'] += 1\n",
    "            elif gold.lower() == pred.lower():\n",
    "                patterns['case_errors'] += 1\n",
    "        \n",
    "        # Ligature issues (œ -> oe, æ -> ae)\n",
    "        if gold in ligatures and pred not in ligatures:\n",
    "            patterns['ligature_issues'] += 1\n",
    "        \n",
    "        # Similar shape confusions (common OCR errors)\n",
    "        similar_pairs = [\n",
    "            ('l', 'i'), ('i', 'l'), ('rn', 'm'), ('m', 'rn'),\n",
    "            ('cl', 'd'), ('d', 'cl'), ('o', '0'), ('0', 'o'),\n",
    "            ('1', 'l'), ('l', '1'), ('s', '5'), ('5', 's')\n",
    "        ]\n",
    "        if (gold, pred) in similar_pairs:\n",
    "            patterns['similar_shape'] += 1\n",
    "        \n",
    "        # Punctuation confusion\n",
    "        if gold in '.,;:!?\\'\"' or pred in '.,;:!?\\'\"':\n",
    "            patterns['punctuation_errors'] += 1\n",
    "        \n",
    "        # Space-related issues\n",
    "        if ' ' in gold or ' ' in pred:\n",
    "            patterns['space_issues'] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Collect all character confusions across all pages\n",
    "print(\"Analyzing character-level confusions across all pages...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_confusions = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        pred_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        # Use standard normalization for fair comparison\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "        \n",
    "        confusions = get_character_confusions(gold_norm, pred_norm)\n",
    "        all_confusions.extend(confusions)\n",
    "\n",
    "# Count confusion frequencies\n",
    "confusion_counter = Counter(all_confusions)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHARACTER CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal character substitutions: {len(all_confusions):,}\")\n",
    "print(f\"Unique confusion pairs: {len(confusion_counter):,}\")\n",
    "\n",
    "# Top 30 most common confusions\n",
    "print(\"\\nTop 30 Most Common Character Substitutions:\")\n",
    "print(f\"{'Gold → Pred':<30} {'Count':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for (gold, pred), count in confusion_counter.most_common(30):\n",
    "    # Escape special characters for display\n",
    "    gold_display = repr(gold)[1:-1] if gold in '\\n\\t\\r' else gold\n",
    "    pred_display = repr(pred)[1:-1] if pred in '\\n\\t\\r' else pred\n",
    "    \n",
    "    # Create display string\n",
    "    if len(gold) == 1 and len(pred) == 1:\n",
    "        display = f\"'{gold_display}' → '{pred_display}'\"\n",
    "    else:\n",
    "        display = f'\"{gold_display}\" → \"{pred_display}\"'\n",
    "    \n",
    "    print(f\"{display:<30} {count:<10}\")\n",
    "\n",
    "# Pattern analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC ERROR PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "patterns = analyze_character_patterns(all_confusions)\n",
    "total_categorized = sum(patterns.values())\n",
    "\n",
    "print(f\"\\nTotal confusions: {len(all_confusions):,}\")\n",
    "print(f\"Categorized: {total_categorized:,} ({total_categorized/len(all_confusions)*100:.1f}%)\")\n",
    "print(f\"Uncategorized: {len(all_confusions) - total_categorized:,} \" +\n",
    "      f\"({(len(all_confusions) - total_categorized)/len(all_confusions)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPattern Breakdown:\")\n",
    "for pattern, count in sorted(patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        pct = count / len(all_confusions) * 100\n",
    "        pattern_name = pattern.replace('_', ' ').title()\n",
    "        print(f\"   {pattern_name:<25} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Specific accent analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ACCENT & DIACRITIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accent_confusions = [(g, p) for g, p in all_confusions \n",
    "                     if len(g) == 1 and len(p) == 1 \n",
    "                     and any(c in 'àáâãäåèéêëìíîïòóôõöùúûüýÿñçÀÁÂÃÄÅÈÉÊËÌÍÎÏÒÓÔÕÖÙÚÛÜÝŸÑÇ' for c in g)]\n",
    "\n",
    "if accent_confusions:\n",
    "    accent_counter = Counter(accent_confusions)\n",
    "    print(f\"\\nAccented character confusions: {len(accent_confusions):,}\")\n",
    "    print(\"\\nMost common accented character errors:\")\n",
    "    for (gold, pred), count in accent_counter.most_common(15):\n",
    "        print(f\"   '{gold}' → '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo accented character confusions detected.\")\n",
    "\n",
    "# Ligature analysis  \n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"LIGATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ligature_confusions = [(g, p) for g, p in all_confusions if g in 'œæŒÆ' or p in 'œæŒÆ']\n",
    "\n",
    "if ligature_confusions:\n",
    "    ligature_counter = Counter(ligature_confusions)\n",
    "    print(f\"\\nLigature-related confusions: {len(ligature_confusions):,}\")\n",
    "    print(\"\\nLigature substitutions:\")\n",
    "    for (gold, pred), count in ligature_counter.most_common(10):\n",
    "        print(f\"   '{gold}' → '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo ligature confusions detected.\")\n",
    "\n",
    "# Case sensitivity analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CASE SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "case_confusions = [(g, p) for g, p in all_confusions \n",
    "                   if len(g) == 1 and len(p) == 1 and g.lower() == p.lower() and g != p]\n",
    "\n",
    "if case_confusions:\n",
    "    case_counter = Counter(case_confusions)\n",
    "    print(f\"\\nCase-only differences: {len(case_confusions):,}\")\n",
    "    print(\"\\nMost common case errors:\")\n",
    "    for (gold, pred), count in case_counter.most_common(10):\n",
    "        print(f\"   '{gold}' → '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo case-only confusions detected.\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBased on the error analysis:\")\n",
    "\n",
    "# Check for high accent issues\n",
    "accent_pct = patterns['accent_removal'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if accent_pct > 5:\n",
    "    print(f\"\\n⚠ HIGH ACCENT REMOVAL RATE ({accent_pct:.1f}%)\")\n",
    "    print(\"   - Consider post-processing to restore accents using dictionary lookup\")\n",
    "    print(\"   - May need model fine-tuning on accented French text\")\n",
    "\n",
    "# Check for ligature issues\n",
    "ligature_pct = patterns['ligature_issues'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if ligature_pct > 2:\n",
    "    print(f\"\\n⚠ LIGATURE HANDLING ISSUES ({ligature_pct:.1f}%)\")\n",
    "    print(\"   - Ligatures (œ, æ) being split or confused\")\n",
    "    print(\"   - Common in historical French texts\")\n",
    "\n",
    "# Check for case errors\n",
    "case_pct = patterns['case_errors'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if case_pct > 3:\n",
    "    print(f\"\\n⚠ CASE SENSITIVITY ISSUES ({case_pct:.1f}%)\")\n",
    "    print(\"   - Model confusing upper/lowercase\")\n",
    "    print(\"   - May indicate line/title detection problems\")\n",
    "\n",
    "# General observation\n",
    "if len(all_confusions) > 0:\n",
    "    unique_ratio = len(confusion_counter) / len(all_confusions)\n",
    "    if unique_ratio > 0.5:\n",
    "        print(f\"\\n✓ ERROR DIVERSITY IS HIGH (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Errors are diverse, not systematic\")\n",
    "        print(\"   - Suggests random OCR noise rather than systematic bias\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ ERROR CONCENTRATION DETECTED (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Same errors repeat frequently\")\n",
    "        print(\"   - Suggests systematic model bias that could be corrected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating classification accuracy...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OVERALL CLASSIFICATION METRICS\n",
      "================================================================================\n",
      "\n",
      "Total matched items evaluated: 39\n",
      "Correctly classified: 37 (94.87%)\n",
      "Misclassified: 2 (5.13%)\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "--------------------------------------------------------------------------------\n",
      "                 prose     verse        ad  paratext   unknown\n",
      "--------------------------------------------------------------------------------\n",
      "       prose         8         0         0         0         0\n",
      "       verse         0         8         0         0         0\n",
      "          ad         0         0         0         1         0\n",
      "    paratext         1         0         0        21         0\n",
      "     unknown         0         0         0         0         0\n",
      "\n",
      "\n",
      "PER-CLASS METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "Class        Precision    Recall       F1-Score     Support     \n",
      "--------------------------------------------------------------------------------\n",
      "prose        88.89%       100.00%      0.941        8           \n",
      "verse        100.00%      100.00%      1.000        8           \n",
      "ad           0.00%        0.00%        0.000        1           \n",
      "paratext     95.45%       95.45%       0.955        22          \n",
      "unknown      0.00%        0.00%        0.000        0           \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg    56.87%       59.09%       0.579        39          \n",
      "Weighted Avg 92.59%       94.87%       0.937        39          \n",
      "\n",
      "\n",
      "MOST COMMON MISCLASSIFICATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "Gold → Predicted               Count      % of Gold Class\n",
      "--------------------------------------------------------------------------------\n",
      "ad → paratext             1          100.0%\n",
      "paratext → prose                1          4.5%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTRIBUTIONS ANALYSIS (Prose + Verse)\n",
      "================================================================================\n",
      "\n",
      "Total contribution items: 16\n",
      "Correctly classified: 16 (100.00%)\n",
      "Misclassified: 0 (0.00%)\n",
      "\n",
      "Contributions Confusion Matrix:\n",
      "                 prose     verse\n",
      "----------------------------------------\n",
      "       prose         8         0\n",
      "       verse         0         8\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PER-PAGE CLASSIFICATION BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "                                    page_id  total_items  correct  accuracy_%  misclassified\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-001            1        1       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-002            2        1        50.0              1\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-003            1        1       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-004            1        1       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-005            1        1       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-006            3        3       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-007            1        1       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-008            0        0         0.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-009            4        4       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-010            3        3       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-011            0        0         0.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-012            7        7       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-013           10       10       100.0              0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-014            3        2        66.7              1\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001            2        2       100.0              0\n",
      "\n",
      "\n",
      "DETAILED PER-PAGE CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 50.00% (1/2)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/2 correct\n",
      "\n",
      "Misclassifications (1):\n",
      "   paratext → prose: 1 time\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   verse        1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   prose        1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (3/3)\n",
      "\n",
      "Class distribution:\n",
      "   prose        1/1 correct\n",
      "   verse        2/2 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (4/4)\n",
      "\n",
      "Class distribution:\n",
      "   prose        2/2 correct\n",
      "   verse        2/2 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (3/3)\n",
      "\n",
      "Class distribution:\n",
      "   prose        2/2 correct\n",
      "   verse        1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (7/7)\n",
      "\n",
      "Class distribution:\n",
      "   prose        1/1 correct\n",
      "   verse        2/2 correct\n",
      "   paratext     4/4 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (10/10)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     10/10 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 66.67% (2/3)\n",
      "\n",
      "Class distribution:\n",
      "   ad           0/1 correct\n",
      "   paratext     2/2 correct\n",
      "\n",
      "Misclassifications (1):\n",
      "   ad → paratext: 1 time\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (2/2)\n",
      "\n",
      "Class distribution:\n",
      "   prose        1/1 correct\n",
      "   paratext     1/1 correct\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Overall classification accuracy: 94.87%\n",
      "- Best performing class: prose\n",
      "- Most challenging class: ad\n",
      "- Most common confusion: ad → paratext (1 times)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classification Accuracy Evaluation\n",
    "Evaluate how well the model classifies items into the five categories:\n",
    "prose, verse, ad, paratext, unknown\n",
    "\n",
    "Structure:\n",
    "1. Overall classification metrics across all pages\n",
    "2. Per-page classification breakdown\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_classification(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                           matches: List[Tuple[int, int, float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate classification accuracy on matched pairs.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Dict with classification metrics\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return {\n",
    "            'gold_classes': [],\n",
    "            'pred_classes': [],\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'accuracy': 0.0\n",
    "        }\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    gold_classes = []\n",
    "    pred_classes = []\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_classes.append(gold_item['item_class'])\n",
    "        pred_classes.append(pred_item['item_class'])\n",
    "    \n",
    "    correct = sum(1 for g, p in zip(gold_classes, pred_classes) if g == p)\n",
    "    total = len(gold_classes)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'gold_classes': gold_classes,\n",
    "        'pred_classes': pred_classes,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Collect classification data from all pages\n",
    "print(\"Evaluating classification accuracy...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_gold_classes = []\n",
    "all_pred_classes = []\n",
    "page_classification_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    result = evaluate_classification(gold_items, pred_items, matches)\n",
    "    result['page_id'] = page_id\n",
    "    page_classification_results.append(result)\n",
    "    \n",
    "    all_gold_classes.extend(result['gold_classes'])\n",
    "    all_pred_classes.extend(result['pred_classes'])\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_matched = len(all_gold_classes)\n",
    "total_correct = sum(1 for g, p in zip(all_gold_classes, all_pred_classes) if g == p)\n",
    "overall_accuracy = total_correct / total_matched if total_matched > 0 else 0.0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL CLASSIFICATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal matched items evaluated: {total_matched}\")\n",
    "print(f\"Correctly classified: {total_correct} ({overall_accuracy:.2%})\")\n",
    "print(f\"Misclassified: {total_matched - total_correct} ({(1-overall_accuracy):.2%})\")\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['prose', 'verse', 'ad', 'paratext', 'unknown']\n",
    "\n",
    "# Confusion matrix\n",
    "if total_matched > 0:\n",
    "    cm = confusion_matrix(all_gold_classes, all_pred_classes, labels=class_labels)\n",
    "    \n",
    "    print(\"\\n\\nCONFUSION MATRIX\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'':>12}\", end='')\n",
    "    for label in class_labels:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, label in enumerate(class_labels):\n",
    "        print(f\"{label:>12}\", end='')\n",
    "        for j in range(len(class_labels)):\n",
    "            print(f\"{cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\n\\nPER-CLASS METRICS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, label in enumerate(class_labels):\n",
    "        # Calculate metrics for this class\n",
    "        tp = cm[i][i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        support = cm[i, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"{label:<12} {precision:<12.2%} {recall:<12.2%} {f1:<12.3f} {support:<12}\")\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    supports = []\n",
    "    \n",
    "    for i in range(len(class_labels)):\n",
    "        tp = cm[i][i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        support = cm[i, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        supports.append(support)\n",
    "    \n",
    "    macro_precision = np.mean(precisions)\n",
    "    macro_recall = np.mean(recalls)\n",
    "    macro_f1 = np.mean(f1s)\n",
    "    \n",
    "    total_support = sum(supports)\n",
    "    weighted_precision = sum(p * s for p, s in zip(precisions, supports)) / total_support\n",
    "    weighted_recall = sum(r * s for r, s in zip(recalls, supports)) / total_support\n",
    "    weighted_f1 = sum(f * s for f, s in zip(f1s, supports)) / total_support\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Macro Avg':<12} {macro_precision:<12.2%} {macro_recall:<12.2%} {macro_f1:<12.3f} {total_support:<12}\")\n",
    "    print(f\"{'Weighted Avg':<12} {weighted_precision:<12.2%} {weighted_recall:<12.2%} {weighted_f1:<12.3f} {total_support:<12}\")\n",
    "    \n",
    "    # Most common misclassifications\n",
    "    print(\"\\n\\nMOST COMMON MISCLASSIFICATIONS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    misclass_counts = []\n",
    "    for i, gold_label in enumerate(class_labels):\n",
    "        for j, pred_label in enumerate(class_labels):\n",
    "            if i != j and cm[i][j] > 0:\n",
    "                misclass_counts.append((gold_label, pred_label, cm[i][j]))\n",
    "    \n",
    "    misclass_counts.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    if misclass_counts:\n",
    "        print(f\"{'Gold → Predicted':<30} {'Count':<10} {'% of Gold Class'}\")\n",
    "        print(\"-\"*80)\n",
    "        for gold_label, pred_label, count in misclass_counts[:10]:\n",
    "            gold_total = cm[class_labels.index(gold_label), :].sum()\n",
    "            pct = count / gold_total * 100 if gold_total > 0 else 0\n",
    "            print(f\"{gold_label} → {pred_label:<20} {count:<10} {pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"No misclassifications detected!\")\n",
    "\n",
    "# Contributions-specific analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CONTRIBUTIONS ANALYSIS (Prose + Verse)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "contrib_gold = [g for g in all_gold_classes if g in ['prose', 'verse']]\n",
    "contrib_pred = [p for g, p in zip(all_gold_classes, all_pred_classes) if g in ['prose', 'verse']]\n",
    "\n",
    "if contrib_gold:\n",
    "    contrib_correct = sum(1 for g, p in zip(contrib_gold, contrib_pred) if g == p)\n",
    "    contrib_accuracy = contrib_correct / len(contrib_gold)\n",
    "    \n",
    "    print(f\"\\nTotal contribution items: {len(contrib_gold)}\")\n",
    "    print(f\"Correctly classified: {contrib_correct} ({contrib_accuracy:.2%})\")\n",
    "    print(f\"Misclassified: {len(contrib_gold) - contrib_correct} ({(1-contrib_accuracy):.2%})\")\n",
    "    \n",
    "    # Contribution confusion\n",
    "    contrib_labels = ['prose', 'verse']\n",
    "    contrib_cm = confusion_matrix(contrib_gold, contrib_pred, labels=contrib_labels)\n",
    "    \n",
    "    print(\"\\nContributions Confusion Matrix:\")\n",
    "    print(f\"{'':>12}\", end='')\n",
    "    for label in contrib_labels:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    for i, label in enumerate(contrib_labels):\n",
    "        print(f\"{label:>12}\", end='')\n",
    "        for j in range(len(contrib_labels)):\n",
    "            print(f\"{contrib_cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "\n",
    "# Per-page classification breakdown\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PER-PAGE CLASSIFICATION BREAKDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "page_class_df_data = []\n",
    "for result in page_classification_results:\n",
    "    page_class_df_data.append({\n",
    "        'page_id': result['page_id'],\n",
    "        'total_items': result['total'],\n",
    "        'correct': result['correct'],\n",
    "        'accuracy_%': round(result['accuracy'] * 100, 1) if result['total'] > 0 else 0.0,\n",
    "        'misclassified': result['total'] - result['correct']\n",
    "    })\n",
    "\n",
    "page_class_df = pd.DataFrame(page_class_df_data)\n",
    "print(\"\\n\" + page_class_df.to_string(index=False))\n",
    "\n",
    "# Detailed per-page analysis\n",
    "print(\"\\n\\nDETAILED PER-PAGE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in page_classification_results:\n",
    "    if result['total'] == 0:\n",
    "        continue\n",
    "    \n",
    "    page_id = result['page_id']\n",
    "    gold_classes = result['gold_classes']\n",
    "    pred_classes = result['pred_classes']\n",
    "    \n",
    "    print(f\"\\n{page_id}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Accuracy: {result['accuracy']:.2%} ({result['correct']}/{result['total']})\")\n",
    "    \n",
    "    # Class distribution\n",
    "    from collections import Counter\n",
    "    gold_dist = Counter(gold_classes)\n",
    "    \n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for cls in ['prose', 'verse', 'ad', 'paratext', 'unknown']:\n",
    "        if cls in gold_dist:\n",
    "            gold_count = gold_dist[cls]\n",
    "            pred_count = sum(1 for g, p in zip(gold_classes, pred_classes) \n",
    "                           if g == cls and p == cls)\n",
    "            print(f\"   {cls:<12} {pred_count}/{gold_count} correct\")\n",
    "    \n",
    "    # Misclassifications for this page\n",
    "    misclass_page = [(g, p) for g, p in zip(gold_classes, pred_classes) if g != p]\n",
    "    if misclass_page:\n",
    "        print(f\"\\nMisclassifications ({len(misclass_page)}):\")\n",
    "        misclass_counter = Counter(misclass_page)\n",
    "        for (gold, pred), count in misclass_counter.most_common():\n",
    "            print(f\"   {gold} → {pred}: {count} time{'s' if count > 1 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Overall classification accuracy: {overall_accuracy:.2%}\")\n",
    "print(f\"- Best performing class: {class_labels[np.argmax([recalls[i] for i in range(len(class_labels))])]}\")\n",
    "print(f\"- Most challenging class: {class_labels[np.argmin([recalls[i] for i in range(len(class_labels))])]}\")\n",
    "if misclass_counts:\n",
    "    print(f\"- Most common confusion: {misclass_counts[0][0]} → {misclass_counts[0][1]} ({misclass_counts[0][2]} times)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c433e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metadata extraction (titles and authors)...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OVERALL METADATA EXTRACTION METRICS\n",
      "================================================================================\n",
      "\n",
      "--- TITLE EXTRACTION ---\n",
      "Gold items with titles:       14\n",
      "Predicted items with titles:  12\n",
      "Exact matches:                10\n",
      "Partial matches (≥80% sim):   10\n",
      "\n",
      "Precision: 83.33%\n",
      "Recall:    71.43%\n",
      "F1 Score:  0.769\n",
      "\n",
      "--- AUTHOR EXTRACTION ---\n",
      "Gold items with authors:      11\n",
      "Predicted items with authors: 10\n",
      "Exact matches:                10\n",
      "Partial matches (≥80% sim):   10\n",
      "\n",
      "Precision: 100.00%\n",
      "Recall:    90.91%\n",
      "F1 Score:  0.952\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PER-PAGE METADATA EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "                                    page_id  title_gold  title_pred  title_F1  author_gold  author_pred  author_F1\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-001           1           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-002           1           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-003           0           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-004           1           1     1.000            1            1        1.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-005           1           1     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-006           2           2     1.000            2            2        1.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-007           0           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-008           0           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-009           3           3     0.667            3            3        1.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-010           2           2     1.000            2            2        1.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-011           0           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-012           2           2     1.000            3            2        0.8\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-013           0           0     0.000            0            0        0.0\n",
      " La_Plume_bpt6k1185893k_1_10_1889__page-014           0           0     0.000            0            0        0.0\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001           1           1     1.000            0            0        0.0\n",
      "\n",
      "\n",
      "DETAILED PER-PAGE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  0/1 extracted (P: 0.00%, R: 0.00%, F1: 0.000)\n",
      "Authors: No gold authors on this page\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  0/1 extracted (P: 0.00%, R: 0.00%, F1: 0.000)\n",
      "Authors: No gold authors on this page\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  1/1 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 1/1 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  0/1 extracted (P: 0.00%, R: 0.00%, F1: 0.000)\n",
      "Authors: No gold authors on this page\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/3 extracted (P: 66.67%, R: 66.67%, F1: 0.667)\n",
      "Authors: 3/3 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 2/3 extracted (P: 100.00%, R: 66.67%, F1: 0.800)\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  1/1 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: No gold authors on this page\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Title extraction F1: 0.769\n",
      "- Author extraction F1: 0.952\n",
      "- Title exact match rate: 10/14 (71.4%)\n",
      "- Author exact match rate: 10/11 (90.9%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metadata Extraction Evaluation\n",
    "Evaluate title and author extraction accuracy on matched items.\n",
    "\n",
    "Metrics:\n",
    "- Exact match: Field matches exactly\n",
    "- Partial match: String similarity above threshold\n",
    "- Presence: Field is present (not None) in both gold and pred\n",
    "- Precision: Of predicted fields, how many are correct?\n",
    "- Recall: Of gold fields, how many were extracted?\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def normalize_metadata_string(s: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Normalize metadata string for comparison.\n",
    "    - Lowercase\n",
    "    - Remove extra whitespace\n",
    "    - Remove punctuation at start/end\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip('.,;:!?')\n",
    "    return s\n",
    "\n",
    "\n",
    "def metadata_similarity(gold: Optional[str], pred: Optional[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity between two metadata strings.\n",
    "    Returns 1.0 for exact match, 0.0 for no match, partial scores for similarity.\n",
    "    \"\"\"\n",
    "    gold_norm = normalize_metadata_string(gold)\n",
    "    pred_norm = normalize_metadata_string(pred)\n",
    "    \n",
    "    if not gold_norm and not pred_norm:\n",
    "        return 1.0  # Both null\n",
    "    if not gold_norm or not pred_norm:\n",
    "        return 0.0  # One null, one not\n",
    "    \n",
    "    if gold_norm == pred_norm:\n",
    "        return 1.0  # Exact match\n",
    "    \n",
    "    # Use SequenceMatcher for partial similarity\n",
    "    return SequenceMatcher(None, gold_norm, pred_norm).ratio()\n",
    "\n",
    "\n",
    "def evaluate_metadata_field(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                            matches: List[Tuple[int, int, float]],\n",
    "                            field_name: str,\n",
    "                            similarity_threshold: float = 0.8) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a specific metadata field (title or author).\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold items\n",
    "        pred_items: List of pred items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        field_name: 'item_title' or 'item_author'\n",
    "        similarity_threshold: Minimum similarity for partial match\n",
    "    \n",
    "    Returns:\n",
    "        Dict with metrics\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return {\n",
    "            'gold_present': 0,\n",
    "            'pred_present': 0,\n",
    "            'exact_matches': 0,\n",
    "            'partial_matches': 0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'examples': []\n",
    "        }\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    gold_present = 0  # Gold has non-null value\n",
    "    pred_present = 0  # Pred has non-null value\n",
    "    exact_matches = 0\n",
    "    partial_matches = 0\n",
    "    examples = []\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_value = gold_item.get(field_name)\n",
    "        pred_value = pred_item.get(field_name)\n",
    "        \n",
    "        gold_has_value = gold_value is not None and gold_value.strip() != ''\n",
    "        pred_has_value = pred_value is not None and pred_value.strip() != ''\n",
    "        \n",
    "        if gold_has_value:\n",
    "            gold_present += 1\n",
    "        \n",
    "        if pred_has_value:\n",
    "            pred_present += 1\n",
    "        \n",
    "        if gold_has_value and pred_has_value:\n",
    "            similarity = metadata_similarity(gold_value, pred_value)\n",
    "            \n",
    "            if similarity == 1.0:\n",
    "                exact_matches += 1\n",
    "                partial_matches += 1\n",
    "            elif similarity >= similarity_threshold:\n",
    "                partial_matches += 1\n",
    "                # Store example for partial matches\n",
    "                if len(examples) < 5:\n",
    "                    examples.append({\n",
    "                        'gold': gold_value,\n",
    "                        'pred': pred_value,\n",
    "                        'similarity': similarity,\n",
    "                        'item_class': gold_item.get('item_class')\n",
    "                    })\n",
    "    \n",
    "    # Calculate metrics based on partial matches (more lenient)\n",
    "    # Precision: of predicted values, how many match gold?\n",
    "    precision = partial_matches / pred_present if pred_present > 0 else 0.0\n",
    "    \n",
    "    # Recall: of gold values, how many were extracted?\n",
    "    recall = partial_matches / gold_present if gold_present > 0 else 0.0\n",
    "    \n",
    "    # F1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'gold_present': gold_present,\n",
    "        'pred_present': pred_present,\n",
    "        'exact_matches': exact_matches,\n",
    "        'partial_matches': partial_matches,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'examples': examples\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate metadata across all pages\n",
    "print(\"Evaluating metadata extraction (titles and authors)...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_metadata_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Evaluate titles\n",
    "    title_metrics = evaluate_metadata_field(gold_items, pred_items, matches, 'item_title')\n",
    "    \n",
    "    # Evaluate authors\n",
    "    author_metrics = evaluate_metadata_field(gold_items, pred_items, matches, 'item_author')\n",
    "    \n",
    "    page_metadata_results.append({\n",
    "        'page_id': page_id,\n",
    "        'title': title_metrics,\n",
    "        'author': author_metrics\n",
    "    })\n",
    "\n",
    "# Aggregate overall metrics\n",
    "overall_title = {\n",
    "    'gold_present': sum(r['title']['gold_present'] for r in page_metadata_results),\n",
    "    'pred_present': sum(r['title']['pred_present'] for r in page_metadata_results),\n",
    "    'exact_matches': sum(r['title']['exact_matches'] for r in page_metadata_results),\n",
    "    'partial_matches': sum(r['title']['partial_matches'] for r in page_metadata_results)\n",
    "}\n",
    "\n",
    "overall_author = {\n",
    "    'gold_present': sum(r['author']['gold_present'] for r in page_metadata_results),\n",
    "    'pred_present': sum(r['author']['pred_present'] for r in page_metadata_results),\n",
    "    'exact_matches': sum(r['author']['exact_matches'] for r in page_metadata_results),\n",
    "    'partial_matches': sum(r['author']['partial_matches'] for r in page_metadata_results)\n",
    "}\n",
    "\n",
    "# Calculate overall precision, recall, F1\n",
    "overall_title['precision'] = (overall_title['partial_matches'] / overall_title['pred_present'] \n",
    "                              if overall_title['pred_present'] > 0 else 0.0)\n",
    "overall_title['recall'] = (overall_title['partial_matches'] / overall_title['gold_present'] \n",
    "                          if overall_title['gold_present'] > 0 else 0.0)\n",
    "overall_title['f1'] = (2 * overall_title['precision'] * overall_title['recall'] / \n",
    "                       (overall_title['precision'] + overall_title['recall']) \n",
    "                       if (overall_title['precision'] + overall_title['recall']) > 0 else 0.0)\n",
    "\n",
    "overall_author['precision'] = (overall_author['partial_matches'] / overall_author['pred_present'] \n",
    "                               if overall_author['pred_present'] > 0 else 0.0)\n",
    "overall_author['recall'] = (overall_author['partial_matches'] / overall_author['gold_present'] \n",
    "                           if overall_author['gold_present'] > 0 else 0.0)\n",
    "overall_author['f1'] = (2 * overall_author['precision'] * overall_author['recall'] / \n",
    "                        (overall_author['precision'] + overall_author['recall']) \n",
    "                        if (overall_author['precision'] + overall_author['recall']) > 0 else 0.0)\n",
    "\n",
    "# Print overall results\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL METADATA EXTRACTION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- TITLE EXTRACTION ---\")\n",
    "print(f\"Gold items with titles:       {overall_title['gold_present']}\")\n",
    "print(f\"Predicted items with titles:  {overall_title['pred_present']}\")\n",
    "print(f\"Exact matches:                {overall_title['exact_matches']}\")\n",
    "print(f\"Partial matches (≥80% sim):   {overall_title['partial_matches']}\")\n",
    "print(f\"\\nPrecision: {overall_title['precision']:.2%}\")\n",
    "print(f\"Recall:    {overall_title['recall']:.2%}\")\n",
    "print(f\"F1 Score:  {overall_title['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n--- AUTHOR EXTRACTION ---\")\n",
    "print(f\"Gold items with authors:      {overall_author['gold_present']}\")\n",
    "print(f\"Predicted items with authors: {overall_author['pred_present']}\")\n",
    "print(f\"Exact matches:                {overall_author['exact_matches']}\")\n",
    "print(f\"Partial matches (≥80% sim):   {overall_author['partial_matches']}\")\n",
    "print(f\"\\nPrecision: {overall_author['precision']:.2%}\")\n",
    "print(f\"Recall:    {overall_author['recall']:.2%}\")\n",
    "print(f\"F1 Score:  {overall_author['f1']:.3f}\")\n",
    "\n",
    "# Collect examples from all pages\n",
    "all_title_examples = []\n",
    "all_author_examples = []\n",
    "\n",
    "for result in page_metadata_results:\n",
    "    all_title_examples.extend(result['title']['examples'])\n",
    "    all_author_examples.extend(result['author']['examples'])\n",
    "\n",
    "# Show examples of partial matches (not exact)\n",
    "if all_title_examples:\n",
    "    print(\"\\n\\nEXAMPLES OF PARTIAL TITLE MATCHES\")\n",
    "    print(\"-\"*80)\n",
    "    for i, ex in enumerate(all_title_examples[:5], 1):\n",
    "        print(f\"\\n{i}. {ex['item_class'].upper()} (Similarity: {ex['similarity']:.2%})\")\n",
    "        print(f\"   Gold: \\\"{ex['gold']}\\\"\")\n",
    "        print(f\"   Pred: \\\"{ex['pred']}\\\"\")\n",
    "\n",
    "if all_author_examples:\n",
    "    print(\"\\n\\nEXAMPLES OF PARTIAL AUTHOR MATCHES\")\n",
    "    print(\"-\"*80)\n",
    "    for i, ex in enumerate(all_author_examples[:5], 1):\n",
    "        print(f\"\\n{i}. {ex['item_class'].upper()} (Similarity: {ex['similarity']:.2%})\")\n",
    "        print(f\"   Gold: \\\"{ex['gold']}\\\"\")\n",
    "        print(f\"   Pred: \\\"{ex['pred']}\\\"\")\n",
    "\n",
    "# Per-page breakdown\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PER-PAGE METADATA EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "page_meta_df_data = []\n",
    "for result in page_metadata_results:\n",
    "    page_meta_df_data.append({\n",
    "        'page_id': result['page_id'],\n",
    "        'title_gold': result['title']['gold_present'],\n",
    "        'title_pred': result['title']['pred_present'],\n",
    "        'title_F1': round(result['title']['f1'], 3),\n",
    "        'author_gold': result['author']['gold_present'],\n",
    "        'author_pred': result['author']['pred_present'],\n",
    "        'author_F1': round(result['author']['f1'], 3)\n",
    "    })\n",
    "\n",
    "page_meta_df = pd.DataFrame(page_meta_df_data)\n",
    "print(\"\\n\" + page_meta_df.to_string(index=False))\n",
    "\n",
    "# Detailed per-page analysis\n",
    "print(\"\\n\\nDETAILED PER-PAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in page_metadata_results:\n",
    "    page_id = result['page_id']\n",
    "    title_metrics = result['title']\n",
    "    author_metrics = result['author']\n",
    "    \n",
    "    if title_metrics['gold_present'] == 0 and author_metrics['gold_present'] == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{page_id}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if title_metrics['gold_present'] > 0:\n",
    "        print(f\"Titles:  {title_metrics['partial_matches']}/{title_metrics['gold_present']} extracted \" +\n",
    "              f\"(P: {title_metrics['precision']:.2%}, R: {title_metrics['recall']:.2%}, \" +\n",
    "              f\"F1: {title_metrics['f1']:.3f})\")\n",
    "    else:\n",
    "        print(f\"Titles:  No gold titles on this page\")\n",
    "    \n",
    "    if author_metrics['gold_present'] > 0:\n",
    "        print(f\"Authors: {author_metrics['partial_matches']}/{author_metrics['gold_present']} extracted \" +\n",
    "              f\"(P: {author_metrics['precision']:.2%}, R: {author_metrics['recall']:.2%}, \" +\n",
    "              f\"F1: {author_metrics['f1']:.3f})\")\n",
    "    else:\n",
    "        print(f\"Authors: No gold authors on this page\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Title extraction F1: {overall_title['f1']:.3f}\")\n",
    "print(f\"- Author extraction F1: {overall_author['f1']:.3f}\")\n",
    "print(f\"- Title exact match rate: {overall_title['exact_matches']}/{overall_title['gold_present']} \" +\n",
    "      f\"({overall_title['exact_matches']/overall_title['gold_present']*100:.1f}%)\" \n",
    "      if overall_title['gold_present'] > 0 else \"- Title exact match rate: N/A\")\n",
    "print(f\"- Author exact match rate: {overall_author['exact_matches']}/{overall_author['gold_present']} \" +\n",
    "      f\"({overall_author['exact_matches']/overall_author['gold_present']*100:.1f}%)\"\n",
    "      if overall_author['gold_present'] > 0 else \"- Author exact match rate: N/A\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "838969fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating continuation tracking (all items)...\n",
      "\n",
      "======================================================================\n",
      "CONTINUATION TRACKING - GLOBAL SUMMARY (All Items)\n",
      "======================================================================\n",
      "\n",
      "Dataset coverage:\n",
      "  Matched items:        39\n",
      "  Unmatched gold items: 34\n",
      "  Unmatched pred items: 18\n",
      "\n",
      "is_continuation field:\n",
      "  Gold positives (True):     7\n",
      "  Pred positives (True):     4  (mismatch: -3)\n",
      "  True Positives (TP):       0\n",
      "  False Positives (FP):      4\n",
      "  False Negatives (FN):      7\n",
      "  True Negatives (TN):       35\n",
      "  Precision:                 0.00%\n",
      "  Recall:                    0.00%\n",
      "  F1 Score:                  0.000\n",
      "\n",
      "continues_on_next_page field:\n",
      "  Gold positives (True):     8\n",
      "  Pred positives (True):     8  (mismatch: +0)\n",
      "  True Positives (TP):       4\n",
      "  False Positives (FP):      4\n",
      "  False Negatives (FN):      4\n",
      "  True Negatives (TN):       34\n",
      "  Precision:                 50.00%\n",
      "  Recall:                    50.00%\n",
      "  F1 Score:                  0.500\n",
      "\n",
      "======================================================================\n",
      "PER-PAGE BREAKDOWN\n",
      "======================================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Matched: 1  |  Unmatched gold: 7  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "  Matched: 2  |  Unmatched gold: 0  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "  Matched: 1  |  Unmatched gold: 2  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 1  (mismatch: +1)\n",
      "    TP: 0  FP: 1  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "  Matched: 1  |  Unmatched gold: 4  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "  Matched: 1  |  Unmatched gold: 4  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "  Matched: 3  |  Unmatched gold: 3  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "  Matched: 1  |  Unmatched gold: 1  |  Unmatched pred: 3\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "  Matched: 0  |  Unmatched gold: 0  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "  Matched: 4  |  Unmatched gold: 3  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "  Matched: 3  |  Unmatched gold: 3  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "  Matched: 0  |  Unmatched gold: 3  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "  Matched: 7  |  Unmatched gold: 1  |  Unmatched pred: 3\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 1  (mismatch: +1)\n",
      "    TP: 0  FP: 1  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "  Matched: 10  |  Unmatched gold: 1  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "  Matched: 3  |  Unmatched gold: 1  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893__page-001.json\n",
      "  Matched: 2  |  Unmatched gold: 1  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Continuation Tracking Evaluation\n",
    "\n",
    "Evaluates the accuracy of continuation fields (is_continuation, continues_on_next_page)\n",
    "across ALL items in the dataset, including unmatched items.\n",
    "\n",
    "Fields are treated as binary:\n",
    "- True = continuation exists\n",
    "- False/None = no continuation (treated identically)\n",
    "\n",
    "Evaluation logic:\n",
    "- Matched items: Compare gold vs pred continuation fields directly\n",
    "- Unmatched gold items with continuation=True: False Negatives (model missed them)\n",
    "- Unmatched pred items with continuation=True: False Positives (model hallucinated them)\n",
    "\n",
    "Metrics: Precision, Recall, F1 for each field\n",
    "Reports: Global aggregates first, then per-page breakdown\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_continuation_all_items(\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict],\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    unmatched_gold: Set[int],\n",
    "    unmatched_pred: Set[int]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy across ALL items.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: Gold standard items\n",
    "        pred_items: Predicted items\n",
    "        matches: List of (gold_idx, pred_idx, similarity) tuples\n",
    "        unmatched_gold: Set of unmatched gold indices\n",
    "        unmatched_pred: Set of unmatched pred indices\n",
    "        \n",
    "    Returns:\n",
    "        Dict with metrics for is_continuation and continues_on_next_page\n",
    "    \"\"\"\n",
    "    # Initialize counters for both fields\n",
    "    is_cont_tp = is_cont_fp = is_cont_fn = is_cont_tn = 0\n",
    "    continues_tp = continues_fp = continues_fn = continues_tn = 0\n",
    "    \n",
    "    # 1. Evaluate matched items\n",
    "    for gold_idx, pred_idx, _ in matches:\n",
    "        gold_item = gold_items[gold_idx]\n",
    "        pred_item = pred_items[pred_idx]\n",
    "        \n",
    "        # Evaluate is_continuation\n",
    "        gold_is_cont = gold_item.get('is_continuation') is True\n",
    "        pred_is_cont = pred_item.get('is_continuation') is True\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_tp += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_fp += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_fn += 1\n",
    "        else:\n",
    "            is_cont_tn += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page') is True\n",
    "        pred_continues = pred_item.get('continues_on_next_page') is True\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_tp += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_fp += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_fn += 1\n",
    "        else:\n",
    "            continues_tn += 1\n",
    "    \n",
    "    # 2. Evaluate unmatched gold items (missed continuations = False Negatives)\n",
    "    for gold_idx in unmatched_gold:\n",
    "        gold_item = gold_items[gold_idx]\n",
    "        \n",
    "        # If gold has continuation=True but item wasn't matched, that's a FN\n",
    "        if gold_item.get('is_continuation') is True:\n",
    "            is_cont_fn += 1\n",
    "        \n",
    "        if gold_item.get('continues_on_next_page') is True:\n",
    "            continues_fn += 1\n",
    "    \n",
    "    # 3. Evaluate unmatched pred items (hallucinated continuations = False Positives)\n",
    "    for pred_idx in unmatched_pred:\n",
    "        pred_item = pred_items[pred_idx]\n",
    "        \n",
    "        # If pred has continuation=True but item wasn't matched, that's a FP\n",
    "        if pred_item.get('is_continuation') is True:\n",
    "            is_cont_fp += 1\n",
    "        \n",
    "        if pred_item.get('continues_on_next_page') is True:\n",
    "            continues_fp += 1\n",
    "    \n",
    "    # Calculate metrics for is_continuation\n",
    "    is_cont_p = is_cont_tp / (is_cont_tp + is_cont_fp) if (is_cont_tp + is_cont_fp) > 0 else 0.0\n",
    "    is_cont_r = is_cont_tp / (is_cont_tp + is_cont_fn) if (is_cont_tp + is_cont_fn) > 0 else 0.0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0.0\n",
    "    \n",
    "    # Calculate metrics for continues_on_next_page\n",
    "    continues_p = continues_tp / (continues_tp + continues_fp) if (continues_tp + continues_fp) > 0 else 0.0\n",
    "    continues_r = continues_tp / (continues_tp + continues_fn) if (continues_tp + continues_fn) > 0 else 0.0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            'tp': is_cont_tp,\n",
    "            'fp': is_cont_fp,\n",
    "            'fn': is_cont_fn,\n",
    "            'tn': is_cont_tn,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            'tp': continues_tp,\n",
    "            'fp': continues_fp,\n",
    "            'fn': continues_fn,\n",
    "            'tn': continues_tn,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        },\n",
    "        'n_matched': len(matches),\n",
    "        'n_unmatched_gold': len(unmatched_gold),\n",
    "        'n_unmatched_pred': len(unmatched_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate continuation tracking on all pages\n",
    "print(\"Evaluating continuation tracking (all items)...\")\n",
    "print()\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    unmatched_gold = page['unmatched_gold']\n",
    "    unmatched_pred = page['unmatched_pred']\n",
    "    page_name = page['page_name']\n",
    "    \n",
    "    result = evaluate_continuation_all_items(\n",
    "        gold_items, pred_items, matches, \n",
    "        unmatched_gold, unmatched_pred\n",
    "    )\n",
    "    result['page'] = page_name\n",
    "    \n",
    "    continuation_results.append(result)\n",
    "\n",
    "# Aggregate global metrics\n",
    "total_is_cont = {\n",
    "    'tp': sum(r['is_continuation']['tp'] for r in continuation_results),\n",
    "    'fp': sum(r['is_continuation']['fp'] for r in continuation_results),\n",
    "    'fn': sum(r['is_continuation']['fn'] for r in continuation_results),\n",
    "    'tn': sum(r['is_continuation']['tn'] for r in continuation_results)\n",
    "}\n",
    "\n",
    "total_continues = {\n",
    "    'tp': sum(r['continues_on_next_page']['tp'] for r in continuation_results),\n",
    "    'fp': sum(r['continues_on_next_page']['fp'] for r in continuation_results),\n",
    "    'fn': sum(r['continues_on_next_page']['fn'] for r in continuation_results),\n",
    "    'tn': sum(r['continues_on_next_page']['tn'] for r in continuation_results)\n",
    "}\n",
    "\n",
    "# Calculate global metrics\n",
    "is_cont_p = total_is_cont['tp'] / (total_is_cont['tp'] + total_is_cont['fp']) if (total_is_cont['tp'] + total_is_cont['fp']) > 0 else 0.0\n",
    "is_cont_r = total_is_cont['tp'] / (total_is_cont['tp'] + total_is_cont['fn']) if (total_is_cont['tp'] + total_is_cont['fn']) > 0 else 0.0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0.0\n",
    "\n",
    "continues_p = total_continues['tp'] / (total_continues['tp'] + total_continues['fp']) if (total_continues['tp'] + total_continues['fp']) > 0 else 0.0\n",
    "continues_r = total_continues['tp'] / (total_continues['tp'] + total_continues['fn']) if (total_continues['tp'] + total_continues['fn']) > 0 else 0.0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0.0\n",
    "\n",
    "total_matched = sum(r['n_matched'] for r in continuation_results)\n",
    "total_unmatched_gold = sum(r['n_unmatched_gold'] for r in continuation_results)\n",
    "total_unmatched_pred = sum(r['n_unmatched_pred'] for r in continuation_results)\n",
    "\n",
    "# Count how many items have True values in gold\n",
    "gold_is_cont_count = total_is_cont['tp'] + total_is_cont['fn']\n",
    "gold_continues_count = total_continues['tp'] + total_continues['fn']\n",
    "\n",
    "# Count how many True values the model predicted\n",
    "pred_is_cont_count = total_is_cont['tp'] + total_is_cont['fp']\n",
    "pred_continues_count = total_continues['tp'] + total_continues['fp']\n",
    "\n",
    "# Calculate quantity mismatch\n",
    "is_cont_mismatch = pred_is_cont_count - gold_is_cont_count\n",
    "continues_mismatch = pred_continues_count - gold_continues_count\n",
    "\n",
    "# Print global summary\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"CONTINUATION TRACKING - GLOBAL SUMMARY (All Items)\")\n",
    "print(f\"{'='*70}\")\n",
    "print()\n",
    "print(f\"Dataset coverage:\")\n",
    "print(f\"  Matched items:        {total_matched}\")\n",
    "print(f\"  Unmatched gold items: {total_unmatched_gold}\")\n",
    "print(f\"  Unmatched pred items: {total_unmatched_pred}\")\n",
    "print()\n",
    "print(f\"is_continuation field:\")\n",
    "print(f\"  Gold positives (True):     {gold_is_cont_count}\")\n",
    "print(f\"  Pred positives (True):     {pred_is_cont_count}  (mismatch: {is_cont_mismatch:+d})\")\n",
    "print(f\"  True Positives (TP):       {total_is_cont['tp']}\")\n",
    "print(f\"  False Positives (FP):      {total_is_cont['fp']}\")\n",
    "print(f\"  False Negatives (FN):      {total_is_cont['fn']}\")\n",
    "print(f\"  True Negatives (TN):       {total_is_cont['tn']}\")\n",
    "print(f\"  Precision:                 {is_cont_p:.2%}\")\n",
    "print(f\"  Recall:                    {is_cont_r:.2%}\")\n",
    "print(f\"  F1 Score:                  {is_cont_f1:.3f}\")\n",
    "print()\n",
    "print(f\"continues_on_next_page field:\")\n",
    "print(f\"  Gold positives (True):     {gold_continues_count}\")\n",
    "print(f\"  Pred positives (True):     {pred_continues_count}  (mismatch: {continues_mismatch:+d})\")\n",
    "print(f\"  True Positives (TP):       {total_continues['tp']}\")\n",
    "print(f\"  False Positives (FP):      {total_continues['fp']}\")\n",
    "print(f\"  False Negatives (FN):      {total_continues['fn']}\")\n",
    "print(f\"  True Negatives (TN):       {total_continues['tn']}\")\n",
    "print(f\"  Precision:                 {continues_p:.2%}\")\n",
    "print(f\"  Recall:                    {continues_r:.2%}\")\n",
    "print(f\"  F1 Score:                  {continues_f1:.3f}\")\n",
    "print()\n",
    "\n",
    "# Per-page breakdown\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"PER-PAGE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "print()\n",
    "\n",
    "for result in continuation_results:\n",
    "    page = result['page']\n",
    "    n_matched = result['n_matched']\n",
    "    n_unmatch_gold = result['n_unmatched_gold']\n",
    "    n_unmatch_pred = result['n_unmatched_pred']\n",
    "    \n",
    "    is_cont = result['is_continuation']\n",
    "    continues = result['continues_on_next_page']\n",
    "    \n",
    "    # Count gold positives for this page\n",
    "    page_is_cont_gold = is_cont['tp'] + is_cont['fn']\n",
    "    page_continues_gold = continues['tp'] + continues['fn']\n",
    "    \n",
    "    # Count pred positives for this page\n",
    "    page_is_cont_pred = is_cont['tp'] + is_cont['fp']\n",
    "    page_continues_pred = continues['tp'] + continues['fp']\n",
    "    \n",
    "    # Calculate mismatch\n",
    "    is_cont_mismatch = page_is_cont_pred - page_is_cont_gold\n",
    "    continues_mismatch = page_continues_pred - page_continues_gold\n",
    "    \n",
    "    print(f\"{page}\")\n",
    "    print(f\"  Matched: {n_matched}  |  Unmatched gold: {n_unmatch_gold}  |  Unmatched pred: {n_unmatch_pred}\")\n",
    "    print()\n",
    "    print(f\"  is_continuation:\")\n",
    "    print(f\"    Gold: {page_is_cont_gold}  Pred: {page_is_cont_pred}  (mismatch: {is_cont_mismatch:+d})\")\n",
    "    print(f\"    TP: {is_cont['tp']}  FP: {is_cont['fp']}  FN: {is_cont['fn']}\")\n",
    "    print(f\"    P: {is_cont['precision']:.2%}  R: {is_cont['recall']:.2%}  F1: {is_cont['f1']:.3f}\")\n",
    "    print()\n",
    "    print(f\"  continues_on_next_page:\")\n",
    "    print(f\"    Gold: {page_continues_gold}  Pred: {page_continues_pred}  (mismatch: {continues_mismatch:+d})\")\n",
    "    print(f\"    TP: {continues['tp']}  FP: {continues['fp']}  FN: {continues['fn']}\")\n",
    "    print(f\"    P: {continues['precision']:.2%}  R: {continues['recall']:.2%}  F1: {continues['f1']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1 OCR EVALUATION - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "AGGREGATE METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "            Dimension                  Metric  Value                                 Details\n",
      "  Structure Detection         Item Match Rate  53.4%                     39/73 items matched\n",
      "  Structure Detection Contribution Match Rate  61.5%             16/26 contributions matched\n",
      "   Text Quality (OCR)     CER (Standard, All)  8.08%               Order-agnostic evaluation\n",
      "   Text Quality (OCR) CER (Standard, Contrib)  2.35%           Structure-aware, matched only\n",
      "   Text Quality (OCR)                Coverage  42.0% Contribution chars successfully matched\n",
      "       Classification        Overall Accuracy  94.9%                             37/39 items\n",
      "       Classification   Contribution Accuracy 100.0%                 16/16 prose/verse items\n",
      "  Metadata Extraction                Title F1  0.846                          P: 92%, R: 79%\n",
      "  Metadata Extraction               Author F1  0.952                         P: 100%, R: 91%\n",
      "Continuation Tracking      is_continuation F1  0.000                            P: 0%, R: 0%\n",
      "Continuation Tracking    continues_on_next F1  0.500                          P: 50%, R: 50%\n",
      "\n",
      "================================================================================\n",
      "PROBLEM PAGES\n",
      "================================================================================\n",
      "\n",
      "Identified 7 pages with significant issues:\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003.json:\n",
      "  • Low match rate (33%)\n",
      "  • Low contribution matching (0%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004.json:\n",
      "  • Low match rate (20%)\n",
      "  • Low contribution matching (33%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005.json:\n",
      "  • Low match rate (20%)\n",
      "  • Low contribution matching (33%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011.json:\n",
      "  • Low match rate (0%)\n",
      "  • Low contribution matching (0%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001.json:\n",
      "  • Low match rate (12%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002.json:\n",
      "  • High classification errors (50%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014.json:\n",
      "  • High classification errors (33%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final Summary Report\n",
    "\n",
    "Synthesizes all evaluation findings into a summary that aggregates metrics across all evaluation dimensions and identifies problematic pages\n",
    "\"\"\"\n",
    "\n",
    "# Collect aggregate metrics from all previous evaluations\n",
    "# These values should be computed from the previous cells\n",
    "\n",
    "# Helper function to create summary table\n",
    "def create_summary_table():\n",
    "    \"\"\"\n",
    "    Create aggregate metrics table summarizing all evaluation dimensions.\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    # 1. STRUCTURE METRICS (from Cell 3)\n",
    "    total_gold_items = sum(len(page['gold_items']) for page in all_pages)\n",
    "    total_pred_items = sum(len(page['pred_items']) for page in all_pages)\n",
    "    total_matches = sum(len(page['matches']) for page in all_pages)\n",
    "    \n",
    "    # Count contributions\n",
    "    total_gold_contrib = sum(\n",
    "        len([item for item in page['gold_items'] \n",
    "             if item['item_class'] in ['prose', 'verse']])\n",
    "        for page in all_pages\n",
    "    )\n",
    "    total_pred_contrib = sum(\n",
    "        len([item for item in page['pred_items'] \n",
    "             if item['item_class'] in ['prose', 'verse']])\n",
    "        for page in all_pages\n",
    "    )\n",
    "    contrib_matches = sum(\n",
    "        len(filter_matches_by_class(page['matches'], page['gold_items'], ['prose', 'verse']))\n",
    "        for page in all_pages\n",
    "    )\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Structure Detection',\n",
    "        'Metric': 'Item Match Rate',\n",
    "        'Value': f\"{(total_matches/total_gold_items)*100:.1f}%\",\n",
    "        'Details': f\"{total_matches}/{total_gold_items} items matched\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Structure Detection',\n",
    "        'Metric': 'Contribution Match Rate',\n",
    "        'Value': f\"{(contrib_matches/total_gold_contrib)*100:.1f}%\",\n",
    "        'Details': f\"{contrib_matches}/{total_gold_contrib} contributions matched\"\n",
    "    })\n",
    "    \n",
    "    # 2. TEXT QUALITY METRICS (from Cell 5 - reference computed values)\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'CER (Standard, All)',\n",
    "        'Value': f\"{avg_oa_all['cer_standard']:.2%}\",\n",
    "        'Details': 'Order-agnostic evaluation'\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'CER (Standard, Contrib)',\n",
    "        'Value': f\"{avg_sa_contrib['cer_standard']:.2%}\",\n",
    "        'Details': 'Structure-aware, matched only'\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'Coverage',\n",
    "        'Value': f\"{total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\",\n",
    "        'Details': 'Contribution chars successfully matched'\n",
    "    })\n",
    "    \n",
    "    # 3. CLASSIFICATION METRICS (from Cell 8 - recalculate)\n",
    "    total_matched = 0\n",
    "    total_correct = 0\n",
    "    contrib_matched = 0\n",
    "    contrib_correct = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        for g_idx, p_idx, _ in matches:\n",
    "            gold_class = gold_items[g_idx]['item_class']\n",
    "            pred_class = pred_items[p_idx]['item_class']\n",
    "            \n",
    "            total_matched += 1\n",
    "            if gold_class == pred_class:\n",
    "                total_correct += 1\n",
    "            \n",
    "            if gold_class in ['prose', 'verse']:\n",
    "                contrib_matched += 1\n",
    "                if gold_class == pred_class:\n",
    "                    contrib_correct += 1\n",
    "    \n",
    "    overall_acc = (total_correct / total_matched * 100) if total_matched > 0 else 0\n",
    "    contrib_acc = (contrib_correct / contrib_matched * 100) if contrib_matched > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Classification',\n",
    "        'Metric': 'Overall Accuracy',\n",
    "        'Value': f\"{overall_acc:.1f}%\",\n",
    "        'Details': f\"{total_correct}/{total_matched} items\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Classification',\n",
    "        'Metric': 'Contribution Accuracy',\n",
    "        'Value': f\"{contrib_acc:.1f}%\",\n",
    "        'Details': f\"{contrib_correct}/{contrib_matched} prose/verse items\"\n",
    "    })\n",
    "    \n",
    "    # 4. METADATA METRICS (from Cell 9 - recalculate)\n",
    "    \n",
    "    METADATA_SIMILARITY_THRESHOLD = 0.8  # Same as Cell 9\n",
    "    \n",
    "    title_gold = 0\n",
    "    title_pred = 0\n",
    "    title_correct = 0\n",
    "    author_gold = 0\n",
    "    author_pred = 0\n",
    "    author_correct = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        for g_idx, p_idx, _ in matches:\n",
    "            gold_item = gold_items[g_idx]\n",
    "            pred_item = pred_items[p_idx]\n",
    "            \n",
    "            # Title evaluation (exact or ≥80% similar using text_similarity)\n",
    "            if gold_item.get('item_title'):\n",
    "                title_gold += 1\n",
    "                if pred_item.get('item_title'):\n",
    "                    title_pred += 1\n",
    "                    # Check exact match or high similarity\n",
    "                    if (gold_item['item_title'].strip() == pred_item['item_title'].strip() or\n",
    "                        text_similarity(gold_item['item_title'], pred_item['item_title']) >= METADATA_SIMILARITY_THRESHOLD):\n",
    "                        title_correct += 1\n",
    "            \n",
    "            # Author evaluation (exact or ≥80% similar using text_similarity)\n",
    "            if gold_item.get('item_author'):\n",
    "                author_gold += 1\n",
    "                if pred_item.get('item_author'):\n",
    "                    author_pred += 1\n",
    "                    # Check exact match or high similarity\n",
    "                    if (gold_item['item_author'].strip() == pred_item['item_author'].strip() or\n",
    "                        text_similarity(gold_item['item_author'], pred_item['item_author']) >= METADATA_SIMILARITY_THRESHOLD):\n",
    "                        author_correct += 1\n",
    "    \n",
    "    title_p = (title_correct / title_pred * 100) if title_pred > 0 else 0\n",
    "    title_r = (title_correct / title_gold * 100) if title_gold > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    author_p = (author_correct / author_pred * 100) if author_pred > 0 else 0\n",
    "    author_r = (author_correct / author_gold * 100) if author_gold > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Metadata Extraction',\n",
    "        'Metric': 'Title F1',\n",
    "        'Value': f\"{title_f1/100:.3f}\",\n",
    "        'Details': f\"P: {title_p:.0f}%, R: {title_r:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Metadata Extraction',\n",
    "        'Metric': 'Author F1',\n",
    "        'Value': f\"{author_f1/100:.3f}\",\n",
    "        'Details': f\"P: {author_p:.0f}%, R: {author_r:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    # 5. CONTINUATION TRACKING (from Cell 10 - recalculate with ALL items)\n",
    "    is_cont_tp = is_cont_fp = is_cont_fn = 0\n",
    "    continues_tp = continues_fp = continues_fn = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        unmatched_gold = page['unmatched_gold']\n",
    "        unmatched_pred = page['unmatched_pred']\n",
    "        \n",
    "        # 1. Evaluate matched items\n",
    "        for g_idx, p_idx, _ in matches:\n",
    "            gold_item = gold_items[g_idx]\n",
    "            pred_item = pred_items[p_idx]\n",
    "            \n",
    "            # is_continuation\n",
    "            gold_is_cont = gold_item.get('is_continuation') is True\n",
    "            pred_is_cont = pred_item.get('is_continuation') is True\n",
    "            \n",
    "            if gold_is_cont and pred_is_cont:\n",
    "                is_cont_tp += 1\n",
    "            elif not gold_is_cont and pred_is_cont:\n",
    "                is_cont_fp += 1\n",
    "            elif gold_is_cont and not pred_is_cont:\n",
    "                is_cont_fn += 1\n",
    "            \n",
    "            # continues_on_next_page\n",
    "            gold_continues = gold_item.get('continues_on_next_page') is True\n",
    "            pred_continues = pred_item.get('continues_on_next_page') is True\n",
    "            \n",
    "            if gold_continues and pred_continues:\n",
    "                continues_tp += 1\n",
    "            elif not gold_continues and pred_continues:\n",
    "                continues_fp += 1\n",
    "            elif gold_continues and not pred_continues:\n",
    "                continues_fn += 1\n",
    "        \n",
    "        # 2. Evaluate unmatched gold items (missed continuations = FN)\n",
    "        for gold_idx in unmatched_gold:\n",
    "            gold_item = gold_items[gold_idx]\n",
    "            if gold_item.get('is_continuation') is True:\n",
    "                is_cont_fn += 1\n",
    "            if gold_item.get('continues_on_next_page') is True:\n",
    "                continues_fn += 1\n",
    "        \n",
    "        # 3. Evaluate unmatched pred items (hallucinated continuations = FP)\n",
    "        for pred_idx in unmatched_pred:\n",
    "            pred_item = pred_items[pred_idx]\n",
    "            if pred_item.get('is_continuation') is True:\n",
    "                is_cont_fp += 1\n",
    "            if pred_item.get('continues_on_next_page') is True:\n",
    "                continues_fp += 1\n",
    "    \n",
    "    is_cont_p = is_cont_tp / (is_cont_tp + is_cont_fp) if (is_cont_tp + is_cont_fp) > 0 else 0\n",
    "    is_cont_r = is_cont_tp / (is_cont_tp + is_cont_fn) if (is_cont_tp + is_cont_fn) > 0 else 0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "    \n",
    "    continues_p = continues_tp / (continues_tp + continues_fp) if (continues_tp + continues_fp) > 0 else 0\n",
    "    continues_r = continues_tp / (continues_tp + continues_fn) if (continues_tp + continues_fn) > 0 else 0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Continuation Tracking',\n",
    "        'Metric': 'is_continuation F1',\n",
    "        'Value': f\"{is_cont_f1:.3f}\",\n",
    "        'Details': f\"P: {is_cont_p*100:.0f}%, R: {is_cont_r*100:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Continuation Tracking',\n",
    "        'Metric': 'continues_on_next F1',\n",
    "        'Value': f\"{continues_f1:.3f}\",\n",
    "        'Details': f\"P: {continues_p*100:.0f}%, R: {continues_r*100:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "def identify_problem_pages():\n",
    "    \"\"\"\n",
    "    Identify pages with significant issues across multiple dimensions.\n",
    "    Returns list of (page_name, issues) tuples.\n",
    "    \"\"\"\n",
    "    problem_pages = {}\n",
    "    \n",
    "    for page in all_pages:\n",
    "        page_name = page['page_name']\n",
    "        issues = []\n",
    "        \n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        # Skip empty pages\n",
    "        if len(gold_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Issue 1: Low match rate\n",
    "        match_rate = len(matches) / len(gold_items) if len(gold_items) > 0 else 0\n",
    "        if match_rate < 0.5:\n",
    "            issues.append(f\"Low match rate ({match_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Issue 2: Zero predictions\n",
    "        if len(pred_items) == 0:\n",
    "            issues.append(\"Zero predictions\")\n",
    "        \n",
    "        # Issue 3: Low contribution match rate\n",
    "        gold_contrib = [item for item in gold_items if item['item_class'] in ['prose', 'verse']]\n",
    "        if gold_contrib:\n",
    "            contrib_matches = filter_matches_by_class(matches, gold_items, ['prose', 'verse'])\n",
    "            contrib_match_rate = len(contrib_matches) / len(gold_contrib) if len(gold_contrib) > 0 else 0\n",
    "            if contrib_match_rate < 0.5:\n",
    "                issues.append(f\"Low contribution matching ({contrib_match_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Issue 4: High classification errors\n",
    "        if matches:\n",
    "            misclassified = 0\n",
    "            for g_idx, p_idx, _ in matches:\n",
    "                if gold_items[g_idx]['item_class'] != pred_items[p_idx]['item_class']:\n",
    "                    misclassified += 1\n",
    "            error_rate = misclassified / len(matches)\n",
    "            if error_rate > 0.3:\n",
    "                issues.append(f\"High classification errors ({error_rate*100:.0f}%)\")\n",
    "        \n",
    "        if issues:\n",
    "            problem_pages[page_name] = issues\n",
    "    \n",
    "    return problem_pages\n",
    "\n",
    "\n",
    "# Generate summary table\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1 OCR EVALUATION - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "summary_df = create_summary_table()\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(\"-\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Problem pages\n",
    "print(\"=\" * 80)\n",
    "print(\"PROBLEM PAGES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "problem_pages = identify_problem_pages()\n",
    "if problem_pages:\n",
    "    print(f\"Identified {len(problem_pages)} pages with significant issues:\")\n",
    "    print()\n",
    "    \n",
    "    # Sort by number of issues\n",
    "    sorted_problems = sorted(problem_pages.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for page_name, issues in sorted_problems:\n",
    "        print(f\"{page_name}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  • {issue}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No pages identified with critical issues.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
