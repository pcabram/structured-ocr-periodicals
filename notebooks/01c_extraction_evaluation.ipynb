{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report, accuracy_score\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m results_to_dataframe\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Project root detection\u001b[39;00m\n\u001b[32m     26\u001b[39m PROJECT_ROOT = Path.cwd()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 OCR Evaluation\n",
    "\n",
    "Evaluates OCR extraction quality by comparing predictions against gold standard.\n",
    "\n",
    "Input:  Predictions from data/predictions/{magazine_name}/\n",
    "        Gold standard from data/gold_standard/cleaned/{magazine_name}/\n",
    "Output: Evaluation metrics and analysis\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Project root detection\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "# Add root to path \n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Stage 1 OCR Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Add schemas to path\n",
    "SCHEMAS_DIR = PROJECT_ROOT / \"schemas\"\n",
    "if str(SCHEMAS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCHEMAS_DIR))\n",
    "\n",
    "# Import schemas for validation\n",
    "from stage1_page import Stage1PageModel\n",
    "\n",
    "# Import evaluation utilities\n",
    "from utils.dataframe_helpers import results_to_dataframe\n",
    "\n",
    "# Paths\n",
    "GOLD_ROOT = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_ROOT = PROJECT_ROOT / \"data\" / \"predictions\"\n",
    "\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Gold standard: {GOLD_ROOT}\")\n",
    "print(f\"  Predictions:   {PRED_ROOT}\")\n",
    "print(f\"  Schema:        {Stage1PageModel.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3780b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find Magazine Pairs for Evaluation\n",
    "\"\"\"\n",
    "\n",
    "def find_magazine_pairs() -> List[Tuple[str, Path, Path, int]]:\n",
    "    \"\"\"\n",
    "    Find magazines that have both gold standard and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        List of (magazine_name, gold_dir, pred_dir, num_matching_files) tuples\n",
    "    \"\"\"\n",
    "    gold_magazines = {d.name: d for d in GOLD_ROOT.iterdir() if d.is_dir()}\n",
    "    pred_magazines = {d.name: d for d in PRED_ROOT.iterdir() if d.is_dir()}\n",
    "    \n",
    "    common_magazines = set(gold_magazines.keys()) & set(pred_magazines.keys())\n",
    "    \n",
    "    pairs = []\n",
    "    for mag_name in sorted(common_magazines):\n",
    "        gold_dir = gold_magazines[mag_name]\n",
    "        pred_dir = pred_magazines[mag_name]\n",
    "        \n",
    "        gold_files = {f.name for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.name for f in pred_dir.glob(\"*.json\")}\n",
    "        \n",
    "        matching_files = gold_files & pred_files\n",
    "        \n",
    "        if matching_files:\n",
    "            pairs.append((mag_name, gold_dir, pred_dir, len(matching_files)))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Find pairs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Finding Magazine Pairs\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "magazine_pairs = find_magazine_pairs()\n",
    "\n",
    "if not magazine_pairs:\n",
    "    print(\"No matching magazine pairs found.\")\n",
    "    print(\"\\nCheck that:\")\n",
    "    print(\"  1. Gold standard magazines exist in cleaned/\")\n",
    "    print(\"  2. Prediction magazines exist in predictions/\")\n",
    "    print(\"  3. Magazine names match between both directories\")\n",
    "else:\n",
    "    print(f\"Found {len(magazine_pairs)} magazine(s) for evaluation:\\n\")\n",
    "    for mag_name, gold_dir, pred_dir, num_files in magazine_pairs:\n",
    "        print(f\"{mag_name}:\")\n",
    "        print(f\"  Gold files:      {len(list(gold_dir.glob('*.json')))}\")\n",
    "        print(f\"  Pred files:      {len(list(pred_dir.glob('*.json')))}\")\n",
    "        print(f\"  Matching files:  {num_files}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Match gold items to predicted items using content-based text similarity.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_THRESHOLD = 0.7  # Minimum text similarity to consider a match (0.0-1.0)\n",
    "METADATA_SIMILARITY_THRESHOLD = 0.8  # For title/author matching\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Item Matching Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"Metadata similarity threshold: {METADATA_SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "        \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item, find the best-matching unmatched pred item.\n",
    "        Accept the match if similarity exceeds threshold.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair and match items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "\n",
    "# Test matching on first page\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Item Matching Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_pairs:\n",
    "    # Get first magazine's first page\n",
    "    mag_name, gold_dir, pred_dir, _ = magazine_pairs[0]\n",
    "    gold_files = sorted(gold_dir.glob(\"*.json\"))\n",
    "    pred_files = sorted(pred_dir.glob(\"*.json\"))\n",
    "    \n",
    "    if gold_files and pred_files:\n",
    "        test_result = load_and_match_page(gold_files[0], pred_files[0])\n",
    "        \n",
    "        print(f\"Test page: {test_result['page_name']}\")\n",
    "        print(f\"  Gold items:     {len(test_result['gold_items'])}\")\n",
    "        print(f\"  Pred items:     {len(test_result['pred_items'])}\")\n",
    "        print(f\"  Matches:        {len(test_result['matches'])}\")\n",
    "        print(f\"  Unmatched gold: {len(test_result['unmatched_gold'])}\")\n",
    "        print(f\"  Unmatched pred: {len(test_result['unmatched_pred'])}\")\n",
    "        \n",
    "        if test_result['matches']:\n",
    "            avg_score = sum(score for _, _, score in test_result['matches']) / len(test_result['matches'])\n",
    "            print(f\"  Avg match quality: {avg_score:.2%}\")\n",
    "else:\n",
    "    print(\"No magazine pairs available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89939141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load and Match All Pages\n",
    "\"\"\"\n",
    "\n",
    "def load_all_magazine_pages(magazine_pairs: List[Tuple[str, Path, Path, int]]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load and match all pages for all magazines.\n",
    "    \n",
    "    Args:\n",
    "        magazine_pairs: List from find_magazine_pairs()\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping magazine_name to list of matched page dicts\n",
    "    \"\"\"\n",
    "    all_magazine_data = {}\n",
    "    \n",
    "    for mag_name, gold_dir, pred_dir, _ in magazine_pairs:\n",
    "        gold_files = {f.name: f for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.name: f for f in pred_dir.glob(\"*.json\")}\n",
    "        \n",
    "        common_files = set(gold_files.keys()) & set(pred_files.keys())\n",
    "        \n",
    "        magazine_pages = []\n",
    "        for filename in sorted(common_files):\n",
    "            page_data = load_and_match_page(gold_files[filename], pred_files[filename])\n",
    "            page_data['page_id'] = gold_files[filename].stem\n",
    "            magazine_pages.append(page_data)\n",
    "        \n",
    "        all_magazine_data[mag_name] = magazine_pages\n",
    "    \n",
    "    return all_magazine_data\n",
    "\n",
    "# Load all pages\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading and Matching All Pages\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "all_magazine_data = load_all_magazine_pages(magazine_pairs)\n",
    "\n",
    "# Summary per magazine\n",
    "for mag_name, pages in all_magazine_data.items():\n",
    "    total_gold = sum(len(p['gold_items']) for p in pages)\n",
    "    total_pred = sum(len(p['pred_items']) for p in pages)\n",
    "    total_matches = sum(len(p['matches']) for p in pages)\n",
    "    \n",
    "    print(f\"{mag_name}:\")\n",
    "    print(f\"  Pages:       {len(pages)}\")\n",
    "    print(f\"  Gold items:  {total_gold}\")\n",
    "    print(f\"  Pred items:  {total_pred}\")\n",
    "    print(f\"  Matches:     {total_matches}\")\n",
    "    print()\n",
    "\n",
    "# Flatten for compatibility with existing evaluation code\n",
    "all_pages = []\n",
    "for pages in all_magazine_data.values():\n",
    "    all_pages.extend(pages)\n",
    "\n",
    "print(f\"Total pages: {len(all_pages)}\")\n",
    "print(f\"Total matches: {sum(len(page['matches']) for page in all_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7322404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Page-Level Diagnostics\n",
    "Generate diagnostic metrics for each page based on item matches.\n",
    "\"\"\"\n",
    "\n",
    "def diagnose_page(page_id: str, gold_items: list, pred_items: list, matches: list) -> dict:\n",
    "    \"\"\"\n",
    "    Generate diagnostic metrics for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page_id: Page identifier\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with diagnostic metrics\n",
    "    \"\"\"\n",
    "    # Count items by class\n",
    "    gold_by_class = {}\n",
    "    pred_by_class = {}\n",
    "    \n",
    "    for item in gold_items:\n",
    "        item_class = item['item_class']\n",
    "        gold_by_class[item_class] = gold_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    for item in pred_items:\n",
    "        item_class = item['item_class']\n",
    "        pred_by_class[item_class] = pred_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    # Count contributions (prose + verse)\n",
    "    gold_contrib = gold_by_class.get('prose', 0) + gold_by_class.get('verse', 0)\n",
    "    pred_contrib = pred_by_class.get('prose', 0) + pred_by_class.get('verse', 0)\n",
    "    \n",
    "    # Filter matches by contribution class\n",
    "    contrib_matches = [\n",
    "        (g_idx, p_idx, score) for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in ('prose', 'verse')\n",
    "    ]\n",
    "    \n",
    "    # Calculate match rates\n",
    "    match_rate = (len(matches) / len(gold_items) * 100) if gold_items else 0\n",
    "    contrib_match_rate = (len(contrib_matches) / gold_contrib * 100) if gold_contrib else 0\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    avg_similarity = (sum(score for _, _, score in matches) / len(matches)) if matches else 0\n",
    "    \n",
    "    # Count continuation flags\n",
    "    gold_cont_in = sum(1 for item in gold_items if item.get('is_continuation') is True)\n",
    "    pred_cont_in = sum(1 for item in pred_items if item.get('is_continuation') is True)\n",
    "    gold_cont_out = sum(1 for item in gold_items if item.get('continues_on_next_page') is True)\n",
    "    pred_cont_out = sum(1 for item in pred_items if item.get('continues_on_next_page') is True)\n",
    "    \n",
    "    # Track matched indices\n",
    "    matched_gold = {g_idx for g_idx, _, _ in matches}\n",
    "    matched_pred = {p_idx for _, p_idx, _ in matches}\n",
    "    \n",
    "    unmatched_gold = [i for i in range(len(gold_items)) if i not in matched_gold]\n",
    "    unmatched_pred = [i for i in range(len(pred_items)) if i not in matched_pred]\n",
    "    \n",
    "    # Count matches by class\n",
    "    matches_by_class = {}\n",
    "    for g_idx, p_idx, score in matches:\n",
    "        item_class = gold_items[g_idx]['item_class']\n",
    "        matches_by_class[item_class] = matches_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items),\n",
    "        'matched': len(matches),\n",
    "        'match_rate': match_rate,\n",
    "        'contrib_match_rate': contrib_match_rate,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'gold_cont_in': gold_cont_in,\n",
    "        'pred_cont_in': pred_cont_in,\n",
    "        'gold_cont_out': gold_cont_out,\n",
    "        'pred_cont_out': pred_cont_out,\n",
    "        'gold_by_class': gold_by_class,\n",
    "        'pred_by_class': pred_by_class,\n",
    "        'matches_by_class': matches_by_class,\n",
    "        'gold_contrib': gold_contrib,\n",
    "        'pred_contrib': pred_contrib,\n",
    "        'contrib_matched': len(contrib_matches),\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def flag_page(metrics: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate flags for problematic pages based on metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary from diagnose_page()\n",
    "        \n",
    "    Returns:\n",
    "        Comma-separated string of flags, or empty string if no issues\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    if metrics['pred_items'] == 0:\n",
    "        flags.append('ZERO_PREDS')\n",
    "    \n",
    "    if metrics['matched'] == 0:\n",
    "        flags.append('ZERO_MATCHES')\n",
    "    \n",
    "    if metrics['match_rate'] < 50:\n",
    "        flags.append('LOW_MATCH')\n",
    "    \n",
    "    if metrics['gold_contrib'] > 0 and metrics['contrib_match_rate'] < 60:\n",
    "        flags.append('LOW_CONTRIB')\n",
    "    \n",
    "    if abs(metrics['gold_items'] - metrics['pred_items']) >= 3:\n",
    "        flags.append('COUNT_MISMATCH')\n",
    "    \n",
    "    return ', '.join(flags)\n",
    "\n",
    "\n",
    "def run_diagnostics(all_pages: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run diagnostics on all pages and generate summary table and detailed reports.\n",
    "    \n",
    "    Args:\n",
    "        all_pages: List of page dicts from load_all_magazine_pages()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary metrics for all pages\n",
    "    \"\"\"\n",
    "    print(\"Running diagnostics on all pages...\\n\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for page in all_pages:\n",
    "        page_id = page['page_id']\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        # Generate metrics\n",
    "        metrics = diagnose_page(page_id, gold_items, pred_items, matches)\n",
    "        metrics['flags'] = flag_page(metrics)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for m in all_metrics:\n",
    "        summary_data.append({\n",
    "            'page_id': m['page_id'],\n",
    "            'gold_items': m['gold_items'],\n",
    "            'pred_items': m['pred_items'],\n",
    "            'matched': m['matched'],\n",
    "            'match_rate_%': round(m['match_rate'], 1),\n",
    "            'contrib_match_rate_%': round(m['contrib_match_rate'], 1),\n",
    "            'avg_similarity': round(m['avg_similarity'], 3),\n",
    "            'gold_cont_in': m['gold_cont_in'],\n",
    "            'pred_cont_in': m['pred_cont_in'],\n",
    "            'gold_cont_out': m['gold_cont_out'],\n",
    "            'pred_cont_out': m['pred_cont_out'],\n",
    "            'flags': m['flags']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print detailed reports for all pages\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for m in all_metrics:\n",
    "        print(f\"\\n=== Page {m['page_id']} ===\")\n",
    "        print(f\"Items: {m['gold_items']} gold, {m['pred_items']} pred\")\n",
    "        print(f\"Matches: {m['matched']} ({m['match_rate']:.1f}% match rate)\")\n",
    "        \n",
    "        print(\"\\nBy class:\")\n",
    "        all_classes = sorted(set(m['gold_by_class'].keys()) | set(m['pred_by_class'].keys()))\n",
    "        for cls in all_classes:\n",
    "            gold_count = m['gold_by_class'].get(cls, 0)\n",
    "            pred_count = m['pred_by_class'].get(cls, 0)\n",
    "            matched_count = m['matches_by_class'].get(cls, 0)\n",
    "            match_pct = (matched_count / gold_count * 100) if gold_count > 0 else 0\n",
    "            print(f\"  {cls:10s} {gold_count} gold, {pred_count} pred, {matched_count} matched ({match_pct:.1f}%)\")\n",
    "\n",
    "        \n",
    "        print(f\"\\nContributions: {m['gold_contrib']} gold, {m['pred_contrib']} pred, \"\n",
    "              f\"{m['contrib_matched']} matched ({m['contrib_match_rate']:.1f}%)\")\n",
    "        print(f\"Avg similarity: {m['avg_similarity']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nContinuations:\")\n",
    "        print(f\"  is_continuation: {m['gold_cont_in']} gold, {m['pred_cont_in']} pred\")\n",
    "        print(f\"  continues_on_next_page: {m['gold_cont_out']} gold, {m['pred_cont_out']} pred\")\n",
    "        \n",
    "        print(f\"\\nUnmatched gold items: {m['unmatched_gold']}\")\n",
    "        print(f\"Unmatched pred items: {m['unmatched_pred']}\")\n",
    "        \n",
    "        if m['flags']:\n",
    "            print(f\"\\nFLAGS: {m['flags']}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostic_df = run_diagnostics(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fcf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Helpers\n",
    "Utility functions for filtering matches and loading all pages efficiently.\n",
    "These helpers are used by the evaluation cells that follow.\n",
    "\"\"\"\n",
    "\n",
    "def filter_matches_by_class(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    item_classes: List[str]\n",
    ") -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Filter matches to only include items of specified classes.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        item_classes: List of classes to include (e.g., ['prose', 'verse'])\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of matches\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (g_idx, p_idx, score) \n",
    "        for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in item_classes\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_matched_pairs(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict]\n",
    ") -> List[Tuple[Dict, Dict, float]]:\n",
    "    \"\"\"\n",
    "    Convert match indices to actual item pairs.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_item, pred_item, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (gold_items[g_idx], pred_items[p_idx], score)\n",
    "        for g_idx, p_idx, score in matches\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text Quality Evaluation\n",
    "Calculate CER and WER using two complementary approaches:\n",
    "1. Order-agnostic: Pure OCR quality regardless of reading order\n",
    "2. Structure-aware: OCR quality on properly aligned content via matching\n",
    "\n",
    "Each approach calculates three normalization levels:\n",
    "- Strict: Preserves all whitespace (including \\n vs \\n\\n differences)\n",
    "- Standard: Normalizes whitespace to single spaces (RECOMMENDED)\n",
    "- Letters Only: Removes all whitespace and punctuation (pure character recognition)\n",
    "\n",
    "References:\n",
    "- Flexible Character Accuracy (FCA) for handling reading order issues:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "- Token sort ratio for order-agnostic OCR comparison:\n",
    "  https://urban-institute.medium.com/choosing-the-right-ocr-service-for-extracting-text-data-d7830399ec5\n",
    "- Unicode normalization and whitespace handling in OCR evaluation:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only,\n",
    "    token_sort_text\n",
    ")\n",
    "\n",
    "from utils.ocr_metrics import character_error_rate, word_error_rate\n",
    "\n",
    "\n",
    "def evaluate_order_agnostic(gold_items: List[Dict], pred_items: List[Dict], \n",
    "                            item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality without considering reading order.\n",
    "    Uses token sort ratio approach - sorts all words before comparison.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        item_classes: If provided, filter to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER for each normalization level, and text statistics\n",
    "    \"\"\"\n",
    "    # Filter by class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items if item['item_class'] in item_classes]\n",
    "        pred_items = [item for item in pred_items if item['item_class'] in item_classes]\n",
    "    \n",
    "    # Concatenate all text\n",
    "    gold_text = ' '.join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = ' '.join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    # Sort tokens for order-agnostic comparison\n",
    "    gold_sorted = token_sort_text(gold_text)\n",
    "    pred_sorted = token_sort_text(pred_text)\n",
    "    \n",
    "    # Calculate for all three normalization levels\n",
    "    results = {\n",
    "        'cer_strict': character_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'wer_strict': word_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'cer_standard': character_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'wer_standard': word_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'cer_letters': character_error_rate(gold_sorted, pred_sorted, 'letters_only'),\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split())\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_structure_aware(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                             matches: List[Tuple[int, int, float]],\n",
    "                             item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality on matched pairs, respecting document structure.\n",
    "    Only compares content that was successfully aligned via matching.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        item_classes: If provided, filter matches to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with matched CER/WER for each normalization level and unmatched content statistics\n",
    "    \"\"\"\n",
    "    # Filter matches by class if specified\n",
    "    if item_classes:\n",
    "        filtered_matches = filter_matches_by_class(matches, gold_items, item_classes)\n",
    "    else:\n",
    "        filtered_matches = matches\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(filtered_matches, gold_items, pred_items)\n",
    "    \n",
    "    # Calculate CER/WER on matched content for all normalization levels\n",
    "    if matched_pairs:\n",
    "        # Concatenate matched texts in gold order\n",
    "        gold_matched_text = ' '.join(gold_item.get('item_text_raw', '') \n",
    "                                     for gold_item, _, _ in matched_pairs)\n",
    "        pred_matched_text = ' '.join(pred_item.get('item_text_raw', '') \n",
    "                                     for _, pred_item, _ in matched_pairs)\n",
    "        \n",
    "        cer_strict = character_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        wer_strict = word_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        cer_standard = character_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        wer_standard = word_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        cer_letters = character_error_rate(gold_matched_text, pred_matched_text, 'letters_only')\n",
    "        \n",
    "        matched_gold_chars = len(gold_matched_text)\n",
    "        matched_pred_chars = len(pred_matched_text)\n",
    "    else:\n",
    "        cer_strict = 0.0\n",
    "        wer_strict = 0.0\n",
    "        cer_standard = 0.0\n",
    "        wer_standard = 0.0\n",
    "        cer_letters = 0.0\n",
    "        matched_gold_chars = 0\n",
    "        matched_pred_chars = 0\n",
    "    \n",
    "    # Calculate unmatched content\n",
    "    matched_gold_indices = {g_idx for g_idx, _, _ in filtered_matches}\n",
    "    matched_pred_indices = {p_idx for _, p_idx, _ in filtered_matches}\n",
    "    \n",
    "    if item_classes:\n",
    "        # Only count unmatched items of the specified classes\n",
    "        unmatched_gold_items = [\n",
    "            gold_items[i] for i in range(len(gold_items))\n",
    "            if i not in matched_gold_indices and gold_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        unmatched_pred_items = [\n",
    "            pred_items[i] for i in range(len(pred_items))\n",
    "            if i not in matched_pred_indices and pred_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                              for item in gold_items if item['item_class'] in item_classes)\n",
    "    else:\n",
    "        unmatched_gold_items = [gold_items[i] for i in range(len(gold_items)) \n",
    "                               if i not in matched_gold_indices]\n",
    "        unmatched_pred_items = [pred_items[i] for i in range(len(pred_items)) \n",
    "                               if i not in matched_pred_indices]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    unmatched_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_gold_items)\n",
    "    unmatched_pred_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_pred_items)\n",
    "    \n",
    "    return {\n",
    "        'cer_strict': cer_strict,\n",
    "        'wer_strict': wer_strict,\n",
    "        'cer_standard': cer_standard,\n",
    "        'wer_standard': wer_standard,\n",
    "        'cer_letters': cer_letters,\n",
    "        'matched_gold_chars': matched_gold_chars,\n",
    "        'matched_pred_chars': matched_pred_chars,\n",
    "        'unmatched_gold_chars': unmatched_gold_chars,\n",
    "        'unmatched_pred_chars': unmatched_pred_chars,\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'matched_percentage': (matched_gold_chars / total_gold_chars * 100) if total_gold_chars else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate text quality across all pages\n",
    "print(\"Evaluating text quality...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "order_agnostic_all = []\n",
    "order_agnostic_contrib = []\n",
    "structure_aware_all = []\n",
    "structure_aware_contrib = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Order-agnostic evaluation\n",
    "    oa_all = evaluate_order_agnostic(gold_items, pred_items)\n",
    "    oa_all['page_id'] = page_id\n",
    "    order_agnostic_all.append(oa_all)\n",
    "    \n",
    "    oa_contrib = evaluate_order_agnostic(gold_items, pred_items, \n",
    "                                         item_classes=['prose', 'verse'])\n",
    "    oa_contrib['page_id'] = page_id\n",
    "    order_agnostic_contrib.append(oa_contrib)\n",
    "    \n",
    "    # Structure-aware evaluation\n",
    "    sa_all = evaluate_structure_aware(gold_items, pred_items, matches)\n",
    "    sa_all['page_id'] = page_id\n",
    "    structure_aware_all.append(sa_all)\n",
    "    \n",
    "    sa_contrib = evaluate_structure_aware(gold_items, pred_items, matches,\n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    sa_contrib['page_id'] = page_id\n",
    "    structure_aware_contrib.append(sa_contrib)\n",
    "\n",
    "# Calculate averages for order-agnostic evaluation\n",
    "avg_oa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in order_agnostic_all) / len(order_agnostic_all)\n",
    "}\n",
    "\n",
    "contrib_with_content = [r for r in order_agnostic_contrib if r['gold_chars'] > 0]\n",
    "avg_oa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in contrib_with_content) / len(contrib_with_content)\n",
    "}\n",
    "\n",
    "# Calculate averages for structure-aware evaluation\n",
    "sa_all_with_matches = [r for r in structure_aware_all if r['matched_gold_chars'] > 0]\n",
    "avg_sa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_all_with_matches) / len(sa_all_with_matches)\n",
    "}\n",
    "\n",
    "sa_contrib_with_matches = [r for r in structure_aware_contrib if r['matched_gold_chars'] > 0]\n",
    "avg_sa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches)\n",
    "}\n",
    "\n",
    "# Calculate total matched percentages\n",
    "total_sa_all_matched = sum(r['matched_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_gold = sum(r['total_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_all)\n",
    "\n",
    "total_sa_contrib_matched = sum(r['matched_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_gold = sum(r['total_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_contrib)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORDER-AGNOSTIC EVALUATION\")\n",
    "print(\"   (Pure OCR quality, reading order irrelevant)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_strict']:.2%}  |  WER: {avg_oa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_standard']:.2%}  |  WER: {avg_oa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_letters']:.2%}\")\n",
    "\n",
    "print(f\"\\n   Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_strict']:.2%}  |  WER: {avg_oa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_standard']:.2%}  |  WER: {avg_oa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_letters']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STRUCTURE-AWARE EVALUATION\")\n",
    "print(\"   (OCR quality on matched content only)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   Matched Content - All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_strict']:.2%}  |  WER: {avg_sa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_standard']:.2%}  |  WER: {avg_sa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_all_matched:,} chars matched \" +\n",
    "      f\"({total_sa_all_matched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_all_unmatched:,} chars \" +\n",
    "      f\"({total_sa_all_unmatched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(f\"\\n   Matched Content - Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_strict']:.2%}  |  WER: {avg_sa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_standard']:.2%}  |  WER: {avg_sa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_contrib_matched:,} chars matched \" +\n",
    "      f\"({total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_contrib_unmatched:,} chars \" +\n",
    "      f\"({total_sa_contrib_unmatched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Strict: Most conservative\")\n",
    "print(\"Standard: Fair baseline - normalizes whitespace\")\n",
    "print(\"Letters Only: Most lenient - pure character recognition quality\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(f\"- Pure OCR quality (standard normalization): {avg_oa_all['cer_standard']:.2%}\")\n",
    "print(f\"- Letter recognition quality: {avg_oa_all['cer_letters']:.2%}\")\n",
    "print(f\"- Structure failures (unmatched content): {total_sa_all_unmatched/total_sa_all_gold*100:.1f}%\")\n",
    "print(f\"- Contributions:\")\n",
    "print(f\"    Standard CER: {avg_sa_contrib['cer_standard']:.2%}\")\n",
    "print(f\"    Successfully matched: {total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb143df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Page-by-Page Text Diagnostics\n",
    "Detailed error analysis for each page with three normalization levels.\n",
    "Shows error type distribution, worst performing pages, and actual text examples.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def get_levenshtein_operations(reference: str, hypothesis: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get detailed Levenshtein operations breakdown.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with counts of substitutions, deletions, insertions\n",
    "    \"\"\"\n",
    "    if not reference and not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': 0, 'total': 0}\n",
    "    \n",
    "    if not reference:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': len(hypothesis), 'total': len(hypothesis)}\n",
    "    \n",
    "    if not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': len(reference), 'insertions': 0, 'total': len(reference)}\n",
    "    \n",
    "    # Use SequenceMatcher to get operations\n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    substitutions = 0\n",
    "    deletions = 0\n",
    "    insertions = 0\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Both strings differ - count as substitutions\n",
    "            substitutions += max(i2 - i1, j2 - j1)\n",
    "        elif tag == 'delete':\n",
    "            # Only in reference\n",
    "            deletions += (i2 - i1)\n",
    "        elif tag == 'insert':\n",
    "            # Only in hypothesis\n",
    "            insertions += (j2 - j1)\n",
    "    \n",
    "    return {\n",
    "        'substitutions': substitutions,\n",
    "        'deletions': deletions,\n",
    "        'insertions': insertions,\n",
    "        'total': substitutions + deletions + insertions\n",
    "    }\n",
    "\n",
    "\n",
    "def diagnose_page_text_quality(page: Dict, normalization: str = 'standard') -> Dict:\n",
    "    \"\"\"\n",
    "    Detailed text quality diagnosis for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page: Page data from all_pages\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detailed metrics and error breakdowns\n",
    "    \"\"\"\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "        return {\n",
    "            'page_id': page_id,\n",
    "            'cer': 0.0,\n",
    "            'wer': 0.0,\n",
    "            'matched_chars': 0,\n",
    "            'total_gold_chars': total_gold_chars,\n",
    "            'match_coverage': 0.0,\n",
    "            'substitutions': 0,\n",
    "            'deletions': 0,\n",
    "            'insertions': 0,\n",
    "            'total_errors': 0,\n",
    "            'items_analyzed': []\n",
    "        }\n",
    "    \n",
    "    # Concatenate matched text\n",
    "    gold_text = ' '.join(gold_item.get('item_text_raw', '') for gold_item, _, _ in matched_pairs)\n",
    "    pred_text = ' '.join(pred_item.get('item_text_raw', '') for _, pred_item, _ in matched_pairs)\n",
    "    \n",
    "    # Calculate CER/WER\n",
    "    cer = character_error_rate(gold_text, pred_text, normalization)\n",
    "    wer = word_error_rate(gold_text, pred_text, normalization)\n",
    "    \n",
    "    # Get error breakdown using normalized text\n",
    "    if normalization == 'strict':\n",
    "        gold_norm = normalize_text_strict(gold_text)\n",
    "        pred_norm = normalize_text_strict(pred_text)\n",
    "    elif normalization == 'standard':\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "    else:  # letters_only\n",
    "        gold_norm = normalize_text_letters_only(gold_text)\n",
    "        pred_norm = normalize_text_letters_only(pred_text)\n",
    "    \n",
    "    ops = get_levenshtein_operations(gold_norm, pred_norm)\n",
    "    \n",
    "    # Analyze individual items\n",
    "    items_analyzed = []\n",
    "    for gold_item, pred_item, similarity in matched_pairs:\n",
    "        gold_item_text = gold_item.get('item_text_raw', '')\n",
    "        pred_item_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        item_cer = character_error_rate(gold_item_text, pred_item_text, normalization)\n",
    "        \n",
    "        items_analyzed.append({\n",
    "            'gold_class': gold_item.get('item_class'),\n",
    "            'cer': item_cer,\n",
    "            'gold_preview': gold_item_text[:100],\n",
    "            'pred_preview': pred_item_text[:100],\n",
    "            'gold_length': len(gold_item_text),\n",
    "            'pred_length': len(pred_item_text)\n",
    "        })\n",
    "    \n",
    "    total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'cer': cer,\n",
    "        'wer': wer,\n",
    "        'matched_chars': len(gold_text),\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'match_coverage': len(gold_text) / total_gold_chars * 100 if total_gold_chars > 0 else 0,\n",
    "        'substitutions': ops['substitutions'],\n",
    "        'deletions': ops['deletions'],\n",
    "        'insertions': ops['insertions'],\n",
    "        'total_errors': ops['total'],\n",
    "        'items_analyzed': items_analyzed\n",
    "    }\n",
    "\n",
    "\n",
    "# Diagnose all pages for all three normalizations\n",
    "print(\"Running detailed page-by-page diagnostics...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_diagnostics_strict = []\n",
    "page_diagnostics_standard = []\n",
    "page_diagnostics_letters = []\n",
    "\n",
    "for page in all_pages:\n",
    "    diag_strict = diagnose_page_text_quality(page, 'strict')\n",
    "    page_diagnostics_strict.append(diag_strict)\n",
    "    \n",
    "    diag_standard = diagnose_page_text_quality(page, 'standard')\n",
    "    page_diagnostics_standard.append(diag_standard)\n",
    "    \n",
    "    diag_letters = diagnose_page_text_quality(page, 'letters_only')\n",
    "    page_diagnostics_letters.append(diag_letters)\n",
    "\n",
    "# Prepare diagnostics data\n",
    "def prepare_diagnostic_row(d):\n",
    "    \"\"\"Convert diagnostic dict to row format.\"\"\"\n",
    "    if d['matched_chars'] > 0:\n",
    "        sub_pct = d['substitutions'] / d['matched_chars'] * 100\n",
    "        del_pct = d['deletions'] / d['matched_chars'] * 100\n",
    "        ins_pct = d['insertions'] / d['matched_chars'] * 100\n",
    "    else:\n",
    "        sub_pct = del_pct = ins_pct = 0\n",
    "    \n",
    "    return {\n",
    "        'page_id': d['page_id'],\n",
    "        'cer_%': d['cer'] * 100,\n",
    "        'wer_%': d['wer'] * 100,\n",
    "        'coverage_%': d['match_coverage'],\n",
    "        'subs_%': sub_pct,\n",
    "        'dels_%': del_pct,\n",
    "        'ins_%': ins_pct,\n",
    "        'matched_chars': d['matched_chars'],\n",
    "        'total_errors': d['total_errors']\n",
    "    }\n",
    "\n",
    "df_strict = results_to_dataframe(\n",
    "    [prepare_diagnostic_row(d) for d in page_diagnostics_strict],\n",
    "    round_columns={'cer_%': 2, 'wer_%': 2, 'coverage_%': 1, 'subs_%': 2, 'dels_%': 2, 'ins_%': 2}\n",
    ")\n",
    "df_standard = results_to_dataframe(\n",
    "    [prepare_diagnostic_row(d) for d in page_diagnostics_standard],\n",
    "    round_columns={'cer_%': 2, 'wer_%': 2, 'coverage_%': 1, 'subs_%': 2, 'dels_%': 2, 'ins_%': 2}\n",
    ")\n",
    "df_letters = results_to_dataframe(\n",
    "    [prepare_diagnostic_row(d) for d in page_diagnostics_letters],\n",
    "    round_columns={'cer_%': 2, 'wer_%': 2, 'coverage_%': 1, 'subs_%': 2, 'dels_%': 2, 'ins_%': 2}\n",
    ")\n",
    "\n",
    "# Print summary tables\n",
    "print(\"=\"*80)\n",
    "print(\"PAGE-BY-PAGE TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- STRICT NORMALIZATION (preserves all whitespace) ---\")\n",
    "print(df_strict.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\")\n",
    "print(df_standard.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- LETTERS ONLY (no whitespace/punctuation) ---\")\n",
    "print(df_letters[['page_id', 'cer_%', 'coverage_%', 'subs_%', 'dels_%', 'ins_%']].to_string(index=False))\n",
    "\n",
    "# Identify worst pages (using standard normalization)\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"WORST PERFORMING PAGES (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "worst_pages = sorted(page_diagnostics_standard, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "for i, page_diag in enumerate(worst_pages, 1):\n",
    "    print(f\"\\n{i}. {page_diag['page_id']}\")\n",
    "    print(f\"   CER: {page_diag['cer']:.2%}  |  WER: {page_diag['wer']:.2%}\")\n",
    "    print(f\"   Coverage: {page_diag['match_coverage']:.1f}% of gold text\")\n",
    "    print(f\"   Errors: {page_diag['substitutions']} subs, {page_diag['deletions']} dels, {page_diag['insertions']} ins\")\n",
    "    \n",
    "    # Show worst items from this page\n",
    "    if page_diag['items_analyzed']:\n",
    "        worst_items = sorted(page_diag['items_analyzed'], key=lambda x: x['cer'], reverse=True)[:2]\n",
    "        print(f\"\\n   Worst items on this page:\")\n",
    "        for j, item in enumerate(worst_items, 1):\n",
    "            print(f\"      Item {j} ({item['gold_class']}, CER: {item['cer']:.2%}):\")\n",
    "            print(f\"         Gold: \\\"{item['gold_preview']}{'...' if item['gold_length'] > 100 else ''}\\\"\")\n",
    "            print(f\"         Pred: \\\"{item['pred_preview']}{'...' if item['pred_length'] > 100 else ''}\\\"\")\n",
    "\n",
    "# Error distribution analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ERROR TYPE DISTRIBUTION (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_errors = sum(d['total_errors'] for d in page_diagnostics_standard)\n",
    "total_subs = sum(d['substitutions'] for d in page_diagnostics_standard)\n",
    "total_dels = sum(d['deletions'] for d in page_diagnostics_standard)\n",
    "total_ins = sum(d['insertions'] for d in page_diagnostics_standard)\n",
    "\n",
    "print(f\"\\nTotal errors across all pages: {total_errors:,}\")\n",
    "print(f\"   Substitutions: {total_subs:,} ({total_subs/total_errors*100:.1f}%)\")\n",
    "print(f\"   Deletions:     {total_dels:,} ({total_dels/total_errors*100:.1f}%)\")\n",
    "print(f\"   Insertions:    {total_ins:,} ({total_ins/total_errors*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Average CER (standard): {df_standard['cer_%'].mean():.2f}%\")\n",
    "print(f\"- Pages with CER > 20%: {len(df_standard[df_standard['cer_%'] > 20])}\")\n",
    "print(f\"- Pages with CER < 5%: {len(df_standard[df_standard['cer_%'] < 5])}\")\n",
    "print(f\"- Most common error type: \" + \n",
    "      (\"Substitutions\" if total_subs > max(total_dels, total_ins) else \n",
    "       \"Deletions\" if total_dels > total_ins else \"Insertions\"))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross-Page Error Analysis\n",
    "Character-level confusion matrix and systematic error pattern detection.\n",
    "Analyzes all pages together to identify recurring OCR issues.\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def get_character_confusions(reference: str, hypothesis: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract character-level substitutions from aligned strings.\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_char, pred_char) tuples for substitutions\n",
    "    \"\"\"\n",
    "    confusions = []\n",
    "    \n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Character substitution\n",
    "            gold_substr = reference[i1:i2]\n",
    "            pred_substr = hypothesis[j1:j2]\n",
    "            \n",
    "            # For single character replacements\n",
    "            if len(gold_substr) == 1 and len(pred_substr) == 1:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "            # For multi-character replacements (like  -> oe)\n",
    "            elif len(gold_substr) > 0 and len(pred_substr) > 0:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "    \n",
    "    return confusions\n",
    "\n",
    "\n",
    "def analyze_character_patterns(confusions: list) -> dict:\n",
    "    \"\"\"\n",
    "    Detect systematic patterns in character confusions.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with pattern names and counts\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'accent_removal': 0,\n",
    "        'accent_confusion': 0,\n",
    "        'ligature_issues': 0,\n",
    "        'case_errors': 0,\n",
    "        'punctuation_errors': 0,\n",
    "        'similar_shape': 0,\n",
    "        'space_issues': 0\n",
    "    }\n",
    "    \n",
    "    accent_chars = ''\n",
    "    ligatures = ''\n",
    "    \n",
    "    for gold, pred in confusions:\n",
    "        # Accent removal ( -> e,  -> a)\n",
    "        if len(gold) == 1 and len(pred) == 1:\n",
    "            gold_base = unicodedata.normalize('NFD', gold)[0]\n",
    "            pred_normalized = unicodedata.normalize('NFD', pred)[0]\n",
    "            if gold in accent_chars and gold_base == pred:\n",
    "                patterns['accent_removal'] += 1\n",
    "            elif gold in accent_chars and pred in accent_chars and gold != pred:\n",
    "                patterns['accent_confusion'] += 1\n",
    "            elif gold.lower() == pred.lower():\n",
    "                patterns['case_errors'] += 1\n",
    "        \n",
    "        # Ligature issues ( -> oe,  -> ae)\n",
    "        if gold in ligatures and pred not in ligatures:\n",
    "            patterns['ligature_issues'] += 1\n",
    "        \n",
    "        # Similar shape confusions (common OCR errors)\n",
    "        similar_pairs = [\n",
    "            ('l', 'i'), ('i', 'l'), ('rn', 'm'), ('m', 'rn'),\n",
    "            ('cl', 'd'), ('d', 'cl'), ('o', '0'), ('0', 'o'),\n",
    "            ('1', 'l'), ('l', '1'), ('s', '5'), ('5', 's')\n",
    "        ]\n",
    "        if (gold, pred) in similar_pairs:\n",
    "            patterns['similar_shape'] += 1\n",
    "        \n",
    "        # Punctuation confusion\n",
    "        if gold in '.,;:!?\\'\"' or pred in '.,;:!?\\'\"':\n",
    "            patterns['punctuation_errors'] += 1\n",
    "        \n",
    "        # Space-related issues\n",
    "        if ' ' in gold or ' ' in pred:\n",
    "            patterns['space_issues'] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Collect all character confusions across all pages\n",
    "print(\"Analyzing character-level confusions across all pages...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_confusions = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        pred_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        # Use standard normalization for fair comparison\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "        \n",
    "        confusions = get_character_confusions(gold_norm, pred_norm)\n",
    "        all_confusions.extend(confusions)\n",
    "\n",
    "# Count confusion frequencies\n",
    "confusion_counter = Counter(all_confusions)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHARACTER CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal character substitutions: {len(all_confusions):,}\")\n",
    "print(f\"Unique confusion pairs: {len(confusion_counter):,}\")\n",
    "\n",
    "# Top 30 most common confusions\n",
    "print(\"\\nTop 30 Most Common Character Substitutions:\")\n",
    "print(f\"{'Gold  Pred':<30} {'Count':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for (gold, pred), count in confusion_counter.most_common(30):\n",
    "    # Escape special characters for display\n",
    "    gold_display = repr(gold)[1:-1] if gold in '\\n\\t\\r' else gold\n",
    "    pred_display = repr(pred)[1:-1] if pred in '\\n\\t\\r' else pred\n",
    "    \n",
    "    # Create display string\n",
    "    if len(gold) == 1 and len(pred) == 1:\n",
    "        display = f\"'{gold_display}'  '{pred_display}'\"\n",
    "    else:\n",
    "        display = f'\"{gold_display}\"  \"{pred_display}\"'\n",
    "    \n",
    "    print(f\"{display:<30} {count:<10}\")\n",
    "\n",
    "# Pattern analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC ERROR PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "patterns = analyze_character_patterns(all_confusions)\n",
    "total_categorized = sum(patterns.values())\n",
    "\n",
    "print(f\"\\nTotal confusions: {len(all_confusions):,}\")\n",
    "print(f\"Categorized: {total_categorized:,} ({total_categorized/len(all_confusions)*100:.1f}%)\")\n",
    "print(f\"Uncategorized: {len(all_confusions) - total_categorized:,} \" +\n",
    "      f\"({(len(all_confusions) - total_categorized)/len(all_confusions)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPattern Breakdown:\")\n",
    "for pattern, count in sorted(patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        pct = count / len(all_confusions) * 100\n",
    "        pattern_name = pattern.replace('_', ' ').title()\n",
    "        print(f\"   {pattern_name:<25} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Specific accent analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ACCENT & DIACRITIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accent_confusions = [(g, p) for g, p in all_confusions \n",
    "                     if len(g) == 1 and len(p) == 1 \n",
    "                     and any(c in '' for c in g)]\n",
    "\n",
    "if accent_confusions:\n",
    "    accent_counter = Counter(accent_confusions)\n",
    "    print(f\"\\nAccented character confusions: {len(accent_confusions):,}\")\n",
    "    print(\"\\nMost common accented character errors:\")\n",
    "    for (gold, pred), count in accent_counter.most_common(15):\n",
    "        print(f\"   '{gold}'  '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo accented character confusions detected.\")\n",
    "\n",
    "# Ligature analysis  \n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"LIGATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ligature_confusions = [(g, p) for g, p in all_confusions if g in '' or p in '']\n",
    "\n",
    "if ligature_confusions:\n",
    "    ligature_counter = Counter(ligature_confusions)\n",
    "    print(f\"\\nLigature-related confusions: {len(ligature_confusions):,}\")\n",
    "    print(\"\\nLigature substitutions:\")\n",
    "    for (gold, pred), count in ligature_counter.most_common(10):\n",
    "        print(f\"   '{gold}'  '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo ligature confusions detected.\")\n",
    "\n",
    "# Case sensitivity analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CASE SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "case_confusions = [(g, p) for g, p in all_confusions \n",
    "                   if len(g) == 1 and len(p) == 1 and g.lower() == p.lower() and g != p]\n",
    "\n",
    "if case_confusions:\n",
    "    case_counter = Counter(case_confusions)\n",
    "    print(f\"\\nCase-only differences: {len(case_confusions):,}\")\n",
    "    print(\"\\nMost common case errors:\")\n",
    "    for (gold, pred), count in case_counter.most_common(10):\n",
    "        print(f\"   '{gold}'  '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo case-only confusions detected.\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBased on the error analysis:\")\n",
    "\n",
    "# Check for high accent issues\n",
    "accent_pct = patterns['accent_removal'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if accent_pct > 5:\n",
    "    print(f\"\\n HIGH ACCENT REMOVAL RATE ({accent_pct:.1f}%)\")\n",
    "    print(\"   - Consider post-processing to restore accents using dictionary lookup\")\n",
    "    print(\"   - May need model fine-tuning on accented French text\")\n",
    "\n",
    "# Check for ligature issues\n",
    "ligature_pct = patterns['ligature_issues'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if ligature_pct > 2:\n",
    "    print(f\"\\n LIGATURE HANDLING ISSUES ({ligature_pct:.1f}%)\")\n",
    "    print(\"   - Ligatures (, ) being split or confused\")\n",
    "    print(\"   - Common in historical French texts\")\n",
    "\n",
    "# Check for case errors\n",
    "case_pct = patterns['case_errors'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if case_pct > 3:\n",
    "    print(f\"\\n CASE SENSITIVITY ISSUES ({case_pct:.1f}%)\")\n",
    "    print(\"   - Model confusing upper/lowercase\")\n",
    "    print(\"   - May indicate line/title detection problems\")\n",
    "\n",
    "# General observation\n",
    "if len(all_confusions) > 0:\n",
    "    unique_ratio = len(confusion_counter) / len(all_confusions)\n",
    "    if unique_ratio > 0.5:\n",
    "        print(f\"\\n ERROR DIVERSITY IS HIGH (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Errors are diverse, not systematic\")\n",
    "        print(\"   - Suggests random OCR noise rather than systematic bias\")\n",
    "    else:\n",
    "        print(f\"\\n ERROR CONCENTRATION DETECTED (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Same errors repeat frequently\")\n",
    "        print(\"   - Suggests systematic model bias that could be corrected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classification Accuracy Evaluation\n",
    "Evaluate how well the model classifies items into the five categories:\n",
    "prose, verse, ad, paratext, unknown\n",
    "\n",
    "Structure:\n",
    "1. Overall classification metrics across all pages\n",
    "2. Per-page classification breakdown\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_classification(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                           matches: List[Tuple[int, int, float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate classification accuracy on matched pairs.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Dict with classification metrics\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return {\n",
    "            'gold_classes': [],\n",
    "            'pred_classes': [],\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'accuracy': 0.0\n",
    "        }\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    gold_classes = []\n",
    "    pred_classes = []\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_classes.append(gold_item['item_class'])\n",
    "        pred_classes.append(pred_item['item_class'])\n",
    "    \n",
    "    correct = sum(1 for g, p in zip(gold_classes, pred_classes) if g == p)\n",
    "    total = len(gold_classes)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'gold_classes': gold_classes,\n",
    "        'pred_classes': pred_classes,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Collect classification data from all pages\n",
    "print(\"Evaluating classification accuracy...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_gold_classes = []\n",
    "all_pred_classes = []\n",
    "page_classification_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    result = evaluate_classification(gold_items, pred_items, matches)\n",
    "    result['page_id'] = page_id\n",
    "    page_classification_results.append(result)\n",
    "    \n",
    "    all_gold_classes.extend(result['gold_classes'])\n",
    "    all_pred_classes.extend(result['pred_classes'])\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_matched = len(all_gold_classes)\n",
    "total_correct = sum(1 for g, p in zip(all_gold_classes, all_pred_classes) if g == p)\n",
    "overall_accuracy = total_correct / total_matched if total_matched > 0 else 0.0\n",
    "\n",
    "# Store aggregate classification results for summary\n",
    "classification_aggregates = {\n",
    "    'total_matched': total_matched,\n",
    "    'total_correct': total_correct,\n",
    "    'overall_accuracy': overall_accuracy,\n",
    "    'contrib_matched': contrib_matched,\n",
    "    'contrib_correct': contrib_correct,\n",
    "    'contrib_accuracy': contrib_acc\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL CLASSIFICATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal matched items evaluated: {total_matched}\")\n",
    "print(f\"Correctly classified: {total_correct} ({overall_accuracy:.2%})\")\n",
    "print(f\"Misclassified: {total_matched - total_correct} ({(1-overall_accuracy):.2%})\")\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['prose', 'verse', 'ad', 'paratext', 'unknown']\n",
    "\n",
    "# Confusion matrix\n",
    "if total_matched > 0:\n",
    "    cm = confusion_matrix(all_gold_classes, all_pred_classes, labels=class_labels)\n",
    "    \n",
    "    print(\"\\n\\nCONFUSION MATRIX\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'':>12}\", end='')\n",
    "    for label in class_labels:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, label in enumerate(class_labels):\n",
    "        print(f\"{label:>12}\", end='')\n",
    "        for j in range(len(class_labels)):\n",
    "            print(f\"{cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\n\\nPER-CLASS METRICS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, label in enumerate(class_labels):\n",
    "        # Calculate metrics for this class\n",
    "        tp = cm[i][i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        support = cm[i, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"{label:<12} {precision:<12.2%} {recall:<12.2%} {f1:<12.3f} {support:<12}\")\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    supports = []\n",
    "    \n",
    "    for i in range(len(class_labels)):\n",
    "        tp = cm[i][i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        support = cm[i, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        supports.append(support)\n",
    "    \n",
    "    macro_precision = np.mean(precisions)\n",
    "    macro_recall = np.mean(recalls)\n",
    "    macro_f1 = np.mean(f1s)\n",
    "    \n",
    "    total_support = sum(supports)\n",
    "    weighted_precision = sum(p * s for p, s in zip(precisions, supports)) / total_support\n",
    "    weighted_recall = sum(r * s for r, s in zip(recalls, supports)) / total_support\n",
    "    weighted_f1 = sum(f * s for f, s in zip(f1s, supports)) / total_support\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Macro Avg':<12} {macro_precision:<12.2%} {macro_recall:<12.2%} {macro_f1:<12.3f} {total_support:<12}\")\n",
    "    print(f\"{'Weighted Avg':<12} {weighted_precision:<12.2%} {weighted_recall:<12.2%} {weighted_f1:<12.3f} {total_support:<12}\")\n",
    "    \n",
    "    # Most common misclassifications\n",
    "    print(\"\\n\\nMOST COMMON MISCLASSIFICATIONS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    misclass_counts = []\n",
    "    for i, gold_label in enumerate(class_labels):\n",
    "        for j, pred_label in enumerate(class_labels):\n",
    "            if i != j and cm[i][j] > 0:\n",
    "                misclass_counts.append((gold_label, pred_label, cm[i][j]))\n",
    "    \n",
    "    misclass_counts.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    if misclass_counts:\n",
    "        print(f\"{'Gold  Predicted':<30} {'Count':<10} {'% of Gold Class'}\")\n",
    "        print(\"-\"*80)\n",
    "        for gold_label, pred_label, count in misclass_counts[:10]:\n",
    "            gold_total = cm[class_labels.index(gold_label), :].sum()\n",
    "            pct = count / gold_total * 100 if gold_total > 0 else 0\n",
    "            print(f\"{gold_label}  {pred_label:<20} {count:<10} {pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"No misclassifications detected!\")\n",
    "\n",
    "# Contributions-specific analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CONTRIBUTIONS ANALYSIS (Prose + Verse)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "contrib_gold = [g for g in all_gold_classes if g in ['prose', 'verse']]\n",
    "contrib_pred = [p for g, p in zip(all_gold_classes, all_pred_classes) if g in ['prose', 'verse']]\n",
    "\n",
    "if contrib_gold:\n",
    "    contrib_correct = sum(1 for g, p in zip(contrib_gold, contrib_pred) if g == p)\n",
    "    contrib_accuracy = contrib_correct / len(contrib_gold)\n",
    "    \n",
    "    print(f\"\\nTotal contribution items: {len(contrib_gold)}\")\n",
    "    print(f\"Correctly classified: {contrib_correct} ({contrib_accuracy:.2%})\")\n",
    "    print(f\"Misclassified: {len(contrib_gold) - contrib_correct} ({(1-contrib_accuracy):.2%})\")\n",
    "    \n",
    "    # Contribution confusion\n",
    "    contrib_labels = ['prose', 'verse']\n",
    "    contrib_cm = confusion_matrix(contrib_gold, contrib_pred, labels=contrib_labels)\n",
    "    \n",
    "    print(\"\\nContributions Confusion Matrix:\")\n",
    "    print(f\"{'':>12}\", end='')\n",
    "    for label in contrib_labels:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    for i, label in enumerate(contrib_labels):\n",
    "        print(f\"{label:>12}\", end='')\n",
    "        for j in range(len(contrib_labels)):\n",
    "            print(f\"{contrib_cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "\n",
    "# Per-page classification breakdown\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PER-PAGE CLASSIFICATION BREAKDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "page_class_df_data = []\n",
    "for result in page_classification_results:\n",
    "    page_class_df_data.append({\n",
    "        'page_id': result['page_id'],\n",
    "        'total_items': result['total'],\n",
    "        'correct': result['correct'],\n",
    "        'accuracy_%': round(result['accuracy'] * 100, 1) if result['total'] > 0 else 0.0,\n",
    "        'misclassified': result['total'] - result['correct']\n",
    "    })\n",
    "\n",
    "page_class_df = pd.DataFrame(page_class_df_data)\n",
    "print(\"\\n\" + page_class_df.to_string(index=False))\n",
    "\n",
    "# Detailed per-page analysis\n",
    "print(\"\\n\\nDETAILED PER-PAGE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in page_classification_results:\n",
    "    if result['total'] == 0:\n",
    "        continue\n",
    "    \n",
    "    page_id = result['page_id']\n",
    "    gold_classes = result['gold_classes']\n",
    "    pred_classes = result['pred_classes']\n",
    "    \n",
    "    print(f\"\\n{page_id}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Accuracy: {result['accuracy']:.2%} ({result['correct']}/{result['total']})\")\n",
    "    \n",
    "    # Class distribution\n",
    "    from collections import Counter\n",
    "    gold_dist = Counter(gold_classes)\n",
    "    \n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for cls in ['prose', 'verse', 'ad', 'paratext', 'unknown']:\n",
    "        if cls in gold_dist:\n",
    "            gold_count = gold_dist[cls]\n",
    "            pred_count = sum(1 for g, p in zip(gold_classes, pred_classes) \n",
    "                           if g == cls and p == cls)\n",
    "            print(f\"   {cls:<12} {pred_count}/{gold_count} correct\")\n",
    "    \n",
    "    # Misclassifications for this page\n",
    "    misclass_page = [(g, p) for g, p in zip(gold_classes, pred_classes) if g != p]\n",
    "    if misclass_page:\n",
    "        print(f\"\\nMisclassifications ({len(misclass_page)}):\")\n",
    "        misclass_counter = Counter(misclass_page)\n",
    "        for (gold, pred), count in misclass_counter.most_common():\n",
    "            print(f\"   {gold}  {pred}: {count} time{'s' if count > 1 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Overall classification accuracy: {overall_accuracy:.2%}\")\n",
    "print(f\"- Best performing class: {class_labels[np.argmax([recalls[i] for i in range(len(class_labels))])]}\")\n",
    "print(f\"- Most challenging class: {class_labels[np.argmin([recalls[i] for i in range(len(class_labels))])]}\")\n",
    "if misclass_counts:\n",
    "    print(f\"- Most common confusion: {misclass_counts[0][0]}  {misclass_counts[0][1]} ({misclass_counts[0][2]} times)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metadata Extraction Evaluation\n",
    "Evaluate title and author extraction accuracy on matched items.\n",
    "\n",
    "Metrics:\n",
    "- Exact match: Field matches exactly\n",
    "- Partial match: String similarity above threshold\n",
    "- Presence: Field is present (not None) in both gold and pred\n",
    "- Precision: Of predicted fields, how many are correct?\n",
    "- Recall: Of gold fields, how many were extracted?\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def normalize_metadata_string(s: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Normalize metadata string for comparison.\n",
    "    - Lowercase\n",
    "    - Remove extra whitespace\n",
    "    - Remove punctuation at start/end\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip('.,;:!?')\n",
    "    return s\n",
    "\n",
    "\n",
    "def metadata_similarity(gold: Optional[str], pred: Optional[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity between two metadata strings.\n",
    "    Returns 1.0 for exact match, 0.0 for no match, partial scores for similarity.\n",
    "    \"\"\"\n",
    "    gold_norm = normalize_metadata_string(gold)\n",
    "    pred_norm = normalize_metadata_string(pred)\n",
    "    \n",
    "    if not gold_norm and not pred_norm:\n",
    "        return 1.0  # Both null\n",
    "    if not gold_norm or not pred_norm:\n",
    "        return 0.0  # One null, one not\n",
    "    \n",
    "    if gold_norm == pred_norm:\n",
    "        return 1.0  # Exact match\n",
    "    \n",
    "    # Use SequenceMatcher for partial similarity\n",
    "    return SequenceMatcher(None, gold_norm, pred_norm).ratio()\n",
    "\n",
    "\n",
    "def evaluate_metadata_field(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                            matches: List[Tuple[int, int, float]],\n",
    "                            field_name: str,\n",
    "                            similarity_threshold: float = 0.8) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a specific metadata field (title or author).\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold items\n",
    "        pred_items: List of pred items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        field_name: 'item_title' or 'item_author'\n",
    "        similarity_threshold: Minimum similarity for partial match\n",
    "    \n",
    "    Returns:\n",
    "        Dict with metrics\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return {\n",
    "            'gold_present': 0,\n",
    "            'pred_present': 0,\n",
    "            'exact_matches': 0,\n",
    "            'partial_matches': 0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'examples': []\n",
    "        }\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    gold_present = 0  # Gold has non-null value\n",
    "    pred_present = 0  # Pred has non-null value\n",
    "    exact_matches = 0\n",
    "    partial_matches = 0\n",
    "    examples = []\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_value = gold_item.get(field_name)\n",
    "        pred_value = pred_item.get(field_name)\n",
    "        \n",
    "        gold_has_value = gold_value is not None and gold_value.strip() != ''\n",
    "        pred_has_value = pred_value is not None and pred_value.strip() != ''\n",
    "        \n",
    "        if gold_has_value:\n",
    "            gold_present += 1\n",
    "        \n",
    "        if pred_has_value:\n",
    "            pred_present += 1\n",
    "        \n",
    "        if gold_has_value and pred_has_value:\n",
    "            similarity = metadata_similarity(gold_value, pred_value)\n",
    "            \n",
    "            if similarity == 1.0:\n",
    "                exact_matches += 1\n",
    "                partial_matches += 1\n",
    "            elif similarity >= similarity_threshold:\n",
    "                partial_matches += 1\n",
    "                # Store example for partial matches\n",
    "                if len(examples) < 5:\n",
    "                    examples.append({\n",
    "                        'gold': gold_value,\n",
    "                        'pred': pred_value,\n",
    "                        'similarity': similarity,\n",
    "                        'item_class': gold_item.get('item_class')\n",
    "                    })\n",
    "    \n",
    "    # Calculate metrics based on partial matches (more lenient)\n",
    "    # Precision: of predicted values, how many match gold?\n",
    "    precision = partial_matches / pred_present if pred_present > 0 else 0.0\n",
    "    \n",
    "    # Recall: of gold values, how many were extracted?\n",
    "    recall = partial_matches / gold_present if gold_present > 0 else 0.0\n",
    "    \n",
    "    # F1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'gold_present': gold_present,\n",
    "        'pred_present': pred_present,\n",
    "        'exact_matches': exact_matches,\n",
    "        'partial_matches': partial_matches,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'examples': examples\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate metadata across all pages\n",
    "print(\"Evaluating metadata extraction (titles and authors)...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_metadata_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Evaluate titles\n",
    "    title_metrics = evaluate_metadata_field(gold_items, pred_items, matches, 'item_title')\n",
    "    \n",
    "    # Evaluate authors\n",
    "    author_metrics = evaluate_metadata_field(gold_items, pred_items, matches, 'item_author')\n",
    "    \n",
    "    page_metadata_results.append({\n",
    "        'page_id': page_id,\n",
    "        'title': title_metrics,\n",
    "        'author': author_metrics\n",
    "    })\n",
    "\n",
    "# Aggregate overall metrics\n",
    "overall_title = {\n",
    "    'gold_present': sum(r['title']['gold_present'] for r in page_metadata_results),\n",
    "    'pred_present': sum(r['title']['pred_present'] for r in page_metadata_results),\n",
    "    'exact_matches': sum(r['title']['exact_matches'] for r in page_metadata_results),\n",
    "    'partial_matches': sum(r['title']['partial_matches'] for r in page_metadata_results)\n",
    "}\n",
    "\n",
    "overall_author = {\n",
    "    'gold_present': sum(r['author']['gold_present'] for r in page_metadata_results),\n",
    "    'pred_present': sum(r['author']['pred_present'] for r in page_metadata_results),\n",
    "    'exact_matches': sum(r['author']['exact_matches'] for r in page_metadata_results),\n",
    "    'partial_matches': sum(r['author']['partial_matches'] for r in page_metadata_results)\n",
    "}\n",
    "\n",
    "# Calculate overall precision, recall, F1\n",
    "overall_title['precision'] = (overall_title['partial_matches'] / overall_title['pred_present'] \n",
    "                              if overall_title['pred_present'] > 0 else 0.0)\n",
    "overall_title['recall'] = (overall_title['partial_matches'] / overall_title['gold_present'] \n",
    "                          if overall_title['gold_present'] > 0 else 0.0)\n",
    "overall_title['f1'] = (2 * overall_title['precision'] * overall_title['recall'] / \n",
    "                       (overall_title['precision'] + overall_title['recall']) \n",
    "                       if (overall_title['precision'] + overall_title['recall']) > 0 else 0.0)\n",
    "\n",
    "overall_author['precision'] = (overall_author['partial_matches'] / overall_author['pred_present'] \n",
    "                               if overall_author['pred_present'] > 0 else 0.0)\n",
    "overall_author['recall'] = (overall_author['partial_matches'] / overall_author['gold_present'] \n",
    "                           if overall_author['gold_present'] > 0 else 0.0)\n",
    "overall_author['f1'] = (2 * overall_author['precision'] * overall_author['recall'] / \n",
    "                        (overall_author['precision'] + overall_author['recall']) \n",
    "                        if (overall_author['precision'] + overall_author['recall']) > 0 else 0.0)\n",
    "\n",
    "# Print overall results\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL METADATA EXTRACTION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- TITLE EXTRACTION ---\")\n",
    "print(f\"Gold items with titles:       {overall_title['gold_present']}\")\n",
    "print(f\"Predicted items with titles:  {overall_title['pred_present']}\")\n",
    "print(f\"Exact matches:                {overall_title['exact_matches']}\")\n",
    "print(f\"Partial matches (80% sim):   {overall_title['partial_matches']}\")\n",
    "print(f\"\\nPrecision: {overall_title['precision']:.2%}\")\n",
    "print(f\"Recall:    {overall_title['recall']:.2%}\")\n",
    "print(f\"F1 Score:  {overall_title['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n--- AUTHOR EXTRACTION ---\")\n",
    "print(f\"Gold items with authors:      {overall_author['gold_present']}\")\n",
    "print(f\"Predicted items with authors: {overall_author['pred_present']}\")\n",
    "print(f\"Exact matches:                {overall_author['exact_matches']}\")\n",
    "print(f\"Partial matches (80% sim):   {overall_author['partial_matches']}\")\n",
    "print(f\"\\nPrecision: {overall_author['precision']:.2%}\")\n",
    "print(f\"Recall:    {overall_author['recall']:.2%}\")\n",
    "print(f\"F1 Score:  {overall_author['f1']:.3f}\")\n",
    "\n",
    "# Collect examples from all pages\n",
    "all_title_examples = []\n",
    "all_author_examples = []\n",
    "\n",
    "for result in page_metadata_results:\n",
    "    all_title_examples.extend(result['title']['examples'])\n",
    "    all_author_examples.extend(result['author']['examples'])\n",
    "\n",
    "# Show examples of partial matches (not exact)\n",
    "if all_title_examples:\n",
    "    print(\"\\n\\nEXAMPLES OF PARTIAL TITLE MATCHES\")\n",
    "    print(\"-\"*80)\n",
    "    for i, ex in enumerate(all_title_examples[:5], 1):\n",
    "        print(f\"\\n{i}. {ex['item_class'].upper()} (Similarity: {ex['similarity']:.2%})\")\n",
    "        print(f\"   Gold: \\\"{ex['gold']}\\\"\")\n",
    "        print(f\"   Pred: \\\"{ex['pred']}\\\"\")\n",
    "\n",
    "if all_author_examples:\n",
    "    print(\"\\n\\nEXAMPLES OF PARTIAL AUTHOR MATCHES\")\n",
    "    print(\"-\"*80)\n",
    "    for i, ex in enumerate(all_author_examples[:5], 1):\n",
    "        print(f\"\\n{i}. {ex['item_class'].upper()} (Similarity: {ex['similarity']:.2%})\")\n",
    "        print(f\"   Gold: \\\"{ex['gold']}\\\"\")\n",
    "        print(f\"   Pred: \\\"{ex['pred']}\\\"\")\n",
    "\n",
    "# Per-page breakdown\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PER-PAGE METADATA EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "page_meta_df_data = []\n",
    "for result in page_metadata_results:\n",
    "    page_meta_df_data.append({\n",
    "        'page_id': result['page_id'],\n",
    "        'title_gold': result['title']['gold_present'],\n",
    "        'title_pred': result['title']['pred_present'],\n",
    "        'title_F1': round(result['title']['f1'], 3),\n",
    "        'author_gold': result['author']['gold_present'],\n",
    "        'author_pred': result['author']['pred_present'],\n",
    "        'author_F1': round(result['author']['f1'], 3)\n",
    "    })\n",
    "\n",
    "page_meta_df = pd.DataFrame(page_meta_df_data)\n",
    "print(\"\\n\" + page_meta_df.to_string(index=False))\n",
    "\n",
    "# Detailed per-page analysis\n",
    "print(\"\\n\\nDETAILED PER-PAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in page_metadata_results:\n",
    "    page_id = result['page_id']\n",
    "    title_metrics = result['title']\n",
    "    author_metrics = result['author']\n",
    "    \n",
    "    if title_metrics['gold_present'] == 0 and author_metrics['gold_present'] == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{page_id}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if title_metrics['gold_present'] > 0:\n",
    "        print(f\"Titles:  {title_metrics['partial_matches']}/{title_metrics['gold_present']} extracted \" +\n",
    "              f\"(P: {title_metrics['precision']:.2%}, R: {title_metrics['recall']:.2%}, \" +\n",
    "              f\"F1: {title_metrics['f1']:.3f})\")\n",
    "    else:\n",
    "        print(f\"Titles:  No gold titles on this page\")\n",
    "    \n",
    "    if author_metrics['gold_present'] > 0:\n",
    "        print(f\"Authors: {author_metrics['partial_matches']}/{author_metrics['gold_present']} extracted \" +\n",
    "              f\"(P: {author_metrics['precision']:.2%}, R: {author_metrics['recall']:.2%}, \" +\n",
    "              f\"F1: {author_metrics['f1']:.3f})\")\n",
    "    else:\n",
    "        print(f\"Authors: No gold authors on this page\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Title extraction F1: {overall_title['f1']:.3f}\")\n",
    "print(f\"- Author extraction F1: {overall_author['f1']:.3f}\")\n",
    "print(f\"- Title exact match rate: {overall_title['exact_matches']}/{overall_title['gold_present']} \" +\n",
    "      f\"({overall_title['exact_matches']/overall_title['gold_present']*100:.1f}%)\" \n",
    "      if overall_title['gold_present'] > 0 else \"- Title exact match rate: N/A\")\n",
    "print(f\"- Author exact match rate: {overall_author['exact_matches']}/{overall_author['gold_present']} \" +\n",
    "      f\"({overall_author['exact_matches']/overall_author['gold_present']*100:.1f}%)\"\n",
    "      if overall_author['gold_present'] > 0 else \"- Author exact match rate: N/A\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838969fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Continuation Tracking Evaluation\n",
    "\n",
    "Evaluates the accuracy of continuation fields (is_continuation, continues_on_next_page)\n",
    "across ALL items in the dataset, including unmatched items.\n",
    "\n",
    "Fields are treated as binary:\n",
    "- True = continuation exists\n",
    "- False/None = no continuation (treated identically)\n",
    "\n",
    "Evaluation logic:\n",
    "- Matched items: Compare gold vs pred continuation fields directly\n",
    "- Unmatched gold items with continuation=True: False Negatives (model missed them)\n",
    "- Unmatched pred items with continuation=True: False Positives (model hallucinated them)\n",
    "\n",
    "Metrics: Precision, Recall, F1 for each field\n",
    "Reports: Global aggregates first, then per-page breakdown\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_continuation_all_items(\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict],\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    unmatched_gold: Set[int],\n",
    "    unmatched_pred: Set[int]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy across ALL items.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: Gold standard items\n",
    "        pred_items: Predicted items\n",
    "        matches: List of (gold_idx, pred_idx, similarity) tuples\n",
    "        unmatched_gold: Set of unmatched gold indices\n",
    "        unmatched_pred: Set of unmatched pred indices\n",
    "        \n",
    "    Returns:\n",
    "        Dict with metrics for is_continuation and continues_on_next_page\n",
    "    \"\"\"\n",
    "    # Initialize counters for both fields\n",
    "    is_cont_tp = is_cont_fp = is_cont_fn = is_cont_tn = 0\n",
    "    continues_tp = continues_fp = continues_fn = continues_tn = 0\n",
    "    \n",
    "    # 1. Evaluate matched items\n",
    "    for gold_idx, pred_idx, _ in matches:\n",
    "        gold_item = gold_items[gold_idx]\n",
    "        pred_item = pred_items[pred_idx]\n",
    "        \n",
    "        # Evaluate is_continuation\n",
    "        gold_is_cont = gold_item.get('is_continuation') is True\n",
    "        pred_is_cont = pred_item.get('is_continuation') is True\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_tp += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_fp += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_fn += 1\n",
    "        else:\n",
    "            is_cont_tn += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page') is True\n",
    "        pred_continues = pred_item.get('continues_on_next_page') is True\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_tp += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_fp += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_fn += 1\n",
    "        else:\n",
    "            continues_tn += 1\n",
    "    \n",
    "    # 2. Evaluate unmatched gold items (missed continuations = False Negatives)\n",
    "    for gold_idx in unmatched_gold:\n",
    "        gold_item = gold_items[gold_idx]\n",
    "        \n",
    "        # If gold has continuation=True but item wasn't matched, that's a FN\n",
    "        if gold_item.get('is_continuation') is True:\n",
    "            is_cont_fn += 1\n",
    "        \n",
    "        if gold_item.get('continues_on_next_page') is True:\n",
    "            continues_fn += 1\n",
    "    \n",
    "    # 3. Evaluate unmatched pred items (hallucinated continuations = False Positives)\n",
    "    for pred_idx in unmatched_pred:\n",
    "        pred_item = pred_items[pred_idx]\n",
    "        \n",
    "        # If pred has continuation=True but item wasn't matched, that's a FP\n",
    "        if pred_item.get('is_continuation') is True:\n",
    "            is_cont_fp += 1\n",
    "        \n",
    "        if pred_item.get('continues_on_next_page') is True:\n",
    "            continues_fp += 1\n",
    "    \n",
    "    # Calculate metrics for is_continuation\n",
    "    is_cont_p = is_cont_tp / (is_cont_tp + is_cont_fp) if (is_cont_tp + is_cont_fp) > 0 else 0.0\n",
    "    is_cont_r = is_cont_tp / (is_cont_tp + is_cont_fn) if (is_cont_tp + is_cont_fn) > 0 else 0.0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0.0\n",
    "    \n",
    "    # Calculate metrics for continues_on_next_page\n",
    "    continues_p = continues_tp / (continues_tp + continues_fp) if (continues_tp + continues_fp) > 0 else 0.0\n",
    "    continues_r = continues_tp / (continues_tp + continues_fn) if (continues_tp + continues_fn) > 0 else 0.0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            'tp': is_cont_tp,\n",
    "            'fp': is_cont_fp,\n",
    "            'fn': is_cont_fn,\n",
    "            'tn': is_cont_tn,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            'tp': continues_tp,\n",
    "            'fp': continues_fp,\n",
    "            'fn': continues_fn,\n",
    "            'tn': continues_tn,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        },\n",
    "        'n_matched': len(matches),\n",
    "        'n_unmatched_gold': len(unmatched_gold),\n",
    "        'n_unmatched_pred': len(unmatched_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate continuation tracking on all pages\n",
    "print(\"Evaluating continuation tracking (all items)...\")\n",
    "print()\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    unmatched_gold = page['unmatched_gold']\n",
    "    unmatched_pred = page['unmatched_pred']\n",
    "    page_name = page['page_name']\n",
    "    \n",
    "    result = evaluate_continuation_all_items(\n",
    "        gold_items, pred_items, matches, \n",
    "        unmatched_gold, unmatched_pred\n",
    "    )\n",
    "    result['page'] = page_name\n",
    "    \n",
    "    continuation_results.append(result)\n",
    "\n",
    "# Aggregate global metrics\n",
    "total_is_cont = {\n",
    "    'tp': sum(r['is_continuation']['tp'] for r in continuation_results),\n",
    "    'fp': sum(r['is_continuation']['fp'] for r in continuation_results),\n",
    "    'fn': sum(r['is_continuation']['fn'] for r in continuation_results),\n",
    "    'tn': sum(r['is_continuation']['tn'] for r in continuation_results)\n",
    "}\n",
    "\n",
    "total_continues = {\n",
    "    'tp': sum(r['continues_on_next_page']['tp'] for r in continuation_results),\n",
    "    'fp': sum(r['continues_on_next_page']['fp'] for r in continuation_results),\n",
    "    'fn': sum(r['continues_on_next_page']['fn'] for r in continuation_results),\n",
    "    'tn': sum(r['continues_on_next_page']['tn'] for r in continuation_results)\n",
    "}\n",
    "\n",
    "# Calculate global metrics\n",
    "is_cont_p = total_is_cont['tp'] / (total_is_cont['tp'] + total_is_cont['fp']) if (total_is_cont['tp'] + total_is_cont['fp']) > 0 else 0.0\n",
    "is_cont_r = total_is_cont['tp'] / (total_is_cont['tp'] + total_is_cont['fn']) if (total_is_cont['tp'] + total_is_cont['fn']) > 0 else 0.0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0.0\n",
    "\n",
    "continues_p = total_continues['tp'] / (total_continues['tp'] + total_continues['fp']) if (total_continues['tp'] + total_continues['fp']) > 0 else 0.0\n",
    "continues_r = total_continues['tp'] / (total_continues['tp'] + total_continues['fn']) if (total_continues['tp'] + total_continues['fn']) > 0 else 0.0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0.0\n",
    "\n",
    "# Store aggregate continuation results for summary\n",
    "continuation_aggregates = {\n",
    "    'is_cont_tp': is_cont_tp,\n",
    "    'is_cont_fp': is_cont_fp,\n",
    "    'is_cont_fn': is_cont_fn,\n",
    "    'is_cont_precision': is_cont_p,\n",
    "    'is_cont_recall': is_cont_r,\n",
    "    'is_cont_f1': is_cont_f1,\n",
    "    'continues_tp': continues_tp,\n",
    "    'continues_fp': continues_fp,\n",
    "    'continues_fn': continues_fn,\n",
    "    'continues_precision': continues_p,\n",
    "    'continues_recall': continues_r,\n",
    "    'continues_f1': continues_f1\n",
    "}\n",
    "\n",
    "total_matched = sum(r['n_matched'] for r in continuation_results)\n",
    "total_unmatched_gold = sum(r['n_unmatched_gold'] for r in continuation_results)\n",
    "total_unmatched_pred = sum(r['n_unmatched_pred'] for r in continuation_results)\n",
    "\n",
    "# Count how many items have True values in gold\n",
    "gold_is_cont_count = total_is_cont['tp'] + total_is_cont['fn']\n",
    "gold_continues_count = total_continues['tp'] + total_continues['fn']\n",
    "\n",
    "# Count how many True values the model predicted\n",
    "pred_is_cont_count = total_is_cont['tp'] + total_is_cont['fp']\n",
    "pred_continues_count = total_continues['tp'] + total_continues['fp']\n",
    "\n",
    "# Calculate quantity mismatch\n",
    "is_cont_mismatch = pred_is_cont_count - gold_is_cont_count\n",
    "continues_mismatch = pred_continues_count - gold_continues_count\n",
    "\n",
    "# Print global summary\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"CONTINUATION TRACKING - GLOBAL SUMMARY (All Items)\")\n",
    "print(f\"{'='*70}\")\n",
    "print()\n",
    "print(f\"Dataset coverage:\")\n",
    "print(f\"  Matched items:        {total_matched}\")\n",
    "print(f\"  Unmatched gold items: {total_unmatched_gold}\")\n",
    "print(f\"  Unmatched pred items: {total_unmatched_pred}\")\n",
    "print()\n",
    "print(f\"is_continuation field:\")\n",
    "print(f\"  Gold positives (True):     {gold_is_cont_count}\")\n",
    "print(f\"  Pred positives (True):     {pred_is_cont_count}  (mismatch: {is_cont_mismatch:+d})\")\n",
    "print(f\"  True Positives (TP):       {total_is_cont['tp']}\")\n",
    "print(f\"  False Positives (FP):      {total_is_cont['fp']}\")\n",
    "print(f\"  False Negatives (FN):      {total_is_cont['fn']}\")\n",
    "print(f\"  True Negatives (TN):       {total_is_cont['tn']}\")\n",
    "print(f\"  Precision:                 {is_cont_p:.2%}\")\n",
    "print(f\"  Recall:                    {is_cont_r:.2%}\")\n",
    "print(f\"  F1 Score:                  {is_cont_f1:.3f}\")\n",
    "print()\n",
    "print(f\"continues_on_next_page field:\")\n",
    "print(f\"  Gold positives (True):     {gold_continues_count}\")\n",
    "print(f\"  Pred positives (True):     {pred_continues_count}  (mismatch: {continues_mismatch:+d})\")\n",
    "print(f\"  True Positives (TP):       {total_continues['tp']}\")\n",
    "print(f\"  False Positives (FP):      {total_continues['fp']}\")\n",
    "print(f\"  False Negatives (FN):      {total_continues['fn']}\")\n",
    "print(f\"  True Negatives (TN):       {total_continues['tn']}\")\n",
    "print(f\"  Precision:                 {continues_p:.2%}\")\n",
    "print(f\"  Recall:                    {continues_r:.2%}\")\n",
    "print(f\"  F1 Score:                  {continues_f1:.3f}\")\n",
    "print()\n",
    "\n",
    "# Per-page breakdown\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"PER-PAGE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "print()\n",
    "\n",
    "for result in continuation_results:\n",
    "    page = result['page']\n",
    "    n_matched = result['n_matched']\n",
    "    n_unmatch_gold = result['n_unmatched_gold']\n",
    "    n_unmatch_pred = result['n_unmatched_pred']\n",
    "    \n",
    "    is_cont = result['is_continuation']\n",
    "    continues = result['continues_on_next_page']\n",
    "    \n",
    "    # Count gold positives for this page\n",
    "    page_is_cont_gold = is_cont['tp'] + is_cont['fn']\n",
    "    page_continues_gold = continues['tp'] + continues['fn']\n",
    "    \n",
    "    # Count pred positives for this page\n",
    "    page_is_cont_pred = is_cont['tp'] + is_cont['fp']\n",
    "    page_continues_pred = continues['tp'] + continues['fp']\n",
    "    \n",
    "    # Calculate mismatch\n",
    "    is_cont_mismatch = page_is_cont_pred - page_is_cont_gold\n",
    "    continues_mismatch = page_continues_pred - page_continues_gold\n",
    "    \n",
    "    print(f\"{page}\")\n",
    "    print(f\"  Matched: {n_matched}  |  Unmatched gold: {n_unmatch_gold}  |  Unmatched pred: {n_unmatch_pred}\")\n",
    "    print()\n",
    "    print(f\"  is_continuation:\")\n",
    "    print(f\"    Gold: {page_is_cont_gold}  Pred: {page_is_cont_pred}  (mismatch: {is_cont_mismatch:+d})\")\n",
    "    print(f\"    TP: {is_cont['tp']}  FP: {is_cont['fp']}  FN: {is_cont['fn']}\")\n",
    "    print(f\"    P: {is_cont['precision']:.2%}  R: {is_cont['recall']:.2%}  F1: {is_cont['f1']:.3f}\")\n",
    "    print()\n",
    "    print(f\"  continues_on_next_page:\")\n",
    "    print(f\"    Gold: {page_continues_gold}  Pred: {page_continues_pred}  (mismatch: {continues_mismatch:+d})\")\n",
    "    print(f\"    TP: {continues['tp']}  FP: {continues['fp']}  FN: {continues['fn']}\")\n",
    "    print(f\"    P: {continues['precision']:.2%}  R: {continues['recall']:.2%}  F1: {continues['f1']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Summary Report\n",
    "\n",
    "Synthesizes all evaluation findings into a summary that aggregates metrics across all evaluation dimensions and identifies problematic pages\n",
    "\"\"\"\n",
    "\n",
    "# Collect aggregate metrics from all previous evaluations\n",
    "# These values should be computed from the previous cells\n",
    "\n",
    "def create_summary_table() -> pd.DataFrame:\n",
    "    summary_data = []\n",
    "    \n",
    "    # Structure detection (from diagnostics)\n",
    "    total_gold_items = sum(len(page['gold_items']) for page in all_pages)\n",
    "    total_matches = sum(len(page['matches']) for page in all_pages)\n",
    "    total_gold_contrib = sum(\n",
    "        len([item for item in page['gold_items'] if item['item_class'] in ['prose', 'verse']])\n",
    "        for page in all_pages\n",
    "    )\n",
    "    contrib_matches = sum(\n",
    "        len([m for m in page['matches'] \n",
    "             if page['gold_items'][m[0]]['item_class'] in ['prose', 'verse']])\n",
    "        for page in all_pages\n",
    "    )\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Structure Detection',\n",
    "        'Metric': 'Item Match Rate',\n",
    "        'Value': f\"{(total_matches/total_gold_items)*100:.1f}%\",\n",
    "        'Details': f\"{total_matches}/{total_gold_items} items matched\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Structure Detection',\n",
    "        'Metric': 'Contribution Match Rate',\n",
    "        'Value': f\"{(contrib_matches/total_gold_contrib)*100:.1f}%\",\n",
    "        'Details': f\"{contrib_matches}/{total_gold_contrib} contributions matched\"\n",
    "    })\n",
    "    \n",
    "    # Text quality (from existing variables)\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'CER (Standard, All)',\n",
    "        'Value': f\"{avg_oa_all['cer_standard']:.2%}\",\n",
    "        'Details': 'Order-agnostic evaluation'\n",
    "    })\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'CER (Standard, Contrib)',\n",
    "        'Value': f\"{avg_sa_contrib['cer_standard']:.2%}\",\n",
    "        'Details': 'Structure-aware, matched only'\n",
    "    })\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'Coverage',\n",
    "        'Value': f\"{total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\",\n",
    "        'Details': 'Contribution chars successfully matched'\n",
    "    })\n",
    "    \n",
    "    # Classification (from stored aggregates)\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Classification',\n",
    "        'Metric': 'Overall Accuracy',\n",
    "        'Value': f\"{classification_aggregates['overall_accuracy']*100:.1f}%\",\n",
    "        'Details': f\"{classification_aggregates['total_correct']}/{classification_aggregates['total_matched']} items\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Classification',\n",
    "        'Metric': 'Contribution Accuracy',\n",
    "        'Value': f\"{classification_aggregates['contrib_accuracy']*100:.1f}%\",\n",
    "        'Details': f\"{classification_aggregates['contrib_correct']}/{classification_aggregates['contrib_matched']} prose/verse items\"\n",
    "    })\n",
    "    \n",
    "    # Metadata (from stored aggregates)\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Metadata Extraction',\n",
    "        'Metric': 'Title F1',\n",
    "        'Value': f\"{metadata_aggregates['title_f1']/100:.3f}\",\n",
    "        'Details': f\"P: {metadata_aggregates['title_precision']:.0f}%, R: {metadata_aggregates['title_recall']:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Metadata Extraction',\n",
    "        'Metric': 'Author F1',\n",
    "        'Value': f\"{metadata_aggregates['author_f1']/100:.3f}\",\n",
    "        'Details': f\"P: {metadata_aggregates['author_precision']:.0f}%, R: {metadata_aggregates['author_recall']:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    # Continuation (from stored aggregates)\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Continuation Tracking',\n",
    "        'Metric': 'is_continuation F1',\n",
    "        'Value': f\"{continuation_aggregates['is_cont_f1']:.3f}\",\n",
    "        'Details': f\"P: {continuation_aggregates['is_cont_precision']*100:.0f}%, R: {continuation_aggregates['is_cont_recall']*100:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Continuation Tracking',\n",
    "        'Metric': 'continues_on_next F1',\n",
    "        'Value': f\"{continuation_aggregates['continues_f1']:.3f}\",\n",
    "        'Details': f\"P: {continuation_aggregates['continues_precision']*100:.0f}%, R: {continuation_aggregates['continues_recall']*100:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "def identify_problem_pages():\n",
    "    \"\"\"\n",
    "    Identify pages with significant issues across multiple dimensions.\n",
    "    Returns list of (page_name, issues) tuples.\n",
    "    \"\"\"\n",
    "    problem_pages = {}\n",
    "    \n",
    "    for page in all_pages:\n",
    "        page_name = page['page_name']\n",
    "        issues = []\n",
    "        \n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        # Skip empty pages\n",
    "        if len(gold_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Issue 1: Low match rate\n",
    "        match_rate = len(matches) / len(gold_items) if len(gold_items) > 0 else 0\n",
    "        if match_rate < 0.5:\n",
    "            issues.append(f\"Low match rate ({match_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Issue 2: Zero predictions\n",
    "        if len(pred_items) == 0:\n",
    "            issues.append(\"Zero predictions\")\n",
    "        \n",
    "        # Issue 3: Low contribution match rate\n",
    "        gold_contrib = [item for item in gold_items if item['item_class'] in ['prose', 'verse']]\n",
    "        if gold_contrib:\n",
    "            contrib_matches = filter_matches_by_class(matches, gold_items, ['prose', 'verse'])\n",
    "            contrib_match_rate = len(contrib_matches) / len(gold_contrib) if len(gold_contrib) > 0 else 0\n",
    "            if contrib_match_rate < 0.5:\n",
    "                issues.append(f\"Low contribution matching ({contrib_match_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Issue 4: High classification errors\n",
    "        if matches:\n",
    "            misclassified = 0\n",
    "            for g_idx, p_idx, _ in matches:\n",
    "                if gold_items[g_idx]['item_class'] != pred_items[p_idx]['item_class']:\n",
    "                    misclassified += 1\n",
    "            error_rate = misclassified / len(matches)\n",
    "            if error_rate > 0.3:\n",
    "                issues.append(f\"High classification errors ({error_rate*100:.0f}%)\")\n",
    "        \n",
    "        if issues:\n",
    "            problem_pages[page_name] = issues\n",
    "    \n",
    "    return problem_pages\n",
    "\n",
    "\n",
    "# Generate summary table\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1 OCR EVALUATION - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "summary_df = create_summary_table()\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(\"-\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Problem pages\n",
    "print(\"=\" * 80)\n",
    "print(\"PROBLEM PAGES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "problem_pages = identify_problem_pages()\n",
    "if problem_pages:\n",
    "    print(f\"Identified {len(problem_pages)} pages with significant issues:\")\n",
    "    print()\n",
    "    \n",
    "    # Sort by number of issues\n",
    "    sorted_problems = sorted(problem_pages.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for page_name, issues in sorted_problems:\n",
    "        print(f\"{page_name}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   {issue}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No pages identified with critical issues.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
