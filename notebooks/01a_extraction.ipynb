{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c08446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Extraction\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian-ramirez/Documents/These/Code/magazine_graphs/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 OCR Extraction - Mistral Document AI\n",
    "\n",
    "Extracts structured page-level data from photographs of historical French literary magazines.\n",
    "\n",
    "Input:  PDF files in data/raw/\n",
    "Output: JSON files per page in data/predictions/\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "# Project imports\n",
    "from utils.paths import PROJECT_ROOT, RAW_DATA, PREDICTIONS, ensure_data_dirs\n",
    "from utils.config import MISTRAL_CONFIG, EXTRACTION_CONFIG\n",
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "from mistralai import Mistral\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, **kwargs: x\n",
    "\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"extraction\")\n",
    "\n",
    "\n",
    "print(\"Stage 1 OCR Extraction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73eb107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  Source directory: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/raw\n",
      "  Output directory: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n",
      "  Model: mistral-ocr-latest\n",
      "  Overwrite existing: False\n",
      "  API key: Configured\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Path Setup\n",
    "\"\"\"\n",
    "\n",
    "# Ensure directories exist\n",
    "ensure_data_dirs()\n",
    "\n",
    "# Use centralized paths\n",
    "SRC_ROOT = RAW_DATA\n",
    "DST_PAGES = PREDICTIONS\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Source directory: {SRC_ROOT}\")\n",
    "print(f\"  Output directory: {DST_PAGES}\")\n",
    "print(f\"  Model: {MISTRAL_CONFIG.model_name}\")\n",
    "print(f\"  Overwrite existing: {EXTRACTION_CONFIG.overwrite}\")\n",
    "\n",
    "# API key setup and Mistral client\n",
    "def get_mistral_client() -> Mistral:\n",
    "    \"\"\"Initialize Mistral client with API key.\"\"\"\n",
    "    return Mistral(api_key=MISTRAL_CONFIG.get_api_key())\n",
    "\n",
    "print(\"  API key: Configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31d488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "  Loaded: Stage1PageModel\n",
      "  Item classes: typing.Literal['prose', 'verse', 'ad', 'paratext', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load Stage 1 Schema\n",
    "\"\"\"\n",
    "\n",
    "# Import schema\n",
    "from schemas.stage1_page import Stage1PageModel, Stage1Item, ITEM_CLASS\n",
    "\n",
    "# Generate response format for Mistral API\n",
    "DOC_ANNOT_FMT = response_format_from_pydantic_model(Stage1PageModel)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "print(f\"  Loaded: {Stage1PageModel.__name__}\")\n",
    "print(f\"  Item classes: {ITEM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087a2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PDF Processing Utilities\n",
    "\"\"\"\n",
    "\n",
    "def count_pages(pdf_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Count number of pages in a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        \n",
    "    Returns:\n",
    "        Number of pages (0 if file cannot be read)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdf_path.open(\"rb\") as fh:\n",
    "            try:\n",
    "                reader = PdfReader(fh, strict=False)\n",
    "            except TypeError:\n",
    "                reader = PdfReader(fh)  # fallback if 'strict' arg unsupported, because I'm unsure\n",
    "            if getattr(reader, \"is_encrypted\", False) and reader.decrypt(\"\") == 0:\n",
    "                logger.warning(f\"Encrypted PDF (cannot decrypt): {pdf_path.name}\")\n",
    "                return 0\n",
    "            return len(reader.pages)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read {pdf_path.name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def encode_file_to_data_url(path: Path, mime: str = \"application/pdf\") -> str:\n",
    "    \"\"\"\n",
    "    Encode file as base64 data URL for Mistral API.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to file\n",
    "        mime: MIME type\n",
    "        \n",
    "    Returns:\n",
    "        Data URL string (data:<mime>;base64,<encoded_content>)\n",
    "    \"\"\"\n",
    "    b64 = base64.b64encode(path.read_bytes()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n",
    "\n",
    "def chunks(seq, size):\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i:i+size]\n",
    "\n",
    "def parse_annotation_response(resp) -> dict:\n",
    "    \"\"\"\n",
    "    Extract annotation dict from Mistral OCR response.\n",
    "    \n",
    "    Handles different response formats:\n",
    "    - resp.document_annotation (string or dict)\n",
    "    - resp.pages[0].document_annotation (fallback)\n",
    "    \n",
    "    Args:\n",
    "        resp: Mistral OCR API response object\n",
    "        \n",
    "    Returns:\n",
    "        Annotation dict (empty dict if parsing fails)\n",
    "    \"\"\"\n",
    "    # Try top-level document_annotation first\n",
    "    ann = getattr(resp, \"document_annotation\", None)\n",
    "    \n",
    "    if isinstance(ann, str):\n",
    "        try:\n",
    "            return json.loads(ann)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    elif isinstance(ann, dict):\n",
    "        return ann or {}\n",
    "    \n",
    "    # Fall back to pages array\n",
    "    pages = getattr(resp, \"pages\", None) or []\n",
    "    if pages:\n",
    "        page_ann = getattr(pages[0], \"document_annotation\", None)\n",
    "        \n",
    "        if isinstance(page_ann, str):\n",
    "            try:\n",
    "                return json.loads(page_ann)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "        elif isinstance(page_ann, dict):\n",
    "            return page_ann or {}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def call_with_retry(fn, *, retries: int = 3, base_delay: float = 1.0, max_delay: float = 8.0):\n",
    "    \"\"\"\n",
    "    Call function with exponential backoff retry logic.\n",
    "    \n",
    "    Args:\n",
    "        fn: Function to call (no arguments)\n",
    "        retries: Maximum number of retry attempts\n",
    "        base_delay: Initial delay between retries (seconds)\n",
    "        max_delay: Maximum delay between retries (seconds)\n",
    "        \n",
    "    Returns:\n",
    "        Function result\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If all retries fail\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            \n",
    "            delay = min(max_delay, base_delay * (2 ** attempt))\n",
    "            jitter = delay * (1 + 0.25 * random.random())\n",
    "            \n",
    "            logger.warning(f\"API call failed ({e}). Retrying in {jitter:.1f}s...\")\n",
    "            time.sleep(jitter)\n",
    "\n",
    "def validate_extraction(annot: dict, page_number: int, pdf_name: str) -> tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate extracted annotation for common issues.\n",
    "    \n",
    "    Args:\n",
    "        annot: Annotation dictionary\n",
    "        page_number: Page number (1-indexed)\n",
    "        pdf_name: PDF filename for logging\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, list_of_warnings)\n",
    "    \"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Check if items exist\n",
    "    if \"items\" not in annot:\n",
    "        warnings.append(f\"Missing 'items' field\")\n",
    "        return False, warnings\n",
    "    \n",
    "    items = annot[\"items\"]\n",
    "    \n",
    "    # Check for empty pages (valid but worth noting)\n",
    "    if len(items) == 0:\n",
    "        warnings.append(f\"Zero items extracted (possibly blank page)\")\n",
    "    \n",
    "    # Check for suspiciously short items\n",
    "    for idx, item in enumerate(items):\n",
    "        text = item.get(\"item_text_raw\", \"\")\n",
    "        if len(text) < 3:\n",
    "            warnings.append(f\"Item {idx} has very short text ({len(text)} chars)\")\n",
    "    \n",
    "    # Schema validation with Pydantic\n",
    "    try:\n",
    "        Stage1PageModel(**annot)\n",
    "    except ValidationError as e:\n",
    "        warnings.append(f\"Schema validation failed: {e}\")\n",
    "        return False, warnings\n",
    "    \n",
    "    return True, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d650fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 14:58:00,824 | INFO | Found 2 PDF(s) to process\n",
      "2025-10-23 14:58:00,920 | INFO | Processing La_Plume_bpt6k1185893k_1_10_1889.pdf (14 pages)\n",
      "2025-10-23 14:58:00,925 | INFO | ✓ La_Plume_bpt6k1185893k_1_10_1889.pdf: 0 written, 14 skipped, 0 failed\n",
      "2025-10-23 14:58:01,046 | INFO | Processing La_Plume_bpt6k1212187t_15-11-1893.pdf (34 pages)\n",
      "2025-10-23 14:58:01,048 | INFO | ✓ La_Plume_bpt6k1212187t_15-11-1893.pdf: 0 written, 34 skipped, 0 failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE\n",
      "============================================================\n",
      "Total pages:   48\n",
      "  Written:     0\n",
      "  Skipped:     48\n",
      "  Failed:      0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Core Extraction Function - Per-Page Processing\n",
    "\"\"\"\n",
    "\n",
    "def extract_pdf_pages(\n",
    "    pdf_path: Path,\n",
    "    out_root: Path = DST_PAGES,\n",
    "    overwrite: bool = None,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extract structured data from all pages of a PDF.\n",
    "    \n",
    "    Creates one JSON file per page in:\n",
    "    out_root / <pdf_name> / <pdf_name>__page-001.json\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        out_root: Root directory for output\n",
    "        overwrite: Override CONFIG[\"overwrite\"] if specified\n",
    "        \n",
    "    Returns:\n",
    "        Dict with statistics: {\"written\": n, \"skipped\": n, \"failed\": n, \"total\": n}\n",
    "    \"\"\"\n",
    "    # Count pages\n",
    "    n_pages = count_pages(pdf_path)\n",
    "    if n_pages == 0:\n",
    "        logger.warning(f\"No pages found in {pdf_path.name}\")\n",
    "        return {\"written\": 0, \"skipped\": 0, \"failed\": 0, \"total\": 0}\n",
    "    \n",
    "    # Setup output directory\n",
    "    rel_path = pdf_path.relative_to(SRC_ROOT).with_suffix(\"\")\n",
    "    out_dir = out_root / rel_path\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize client\n",
    "    client = get_mistral_client()\n",
    "    data_url = encode_file_to_data_url(pdf_path)\n",
    "    \n",
    "    # Use CONFIG value unless overridden\n",
    "    should_overwrite = EXTRACTION_CONFIG.overwrite if overwrite is None else overwrite\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\"written\": 0, \"skipped\": 0, \"failed\": 0, \"total\": n_pages}\n",
    "    \n",
    "    # Process each page\n",
    "    logger.info(f\"Processing {pdf_path.name} ({n_pages} pages)\")\n",
    "    \n",
    "    for page_idx in tqdm(range(n_pages), desc=f\"  {pdf_path.name}\", leave=False):\n",
    "        page_num = page_idx + 1\n",
    "        out_json = out_dir / f\"{pdf_path.stem}__page-{page_num:0{EXTRACTION_CONFIG.zero_pad}d}.json\"\n",
    "        \n",
    "        # Skip if exists and not overwriting\n",
    "        if out_json.exists() and not should_overwrite:\n",
    "            stats[\"skipped\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Call API with retry logic\n",
    "        try:\n",
    "            def _call():\n",
    "                return client.ocr.process(\n",
    "                    model=MISTRAL_CONFIG.model_name,\n",
    "                    document={\"type\": \"document_url\", \"document_url\": data_url},\n",
    "                    pages=[page_idx],\n",
    "                    document_annotation_format=DOC_ANNOT_FMT,\n",
    "                    include_image_base64=False,\n",
    "                )\n",
    "            \n",
    "            resp = call_with_retry(\n",
    "                _call,\n",
    "                retries=MISTRAL_CONFIG.max_retries,\n",
    "                base_delay=MISTRAL_CONFIG.base_delay,\n",
    "                max_delay=MISTRAL_CONFIG.max_delay,\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Page {page_num} failed after {MISTRAL_CONFIG.max_retries} retries: {e}\")\n",
    "            stats[\"failed\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Parse response\n",
    "        annot = parse_annotation_response(resp) or {}\n",
    "        \n",
    "        # Ensure items key exists\n",
    "        if \"items\" not in annot:\n",
    "            annot[\"items\"] = []\n",
    "        \n",
    "        # Validate (but don't block writing)\n",
    "        is_valid, warnings = validate_extraction(annot, page_num, pdf_path.name)\n",
    "        if warnings:\n",
    "            for warning in warnings:\n",
    "                logger.warning(f\"Page {page_num}: {warning}\")\n",
    "        \n",
    "        # Write output\n",
    "        try:\n",
    "            out_json.write_text(\n",
    "                json.dumps(annot, ensure_ascii=False, indent=2),\n",
    "                encoding=\"utf-8\"\n",
    "            )\n",
    "            stats[\"written\"] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write {out_json.name}: {e}\")\n",
    "            stats[\"failed\"] += 1\n",
    "    \n",
    "    # Log summary\n",
    "    logger.info(\n",
    "        f\"✓ {pdf_path.name}: \"\n",
    "        f\"{stats['written']} written, \"\n",
    "        f\"{stats['skipped']} skipped, \"\n",
    "        f\"{stats['failed']} failed\"\n",
    "    )\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def extract_all_pdfs(src_root: Path = SRC_ROOT) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extract all PDFs in source directory.\n",
    "    \n",
    "    Returns:\n",
    "        Combined statistics across all PDFs\n",
    "    \"\"\"\n",
    "    pdfs = sorted([p for p in src_root.rglob(\"*.pdf\") if p.is_file()])\n",
    "    \n",
    "    if not pdfs:\n",
    "        logger.warning(f\"No PDF files found in {src_root}\")\n",
    "        return {\"written\": 0, \"skipped\": 0, \"failed\": 0, \"total\": 0}\n",
    "    \n",
    "    logger.info(f\"Found {len(pdfs)} PDF(s) to process\")\n",
    "    \n",
    "    # Accumulate statistics\n",
    "    total_stats = {\"written\": 0, \"skipped\": 0, \"failed\": 0, \"total\": 0}\n",
    "    \n",
    "    for pdf_path in pdfs:\n",
    "        stats = extract_pdf_pages(pdf_path)\n",
    "        for key in total_stats:\n",
    "            total_stats[key] += stats[key]\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total pages:   {total_stats['total']}\")\n",
    "    print(f\"  Written:     {total_stats['written']}\")\n",
    "    print(f\"  Skipped:     {total_stats['skipped']}\")\n",
    "    print(f\"  Failed:      {total_stats['failed']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "# Execute extraction on all PDFs in data/raw/\n",
    "results = extract_all_pdfs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
