{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Evaluation\n",
      "\n",
      "\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "Predictions: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/interim_pages/La_Plume_bpt6k1185893k_1_10_1889\n",
      "\n",
      "Dataset:\n",
      "  Gold files: 14\n",
      "  Pred files: 14\n",
      "  Matching pairs: 14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Path setup\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import schemas for validation\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "\n",
    "# Paths\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "print(\"Stage 1 OCR Evaluation\")\n",
    "print(\"\\n\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Gold standard: {GOLD_DIR}\")\n",
    "print(f\"Predictions: {PRED_DIR}\")\n",
    "\n",
    "# Find common files\n",
    "def load_page_pairs() -> List[Tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Match gold standard files with prediction files by filename.\n",
    "    Returns list of (gold_path, pred_path) tuples.\n",
    "    \"\"\"\n",
    "    gold_files = {f.name: f for f in GOLD_DIR.glob(\"*.json\")}\n",
    "    pred_files = {f.name: f for f in PRED_DIR.glob(\"*.json\")}\n",
    "    \n",
    "    common_names = set(gold_files.keys()) & set(pred_files.keys())\n",
    "    \n",
    "    pairs = [(gold_files[name], pred_files[name]) for name in sorted(common_names)]\n",
    "    \n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Gold files: {len(gold_files)}\")\n",
    "    print(f\"  Pred files: {len(pred_files)}\")\n",
    "    print(f\"  Matching pairs: {len(pairs)}\")\n",
    "    \n",
    "    if len(pairs) < len(gold_files):\n",
    "        missing = set(gold_files.keys()) - set(pred_files.keys())\n",
    "        print(f\"Warning: {len(missing)} gold standard pages without predictions:\")\n",
    "        for name in sorted(missing):\n",
    "            print(f\"   - {name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "page_pairs = load_page_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2a9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Item Matching Configuration\n",
      "\n",
      "\n",
      "Similarity threshold: 0.7\n",
      "\n",
      "\n",
      "Item Matching Test\n",
      "\n",
      "\n",
      "\n",
      "Test page: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Gold items: 8\n",
      "  Pred items: 5\n",
      "  Matches found: 4\n",
      "  Unmatched gold: 4\n",
      "  Unmatched pred: 1\n",
      "  Average match quality: 92.47%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Match gold items to predicted items using content-based text similarity.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_THRESHOLD = 0.7  # Minimum text similarity to consider a match (0.0-1.0)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Configuration\")\n",
    "print(\"\\n\")\n",
    "print(f\"Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "        \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item, find the best-matching unmatched pred item.\n",
    "        Accept the match if similarity exceeds threshold.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair and match items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "\n",
    "# Test matching on first page\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Test\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if page_pairs:\n",
    "    test_gold, test_pred = page_pairs[0]\n",
    "    test_result = load_and_match_page(test_gold, test_pred)\n",
    "    \n",
    "    print(f\"\\nTest page: {test_result['page_name']}\")\n",
    "    print(f\"  Gold items: {len(test_result['gold_items'])}\")\n",
    "    print(f\"  Pred items: {len(test_result['pred_items'])}\")\n",
    "    print(f\"  Matches found: {len(test_result['matches'])}\")\n",
    "    print(f\"  Unmatched gold: {len(test_result['unmatched_gold'])}\")\n",
    "    print(f\"  Unmatched pred: {len(test_result['unmatched_pred'])}\")\n",
    "    \n",
    "    if test_result['matches']:\n",
    "        avg_score = sum(score for _, _, score in test_result['matches']) / len(test_result['matches'])\n",
    "        print(f\"  Average match quality: {avg_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7322404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running diagnostics on all pages...\n",
      "\n",
      "\n",
      "\n",
      "SUMMARY TABLE\n",
      "\n",
      "\n",
      "                                   page_id  gold_items  pred_items  matched  match_rate_%  contrib_match_rate_%  avg_similarity  gold_cont_in  pred_cont_in  gold_cont_out  pred_cont_out                                                            flags\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001           8           5        4          50.0                   0.0           0.925             0             0              0              0                                                   COUNT_MISMATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002           2           2        2         100.0                   0.0           1.000             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003           3           3        1          33.3                   0.0           0.713             0             1              1              2                                           LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004           5           4        3          60.0                 100.0           0.942             1             1              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005           5           4        1          20.0                  33.3           0.755             1             0              1              1                                           LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006           6           4        3          50.0                  75.0           0.967             1             0              1              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007           2           3        1          50.0                   0.0           1.000             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008           0           0        0           0.0                   0.0           0.000             0             0              0              0                              ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009           7           5        4          57.1                  80.0           0.953             1             0              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010           6           4        4          66.7                 100.0           0.971             1             0              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011           3           1        0           0.0                   0.0           0.000             1             1              1              1                             ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012           8           0        0           0.0                   0.0           0.000             1             0              0              0 ZERO_PREDS, ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB, COUNT_MISMATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013          11          11       10          90.9                   0.0           0.951             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014           4           6        4         100.0                   0.0           0.968             0             0              0              0                                                                 \n",
      "\n",
      "\n",
      "================================================================================\n",
      "DETAILED REPORTS\n",
      "================================================================================\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-001 ===\n",
      "Items: 8 gold, 5 pred\n",
      "Matches: 4 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   8 gold, 5 pred, 4 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.925\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1, 2, 3, 7]\n",
      "Unmatched pred items: [1]\n",
      "\n",
      "FLAGS: COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-002 ===\n",
      "Items: 2 gold, 2 pred\n",
      "Matches: 2 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 2 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-003 ===\n",
      "Items: 3 gold, 3 pred\n",
      "Matches: 1 (33.3% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 1 matched (50.0%)\n",
      "  prose      1 gold, 2 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 2 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.713\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 2 pred\n",
      "\n",
      "Unmatched gold items: [1, 2]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-004 ===\n",
      "Items: 5 gold, 4 pred\n",
      "Matches: 3 (60.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 3 pred, 2 matched (100.0%)\n",
      "  verse      1 gold, 1 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 4 pred, 3 matched (100.0%)\n",
      "Avg similarity: 0.942\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1]\n",
      "Unmatched pred items: [1]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-005 ===\n",
      "Items: 5 gold, 4 pred\n",
      "Matches: 1 (20.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 0 matched (0.0%)\n",
      "  verse      1 gold, 2 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 4 pred, 1 matched (33.3%)\n",
      "Avg similarity: 0.755\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 4]\n",
      "Unmatched pred items: [0, 1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-006 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 3 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 3 matched (75.0%)\n",
      "Avg similarity: 0.967\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 5]\n",
      "Unmatched pred items: [3]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-007 ===\n",
      "Items: 2 gold, 3 pred\n",
      "Matches: 1 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 3 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-008 ===\n",
      "Items: 0 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-009 ===\n",
      "Items: 7 gold, 5 pred\n",
      "Matches: 4 (57.1% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 1 pred, 2 matched (100.0%)\n",
      "  verse      3 gold, 4 pred, 2 matched (66.7%)\n",
      "\n",
      "Contributions: 5 gold, 5 pred, 4 matched (80.0%)\n",
      "Avg similarity: 0.953\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 4]\n",
      "Unmatched pred items: [2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-010 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 4 (66.7% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 2 matched (100.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 4 matched (100.0%)\n",
      "Avg similarity: 0.971\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1]\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-011 ===\n",
      "Items: 3 gold, 1 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      1 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2]\n",
      "Unmatched pred items: [0]\n",
      "\n",
      "FLAGS: ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-012 ===\n",
      "Items: 8 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   4 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 0 pred, 0 matched (0.0%)\n",
      "  verse      2 gold, 0 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 4 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB, COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-013 ===\n",
      "Items: 11 gold, 11 pred\n",
      "Matches: 10 (90.9% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   11 gold, 1 pred, 10 matched (90.9%)\n",
      "  prose      0 gold, 10 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 10 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.951\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [7]\n",
      "Unmatched pred items: [7]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-014 ===\n",
      "Items: 4 gold, 6 pred\n",
      "Matches: 4 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  ad         1 gold, 0 pred, 1 matched (100.0%)\n",
      "  paratext   3 gold, 5 pred, 3 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.968\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-Level Diagnostics\n",
    "Generate diagnostic metrics for each page based on item matches.\n",
    "\"\"\"\n",
    "\n",
    "def diagnose_page(page_id: str, gold_items: list, pred_items: list, matches: list) -> dict:\n",
    "    \"\"\"\n",
    "    Generate diagnostic metrics for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page_id: Page identifier\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with diagnostic metrics\n",
    "    \"\"\"\n",
    "    # Count items by class\n",
    "    gold_by_class = {}\n",
    "    pred_by_class = {}\n",
    "    \n",
    "    for item in gold_items:\n",
    "        item_class = item['item_class']\n",
    "        gold_by_class[item_class] = gold_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    for item in pred_items:\n",
    "        item_class = item['item_class']\n",
    "        pred_by_class[item_class] = pred_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    # Count contributions (prose + verse)\n",
    "    gold_contrib = gold_by_class.get('prose', 0) + gold_by_class.get('verse', 0)\n",
    "    pred_contrib = pred_by_class.get('prose', 0) + pred_by_class.get('verse', 0)\n",
    "    \n",
    "    # Filter matches by contribution class\n",
    "    contrib_matches = [\n",
    "        (g_idx, p_idx, score) for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in ('prose', 'verse')\n",
    "    ]\n",
    "    \n",
    "    # Calculate match rates\n",
    "    match_rate = (len(matches) / len(gold_items) * 100) if gold_items else 0\n",
    "    contrib_match_rate = (len(contrib_matches) / gold_contrib * 100) if gold_contrib else 0\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    avg_similarity = (sum(score for _, _, score in matches) / len(matches)) if matches else 0\n",
    "    \n",
    "    # Count continuation flags\n",
    "    gold_cont_in = sum(1 for item in gold_items if item.get('is_continuation') is True)\n",
    "    pred_cont_in = sum(1 for item in pred_items if item.get('is_continuation') is True)\n",
    "    gold_cont_out = sum(1 for item in gold_items if item.get('continues_on_next_page') is True)\n",
    "    pred_cont_out = sum(1 for item in pred_items if item.get('continues_on_next_page') is True)\n",
    "    \n",
    "    # Track matched indices\n",
    "    matched_gold = {g_idx for g_idx, _, _ in matches}\n",
    "    matched_pred = {p_idx for _, p_idx, _ in matches}\n",
    "    \n",
    "    unmatched_gold = [i for i in range(len(gold_items)) if i not in matched_gold]\n",
    "    unmatched_pred = [i for i in range(len(pred_items)) if i not in matched_pred]\n",
    "    \n",
    "    # Count matches by class\n",
    "    matches_by_class = {}\n",
    "    for g_idx, p_idx, score in matches:\n",
    "        item_class = gold_items[g_idx]['item_class']\n",
    "        matches_by_class[item_class] = matches_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items),\n",
    "        'matched': len(matches),\n",
    "        'match_rate': match_rate,\n",
    "        'contrib_match_rate': contrib_match_rate,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'gold_cont_in': gold_cont_in,\n",
    "        'pred_cont_in': pred_cont_in,\n",
    "        'gold_cont_out': gold_cont_out,\n",
    "        'pred_cont_out': pred_cont_out,\n",
    "        'gold_by_class': gold_by_class,\n",
    "        'pred_by_class': pred_by_class,\n",
    "        'matches_by_class': matches_by_class,\n",
    "        'gold_contrib': gold_contrib,\n",
    "        'pred_contrib': pred_contrib,\n",
    "        'contrib_matched': len(contrib_matches),\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def flag_page(metrics: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate flags for problematic pages based on metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary from diagnose_page()\n",
    "        \n",
    "    Returns:\n",
    "        Comma-separated string of flags, or empty string if no issues\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    if metrics['pred_items'] == 0:\n",
    "        flags.append('ZERO_PREDS')\n",
    "    \n",
    "    if metrics['matched'] == 0:\n",
    "        flags.append('ZERO_MATCHES')\n",
    "    \n",
    "    if metrics['match_rate'] < 50:\n",
    "        flags.append('LOW_MATCH')\n",
    "    \n",
    "    if metrics['gold_contrib'] > 0 and metrics['contrib_match_rate'] < 60:\n",
    "        flags.append('LOW_CONTRIB')\n",
    "    \n",
    "    if abs(metrics['gold_items'] - metrics['pred_items']) >= 3:\n",
    "        flags.append('COUNT_MISMATCH')\n",
    "    \n",
    "    return ', '.join(flags)\n",
    "\n",
    "\n",
    "def run_diagnostics(page_pairs: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run diagnostics on all pages and generate summary table and detailed reports.\n",
    "    \n",
    "    Args:\n",
    "        page_pairs: List of (gold_path, pred_path) tuples from load_page_pairs()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary metrics for all pages\n",
    "    \"\"\"\n",
    "    print(\"Running diagnostics on all pages...\\n\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for gold_path, pred_path in page_pairs:\n",
    "        # Extract page_id from filename\n",
    "        page_id = gold_path.stem\n",
    "        \n",
    "        # Load and match page\n",
    "        result = load_and_match_page(gold_path, pred_path)\n",
    "        gold_items = result['gold_items']\n",
    "        pred_items = result['pred_items']\n",
    "        matches = result['matches']\n",
    "        \n",
    "        # Generate metrics\n",
    "        metrics = diagnose_page(page_id, gold_items, pred_items, matches)\n",
    "        metrics['flags'] = flag_page(metrics)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for m in all_metrics:\n",
    "        summary_data.append({\n",
    "            'page_id': m['page_id'],\n",
    "            'gold_items': m['gold_items'],\n",
    "            'pred_items': m['pred_items'],\n",
    "            'matched': m['matched'],\n",
    "            'match_rate_%': round(m['match_rate'], 1),\n",
    "            'contrib_match_rate_%': round(m['contrib_match_rate'], 1),\n",
    "            'avg_similarity': round(m['avg_similarity'], 3),\n",
    "            'gold_cont_in': m['gold_cont_in'],\n",
    "            'pred_cont_in': m['pred_cont_in'],\n",
    "            'gold_cont_out': m['gold_cont_out'],\n",
    "            'pred_cont_out': m['pred_cont_out'],\n",
    "            'flags': m['flags']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print detailed reports for all pages\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for m in all_metrics:\n",
    "        print(f\"\\n=== Page {m['page_id']} ===\")\n",
    "        print(f\"Items: {m['gold_items']} gold, {m['pred_items']} pred\")\n",
    "        print(f\"Matches: {m['matched']} ({m['match_rate']:.1f}% match rate)\")\n",
    "        \n",
    "        print(\"\\nBy class:\")\n",
    "        all_classes = sorted(set(m['gold_by_class'].keys()) | set(m['pred_by_class'].keys()))\n",
    "        for cls in all_classes:\n",
    "            gold_count = m['gold_by_class'].get(cls, 0)\n",
    "            pred_count = m['pred_by_class'].get(cls, 0)\n",
    "            matched_count = m['matches_by_class'].get(cls, 0)\n",
    "            match_pct = (matched_count / gold_count * 100) if gold_count > 0 else 0\n",
    "            print(f\"  {cls:10s} {gold_count} gold, {pred_count} pred, {matched_count} matched ({match_pct:.1f}%)\")\n",
    "\n",
    "        \n",
    "        print(f\"\\nContributions: {m['gold_contrib']} gold, {m['pred_contrib']} pred, \"\n",
    "              f\"{m['contrib_matched']} matched ({m['contrib_match_rate']:.1f}%)\")\n",
    "        print(f\"Avg similarity: {m['avg_similarity']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nContinuations:\")\n",
    "        print(f\"  is_continuation: {m['gold_cont_in']} gold, {m['pred_cont_in']} pred\")\n",
    "        print(f\"  continues_on_next_page: {m['gold_cont_out']} gold, {m['pred_cont_out']} pred\")\n",
    "        \n",
    "        print(f\"\\nUnmatched gold items: {m['unmatched_gold']}\")\n",
    "        print(f\"Unmatched pred items: {m['unmatched_pred']}\")\n",
    "        \n",
    "        if m['flags']:\n",
    "            print(f\"\\nFLAGS: {m['flags']}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostic_df = run_diagnostics(page_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fcf45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and matching all pages...\n",
      "Loaded 14 pages\n",
      "Total matches across all pages: 37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation Helpers\n",
    "Utility functions for filtering matches and loading all pages efficiently.\n",
    "These helpers are used by the evaluation cells that follow.\n",
    "\"\"\"\n",
    "\n",
    "def filter_matches_by_class(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    item_classes: List[str]\n",
    ") -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Filter matches to only include items of specified classes.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        item_classes: List of classes to include (e.g., ['prose', 'verse'])\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of matches\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (g_idx, p_idx, score) \n",
    "        for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in item_classes\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_matched_pairs(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict]\n",
    ") -> List[Tuple[Dict, Dict, float]]:\n",
    "    \"\"\"\n",
    "    Convert match indices to actual item pairs.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_item, pred_item, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (gold_items[g_idx], pred_items[p_idx], score)\n",
    "        for g_idx, p_idx, score in matches\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_all_pages(page_pairs: List[Tuple[Path, Path]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and match all pages at once for efficient batch evaluation.\n",
    "    \n",
    "    Args:\n",
    "        page_pairs: List of (gold_path, pred_path) tuples from load_page_pairs()\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, one per page, each containing:\n",
    "        - page_id: Page identifier\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "    \n",
    "    for gold_path, pred_path in page_pairs:\n",
    "        result = load_and_match_page(gold_path, pred_path)\n",
    "        result['page_id'] = gold_path.stem\n",
    "        all_pages.append(result)\n",
    "    \n",
    "    return all_pages\n",
    "\n",
    "\n",
    "# Load all pages once for reuse in subsequent evaluation cells\n",
    "print(\"Loading and matching all pages...\")\n",
    "all_pages = load_all_pages(page_pairs)\n",
    "print(f\"Loaded {len(all_pages)} pages\")\n",
    "print(f\"Total matches across all pages: {sum(len(page['matches']) for page in all_pages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text quality...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEXT QUALITY SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. ORDER-AGNOSTIC EVALUATION\n",
      "   (Pure OCR quality, reading order irrelevant)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 14.83%  |  WER: 17.66%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 14.83%  |  WER: 17.66%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 13.48%\n",
      "\n",
      "   Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 18.29%  |  WER: 20.85%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 18.29%  |  WER: 20.85%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 17.94%\n",
      "\n",
      "======================================================================\n",
      "2. STRUCTURE-AWARE EVALUATION\n",
      "   (OCR quality on matched content only)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   Matched Content - All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 15.13%  |  WER: 21.12%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 14.61%  |  WER: 21.12%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 13.81%\n",
      "      Coverage: 27,797 chars matched (59.0% of gold)\n",
      "      Unmatched: 19,350 chars (41.1% of gold)\n",
      "\n",
      "   Matched Content - Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 13.05%  |  WER: 17.63%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 12.42%  |  WER: 17.63%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 12.11%\n",
      "      Coverage: 17,510 chars matched (48.8% of gold)\n",
      "      Unmatched: 18,365 chars (51.2% of gold)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION GUIDE:\n",
      "----------------------------------------------------------------------\n",
      "Strict: Most conservative\n",
      "Standard: Fair baseline - normalizes whitespace\n",
      "Letters Only: Most lenient - pure character recognition quality\n",
      "\n",
      "======================================================================\n",
      "\n",
      "KEY INSIGHTS:\n",
      "- Pure OCR quality (standard normalization): 14.83%\n",
      "- Letter recognition quality: 13.48%\n",
      "- Structure failures (unmatched content): 41.1%\n",
      "- Contributions:\n",
      "    Standard CER: 12.42%\n",
      "    Successfully matched: 48.8%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Quality Evaluation\n",
    "Calculate CER and WER using two complementary approaches:\n",
    "1. Order-agnostic: Pure OCR quality regardless of reading order\n",
    "2. Structure-aware: OCR quality on properly aligned content via matching\n",
    "\n",
    "Each approach calculates three normalization levels:\n",
    "- Strict: Preserves all whitespace (including \\n vs \\n\\n differences)\n",
    "- Standard: Normalizes whitespace to single spaces (RECOMMENDED)\n",
    "- Letters Only: Removes all whitespace and punctuation (pure character recognition)\n",
    "\n",
    "References:\n",
    "- Flexible Character Accuracy (FCA) for handling reading order issues:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "- Token sort ratio for order-agnostic OCR comparison:\n",
    "  https://urban-institute.medium.com/choosing-the-right-ocr-service-for-extracting-text-data-d7830399ec5\n",
    "- Unicode normalization and whitespace handling in OCR evaluation:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_text_strict(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strict normalization: only Unicode NFC normalization.\n",
    "    Preserves all whitespace, punctuation, and capitalization.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "\n",
    "def normalize_text_standard(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standard normalization for fair OCR evaluation:\n",
    "    - Unicode NFC normalization\n",
    "    - All whitespace (spaces, tabs, newlines) â†’ single space\n",
    "    - Preserves punctuation and capitalization\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text_letters_only(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Letter-only normalization for pure character recognition quality:\n",
    "    - Unicode NFC normalization\n",
    "    - Remove all whitespace\n",
    "    - Remove all punctuation\n",
    "    - Preserves capitalization and diacritics\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'[^\\w]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def character_error_rate(reference: str, hypothesis: str, normalization: str = 'strict') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Character Error Rate using Levenshtein distance.\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: OCR output text\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        CER = (insertions + deletions + substitutions) / total_reference_chars\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        ref = normalize_text_letters_only(reference)\n",
    "        hyp = normalize_text_letters_only(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    if not ref:\n",
    "        return 1.0 if hyp else 0.0\n",
    "    distance = Levenshtein.distance(ref, hyp)\n",
    "    return distance / len(ref)\n",
    "\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str, normalization: str = 'strict') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate using Levenshtein distance on words.\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: OCR output text\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        WER = (insertions + deletions + substitutions) / total_reference_words\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        # For letters only, WER doesn't make sense without word boundaries\n",
    "        # So we use standard normalization\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    if not ref_words:\n",
    "        return 1.0 if hyp_words else 0.0\n",
    "    distance = Levenshtein.distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "\n",
    "def token_sort_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Sort tokens (words) alphabetically for order-agnostic comparison.\n",
    "    This removes the impact of reading order on text similarity.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return ' '.join(sorted(tokens))\n",
    "\n",
    "\n",
    "def evaluate_order_agnostic(gold_items: List[Dict], pred_items: List[Dict], \n",
    "                            item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality without considering reading order.\n",
    "    Uses token sort ratio approach - sorts all words before comparison.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        item_classes: If provided, filter to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER for each normalization level, and text statistics\n",
    "    \"\"\"\n",
    "    # Filter by class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items if item['item_class'] in item_classes]\n",
    "        pred_items = [item for item in pred_items if item['item_class'] in item_classes]\n",
    "    \n",
    "    # Concatenate all text\n",
    "    gold_text = ' '.join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = ' '.join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    # Sort tokens for order-agnostic comparison\n",
    "    gold_sorted = token_sort_text(gold_text)\n",
    "    pred_sorted = token_sort_text(pred_text)\n",
    "    \n",
    "    # Calculate for all three normalization levels\n",
    "    results = {\n",
    "        'cer_strict': character_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'wer_strict': word_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'cer_standard': character_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'wer_standard': word_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'cer_letters': character_error_rate(gold_sorted, pred_sorted, 'letters_only'),\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split())\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_structure_aware(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                             matches: List[Tuple[int, int, float]],\n",
    "                             item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality on matched pairs, respecting document structure.\n",
    "    Only compares content that was successfully aligned via matching.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        item_classes: If provided, filter matches to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with matched CER/WER for each normalization level and unmatched content statistics\n",
    "    \"\"\"\n",
    "    # Filter matches by class if specified\n",
    "    if item_classes:\n",
    "        filtered_matches = filter_matches_by_class(matches, gold_items, item_classes)\n",
    "    else:\n",
    "        filtered_matches = matches\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(filtered_matches, gold_items, pred_items)\n",
    "    \n",
    "    # Calculate CER/WER on matched content for all normalization levels\n",
    "    if matched_pairs:\n",
    "        # Concatenate matched texts in gold order\n",
    "        gold_matched_text = ' '.join(gold_item.get('item_text_raw', '') \n",
    "                                     for gold_item, _, _ in matched_pairs)\n",
    "        pred_matched_text = ' '.join(pred_item.get('item_text_raw', '') \n",
    "                                     for _, pred_item, _ in matched_pairs)\n",
    "        \n",
    "        cer_strict = character_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        wer_strict = word_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        cer_standard = character_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        wer_standard = word_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        cer_letters = character_error_rate(gold_matched_text, pred_matched_text, 'letters_only')\n",
    "        \n",
    "        matched_gold_chars = len(gold_matched_text)\n",
    "        matched_pred_chars = len(pred_matched_text)\n",
    "    else:\n",
    "        cer_strict = 0.0\n",
    "        wer_strict = 0.0\n",
    "        cer_standard = 0.0\n",
    "        wer_standard = 0.0\n",
    "        cer_letters = 0.0\n",
    "        matched_gold_chars = 0\n",
    "        matched_pred_chars = 0\n",
    "    \n",
    "    # Calculate unmatched content\n",
    "    matched_gold_indices = {g_idx for g_idx, _, _ in filtered_matches}\n",
    "    matched_pred_indices = {p_idx for _, p_idx, _ in filtered_matches}\n",
    "    \n",
    "    if item_classes:\n",
    "        # Only count unmatched items of the specified classes\n",
    "        unmatched_gold_items = [\n",
    "            gold_items[i] for i in range(len(gold_items))\n",
    "            if i not in matched_gold_indices and gold_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        unmatched_pred_items = [\n",
    "            pred_items[i] for i in range(len(pred_items))\n",
    "            if i not in matched_pred_indices and pred_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                              for item in gold_items if item['item_class'] in item_classes)\n",
    "    else:\n",
    "        unmatched_gold_items = [gold_items[i] for i in range(len(gold_items)) \n",
    "                               if i not in matched_gold_indices]\n",
    "        unmatched_pred_items = [pred_items[i] for i in range(len(pred_items)) \n",
    "                               if i not in matched_pred_indices]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    unmatched_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_gold_items)\n",
    "    unmatched_pred_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_pred_items)\n",
    "    \n",
    "    return {\n",
    "        'cer_strict': cer_strict,\n",
    "        'wer_strict': wer_strict,\n",
    "        'cer_standard': cer_standard,\n",
    "        'wer_standard': wer_standard,\n",
    "        'cer_letters': cer_letters,\n",
    "        'matched_gold_chars': matched_gold_chars,\n",
    "        'matched_pred_chars': matched_pred_chars,\n",
    "        'unmatched_gold_chars': unmatched_gold_chars,\n",
    "        'unmatched_pred_chars': unmatched_pred_chars,\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'matched_percentage': (matched_gold_chars / total_gold_chars * 100) if total_gold_chars else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate text quality across all pages\n",
    "print(\"Evaluating text quality...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "order_agnostic_all = []\n",
    "order_agnostic_contrib = []\n",
    "structure_aware_all = []\n",
    "structure_aware_contrib = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Order-agnostic evaluation\n",
    "    oa_all = evaluate_order_agnostic(gold_items, pred_items)\n",
    "    oa_all['page_id'] = page_id\n",
    "    order_agnostic_all.append(oa_all)\n",
    "    \n",
    "    oa_contrib = evaluate_order_agnostic(gold_items, pred_items, \n",
    "                                         item_classes=['prose', 'verse'])\n",
    "    oa_contrib['page_id'] = page_id\n",
    "    order_agnostic_contrib.append(oa_contrib)\n",
    "    \n",
    "    # Structure-aware evaluation\n",
    "    sa_all = evaluate_structure_aware(gold_items, pred_items, matches)\n",
    "    sa_all['page_id'] = page_id\n",
    "    structure_aware_all.append(sa_all)\n",
    "    \n",
    "    sa_contrib = evaluate_structure_aware(gold_items, pred_items, matches,\n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    sa_contrib['page_id'] = page_id\n",
    "    structure_aware_contrib.append(sa_contrib)\n",
    "\n",
    "# Calculate averages for order-agnostic evaluation\n",
    "avg_oa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in order_agnostic_all) / len(order_agnostic_all)\n",
    "}\n",
    "\n",
    "contrib_with_content = [r for r in order_agnostic_contrib if r['gold_chars'] > 0]\n",
    "avg_oa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in contrib_with_content) / len(contrib_with_content)\n",
    "}\n",
    "\n",
    "# Calculate averages for structure-aware evaluation\n",
    "sa_all_with_matches = [r for r in structure_aware_all if r['matched_gold_chars'] > 0]\n",
    "avg_sa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_all_with_matches) / len(sa_all_with_matches)\n",
    "}\n",
    "\n",
    "sa_contrib_with_matches = [r for r in structure_aware_contrib if r['matched_gold_chars'] > 0]\n",
    "avg_sa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches)\n",
    "}\n",
    "\n",
    "# Calculate total matched percentages\n",
    "total_sa_all_matched = sum(r['matched_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_gold = sum(r['total_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_all)\n",
    "\n",
    "total_sa_contrib_matched = sum(r['matched_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_gold = sum(r['total_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_contrib)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORDER-AGNOSTIC EVALUATION\")\n",
    "print(\"   (Pure OCR quality, reading order irrelevant)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_strict']:.2%}  |  WER: {avg_oa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_standard']:.2%}  |  WER: {avg_oa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_letters']:.2%}\")\n",
    "\n",
    "print(f\"\\n   Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_strict']:.2%}  |  WER: {avg_oa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_standard']:.2%}  |  WER: {avg_oa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_letters']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STRUCTURE-AWARE EVALUATION\")\n",
    "print(\"   (OCR quality on matched content only)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   Matched Content - All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_strict']:.2%}  |  WER: {avg_sa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_standard']:.2%}  |  WER: {avg_sa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_all_matched:,} chars matched \" +\n",
    "      f\"({total_sa_all_matched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_all_unmatched:,} chars \" +\n",
    "      f\"({total_sa_all_unmatched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(f\"\\n   Matched Content - Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_strict']:.2%}  |  WER: {avg_sa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_standard']:.2%}  |  WER: {avg_sa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_contrib_matched:,} chars matched \" +\n",
    "      f\"({total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_contrib_unmatched:,} chars \" +\n",
    "      f\"({total_sa_contrib_unmatched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Strict: Most conservative\")\n",
    "print(\"Standard: Fair baseline - normalizes whitespace\")\n",
    "print(\"Letters Only: Most lenient - pure character recognition quality\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(f\"- Pure OCR quality (standard normalization): {avg_oa_all['cer_standard']:.2%}\")\n",
    "print(f\"- Letter recognition quality: {avg_oa_all['cer_letters']:.2%}\")\n",
    "print(f\"- Structure failures (unmatched content): {total_sa_all_unmatched/total_sa_all_gold*100:.1f}%\")\n",
    "print(f\"- Contributions:\")\n",
    "print(f\"    Standard CER: {avg_sa_contrib['cer_standard']:.2%}\")\n",
    "print(f\"    Successfully matched: {total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb143df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detailed page-by-page diagnostics...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PAGE-BY-PAGE TEXT QUALITY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "--- STRICT NORMALIZATION (preserves all whitespace) ---\n",
      "                                   page_id  cer_%  wer_%  coverage_%  subs_%  dels_%  ins_%  matched_chars  total_errors\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001   6.17   8.70        87.8    1.90    0.76   3.70           1054            67\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002   0.61   0.25       100.0    0.08    0.00   0.53           2475            15\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003  76.32  83.33         1.0    0.00    0.00  76.32             38            29\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004  17.17  20.07        99.8    9.81   15.21   1.36           5219          1377\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005  39.80  42.02        15.1    0.00   39.80   0.00            716           285\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006   2.12   5.83        78.2    6.03   10.75   9.55           3583           943\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00   0.00        64.2    0.00    0.00   0.00             43             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009   3.85   6.29        78.8   10.86    1.37   3.56           2780           439\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010   2.32  13.95        99.9   32.19   14.70   9.32           5212          2930\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013   4.71  34.89        89.9   43.53   21.51  39.08           5522          5750\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014  13.33  17.03       100.3   18.10    6.15   3.90           1155           325\n",
      "\n",
      "\n",
      "--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\n",
      "                                   page_id  cer_%  wer_%  coverage_%  subs_%  dels_%  ins_%  matched_chars  total_errors\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001   5.23   8.70        87.8    0.76    0.76   3.70           1054            55\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002   0.08   0.25       100.0    0.08    0.00   0.00           2475             2\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003  76.32  83.33         1.0    0.00    0.00  76.32             38            29\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004  16.48  20.07        99.8    5.50   15.12   5.10           5219          1342\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005  39.52  42.02        15.1    0.00   39.25   0.00            716           281\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006   1.04   5.83        78.2    5.05   10.69   9.55           3583           906\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00   0.00        64.2    0.00    0.00   0.00             43             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009   2.79   6.29        78.8    8.13    0.11   3.53           2780           327\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010   2.25  13.95        99.9   31.50   14.68   9.32           5212          2893\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013   4.48  34.89        89.9   83.01   13.35  52.75           5522          8234\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014  12.57  17.03       100.3    5.11    5.54   3.90           1155           168\n",
      "\n",
      "\n",
      "--- LETTERS ONLY (no whitespace/punctuation) ---\n",
      "                                   page_id  cer_%  coverage_%  subs_%  dels_%  ins_%\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001   5.01        87.8    0.76    0.00   2.94\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002   0.10       100.0    0.08    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003  75.00         1.0    0.00    0.00  63.16\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004  15.96        99.8    0.69   11.78   0.04\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005  39.22        15.1    0.00   31.01   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006   0.75        78.2    0.67    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00        64.2    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00         0.0    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009   3.14        78.8   10.32    0.00   0.11\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010   1.47        99.9    1.86    0.77   1.02\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00         0.0    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012   0.00         0.0    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013   2.37        89.9    2.66    0.22   0.04\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014   8.93       100.3    3.46    4.42   0.00\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WORST PERFORMING PAGES (Standard Normalization)\n",
      "================================================================================\n",
      "\n",
      "1. La_Plume_bpt6k1185893k_1_10_1889__page-003\n",
      "   CER: 76.32%  |  WER: 83.33%\n",
      "   Coverage: 1.0% of gold text\n",
      "   Errors: 0 subs, 0 dels, 29 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 76.32%):\n",
      "         Gold: \"LA PLUME\n",
      "Revue LittÃ©raire & Artistique\"\n",
      "         Pred: \"LA PLUME\n",
      "Revue LittÃ©raire & Artistique\n",
      "NUMÃ‰RO 10\n",
      "1er SEPTEMBRE 1889\"\n",
      "\n",
      "2. La_Plume_bpt6k1185893k_1_10_1889__page-005\n",
      "   CER: 39.52%  |  WER: 42.02%\n",
      "   Coverage: 15.1% of gold text\n",
      "   Errors: 0 subs, 281 dels, 0 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (verse, CER: 39.52%):\n",
      "         Gold: \"LES BONNES SOUVENANCES\n",
      "\n",
      "Parmi les marronniers, parmi les\n",
      "Lilas blancs, les lilas violets,\n",
      "La villa d...\"\n",
      "         Pred: \"Parmi les marronniers, parmi les\n",
      "Lilas blancs, les lilas violets,\n",
      "La villa de houblon s'enguirlande,...\"\n",
      "\n",
      "3. La_Plume_bpt6k1185893k_1_10_1889__page-004\n",
      "   CER: 16.48%  |  WER: 20.07%\n",
      "   Coverage: 99.8% of gold text\n",
      "   Errors: 287 subs, 789 dels, 266 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (prose, CER: 21.90%):\n",
      "         Gold: \"d'art : de grÃ¢ce, n'imprimons plus que des chefs-d'Å“uvre !...\n",
      "\n",
      "(Le mien, mon chef-d'Å“uvre, publiÃ© de...\"\n",
      "         Pred: \"d'art : de grÃ¢ce, n'imprimons plus que des chefs-d'Å“uvre !... (Le mien, mon chef-d'Å“uvre, publiÃ© dep...\"\n",
      "      Item 2 (verse, CER: 2.92%):\n",
      "         Gold: \"LES INGÃ‰NUS\n",
      "\n",
      "Les hauts talons luttaient avec les longues jupes.\n",
      "En sorte que, selon le terrain et le...\"\n",
      "         Pred: \"LES INGÃ‰NUS Les hauts talons luttaient avec les longues jupes. En sorte que, selon le terrain et le ...\"\n",
      "\n",
      "4. La_Plume_bpt6k1185893k_1_10_1889__page-014\n",
      "   CER: 12.57%  |  WER: 17.03%\n",
      "   Coverage: 100.3% of gold text\n",
      "   Errors: 59 subs, 64 dels, 45 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (ad, CER: 22.15%):\n",
      "         Gold: \"En vente aux bureaux de LA PLUME\n",
      "\n",
      "Å’UVRES DE LÃ‰ON DESCHAMPS\n",
      "\n",
      "A LA GUEULE DU MONSTRE, poÃ©sies, un vol....\"\n",
      "         Pred: \"A LA GUEULE DU MONSTRE, poÃ©sies, un vol. in-18, vÃ©lin teintÃ©. (A. Dupret, Ã©dit.) ......................\"\n",
      "      Item 2 (paratext, CER: 7.18%):\n",
      "         Gold: \"LA PLUME, revue littÃ©raire et artistique\n",
      "\n",
      "BULLETIN Dâ€™ABONNEMENT\n",
      "\n",
      "Je soussignÃ©\n",
      "demeurant Ã \n",
      "dÃ©clare so...\"\n",
      "         Pred: \"LA PLUME, revue littÃ©raire et artistique\n",
      "BULLETIN Dâ€™ABONNEMENT\n",
      "Je soussignÃ©\n",
      "Demeurant Ã \n",
      "DÃ©clare\n",
      "Sous...\"\n",
      "\n",
      "5. La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "   CER: 5.23%  |  WER: 8.70%\n",
      "   Coverage: 87.8% of gold text\n",
      "   Errors: 8 subs, 8 dels, 39 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 68.42%):\n",
      "         Gold: \"RÃ‰DACTION ET ADMINISTRATION\n",
      "36, Boulevard Arago, 36\n",
      "PARIS\"\n",
      "         Pred: \"RÃ‰DACTION ET ADMINISTRATION\n",
      "36, Boulevard Arago, 36\n",
      "PARIS\n",
      "Directeur de la Revue : LÃ©on DESCHAMPS\"\n",
      "      Item 2 (paratext, CER: 1.79%):\n",
      "         Gold: \"SOMMAIRE :\n",
      "\n",
      "Texte :\n",
      "LÃ©on DESCHAMPS : LÃ©on Vanier. â€” Paul VERLAINE : Les IngÃ©nus. â€” StÃ©phane MALLARMÃ‰...\"\n",
      "         Pred: \"SOMMAIRE\n",
      "Texte :\n",
      "LÃ©on DESCHAMPS : LÃ©on Vanier. â€” Paul VERLAINE : Les IngÃ©nus. â€” StÃ©phane MALLARMÃ‰ : ...\"\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ERROR TYPE DISTRIBUTION (Standard Normalization)\n",
      "================================================================================\n",
      "\n",
      "Total errors across all pages: 14,237\n",
      "   Substitutions: 6,989 (49.1%)\n",
      "   Deletions:     3,030 (21.3%)\n",
      "   Insertions:    4,218 (29.6%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Average CER (standard): 11.48%\n",
      "- Pages with CER > 20%: 2\n",
      "- Pages with CER < 5%: 9\n",
      "- Most common error type: Substitutions\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-by-Page Text Diagnostics\n",
    "Detailed error analysis for each page with three normalization levels.\n",
    "Shows error type distribution, worst performing pages, and actual text examples.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def get_levenshtein_operations(reference: str, hypothesis: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get detailed Levenshtein operations breakdown.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with counts of substitutions, deletions, insertions\n",
    "    \"\"\"\n",
    "    if not reference and not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': 0, 'total': 0}\n",
    "    \n",
    "    if not reference:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': len(hypothesis), 'total': len(hypothesis)}\n",
    "    \n",
    "    if not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': len(reference), 'insertions': 0, 'total': len(reference)}\n",
    "    \n",
    "    # Use SequenceMatcher to get operations\n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    substitutions = 0\n",
    "    deletions = 0\n",
    "    insertions = 0\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Both strings differ - count as substitutions\n",
    "            substitutions += max(i2 - i1, j2 - j1)\n",
    "        elif tag == 'delete':\n",
    "            # Only in reference\n",
    "            deletions += (i2 - i1)\n",
    "        elif tag == 'insert':\n",
    "            # Only in hypothesis\n",
    "            insertions += (j2 - j1)\n",
    "    \n",
    "    return {\n",
    "        'substitutions': substitutions,\n",
    "        'deletions': deletions,\n",
    "        'insertions': insertions,\n",
    "        'total': substitutions + deletions + insertions\n",
    "    }\n",
    "\n",
    "\n",
    "def diagnose_page_text_quality(page: Dict, normalization: str = 'standard') -> Dict:\n",
    "    \"\"\"\n",
    "    Detailed text quality diagnosis for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page: Page data from all_pages\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detailed metrics and error breakdowns\n",
    "    \"\"\"\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "        return {\n",
    "            'page_id': page_id,\n",
    "            'cer': 0.0,\n",
    "            'wer': 0.0,\n",
    "            'matched_chars': 0,\n",
    "            'total_gold_chars': total_gold_chars,\n",
    "            'match_coverage': 0.0,\n",
    "            'substitutions': 0,\n",
    "            'deletions': 0,\n",
    "            'insertions': 0,\n",
    "            'total_errors': 0,\n",
    "            'items_analyzed': []\n",
    "        }\n",
    "    \n",
    "    # Concatenate matched text\n",
    "    gold_text = ' '.join(gold_item.get('item_text_raw', '') for gold_item, _, _ in matched_pairs)\n",
    "    pred_text = ' '.join(pred_item.get('item_text_raw', '') for _, pred_item, _ in matched_pairs)\n",
    "    \n",
    "    # Calculate CER/WER\n",
    "    cer = character_error_rate(gold_text, pred_text, normalization)\n",
    "    wer = word_error_rate(gold_text, pred_text, normalization)\n",
    "    \n",
    "    # Get error breakdown using normalized text\n",
    "    if normalization == 'strict':\n",
    "        gold_norm = normalize_text_strict(gold_text)\n",
    "        pred_norm = normalize_text_strict(pred_text)\n",
    "    elif normalization == 'standard':\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "    else:  # letters_only\n",
    "        gold_norm = normalize_text_letters_only(gold_text)\n",
    "        pred_norm = normalize_text_letters_only(pred_text)\n",
    "    \n",
    "    ops = get_levenshtein_operations(gold_norm, pred_norm)\n",
    "    \n",
    "    # Analyze individual items\n",
    "    items_analyzed = []\n",
    "    for gold_item, pred_item, similarity in matched_pairs:\n",
    "        gold_item_text = gold_item.get('item_text_raw', '')\n",
    "        pred_item_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        item_cer = character_error_rate(gold_item_text, pred_item_text, normalization)\n",
    "        \n",
    "        items_analyzed.append({\n",
    "            'gold_class': gold_item.get('item_class'),\n",
    "            'cer': item_cer,\n",
    "            'gold_preview': gold_item_text[:100],\n",
    "            'pred_preview': pred_item_text[:100],\n",
    "            'gold_length': len(gold_item_text),\n",
    "            'pred_length': len(pred_item_text)\n",
    "        })\n",
    "    \n",
    "    total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'cer': cer,\n",
    "        'wer': wer,\n",
    "        'matched_chars': len(gold_text),\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'match_coverage': len(gold_text) / total_gold_chars * 100 if total_gold_chars > 0 else 0,\n",
    "        'substitutions': ops['substitutions'],\n",
    "        'deletions': ops['deletions'],\n",
    "        'insertions': ops['insertions'],\n",
    "        'total_errors': ops['total'],\n",
    "        'items_analyzed': items_analyzed\n",
    "    }\n",
    "\n",
    "\n",
    "# Diagnose all pages for all three normalizations\n",
    "print(\"Running detailed page-by-page diagnostics...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_diagnostics_strict = []\n",
    "page_diagnostics_standard = []\n",
    "page_diagnostics_letters = []\n",
    "\n",
    "for page in all_pages:\n",
    "    diag_strict = diagnose_page_text_quality(page, 'strict')\n",
    "    page_diagnostics_strict.append(diag_strict)\n",
    "    \n",
    "    diag_standard = diagnose_page_text_quality(page, 'standard')\n",
    "    page_diagnostics_standard.append(diag_standard)\n",
    "    \n",
    "    diag_letters = diagnose_page_text_quality(page, 'letters_only')\n",
    "    page_diagnostics_letters.append(diag_letters)\n",
    "\n",
    "# Create summary DataFrames\n",
    "def create_summary_df(diagnostics, normalization_name):\n",
    "    \"\"\"Create summary DataFrame from diagnostics.\"\"\"\n",
    "    data = []\n",
    "    for d in diagnostics:\n",
    "        if d['matched_chars'] > 0:\n",
    "            sub_pct = d['substitutions'] / d['matched_chars'] * 100\n",
    "            del_pct = d['deletions'] / d['matched_chars'] * 100\n",
    "            ins_pct = d['insertions'] / d['matched_chars'] * 100\n",
    "        else:\n",
    "            sub_pct = del_pct = ins_pct = 0\n",
    "        \n",
    "        data.append({\n",
    "            'page_id': d['page_id'],\n",
    "            'cer_%': round(d['cer'] * 100, 2),\n",
    "            'wer_%': round(d['wer'] * 100, 2),\n",
    "            'coverage_%': round(d['match_coverage'], 1),\n",
    "            'subs_%': round(sub_pct, 2),\n",
    "            'dels_%': round(del_pct, 2),\n",
    "            'ins_%': round(ins_pct, 2),\n",
    "            'matched_chars': d['matched_chars'],\n",
    "            'total_errors': d['total_errors']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_strict = create_summary_df(page_diagnostics_strict, 'Strict')\n",
    "df_standard = create_summary_df(page_diagnostics_standard, 'Standard')\n",
    "df_letters = create_summary_df(page_diagnostics_letters, 'Letters Only')\n",
    "\n",
    "# Print summary tables\n",
    "print(\"=\"*80)\n",
    "print(\"PAGE-BY-PAGE TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- STRICT NORMALIZATION (preserves all whitespace) ---\")\n",
    "print(df_strict.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\")\n",
    "print(df_standard.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- LETTERS ONLY (no whitespace/punctuation) ---\")\n",
    "print(df_letters[['page_id', 'cer_%', 'coverage_%', 'subs_%', 'dels_%', 'ins_%']].to_string(index=False))\n",
    "\n",
    "# Identify worst pages (using standard normalization)\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"WORST PERFORMING PAGES (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "worst_pages = sorted(page_diagnostics_standard, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "for i, page_diag in enumerate(worst_pages, 1):\n",
    "    print(f\"\\n{i}. {page_diag['page_id']}\")\n",
    "    print(f\"   CER: {page_diag['cer']:.2%}  |  WER: {page_diag['wer']:.2%}\")\n",
    "    print(f\"   Coverage: {page_diag['match_coverage']:.1f}% of gold text\")\n",
    "    print(f\"   Errors: {page_diag['substitutions']} subs, {page_diag['deletions']} dels, {page_diag['insertions']} ins\")\n",
    "    \n",
    "    # Show worst items from this page\n",
    "    if page_diag['items_analyzed']:\n",
    "        worst_items = sorted(page_diag['items_analyzed'], key=lambda x: x['cer'], reverse=True)[:2]\n",
    "        print(f\"\\n   Worst items on this page:\")\n",
    "        for j, item in enumerate(worst_items, 1):\n",
    "            print(f\"      Item {j} ({item['gold_class']}, CER: {item['cer']:.2%}):\")\n",
    "            print(f\"         Gold: \\\"{item['gold_preview']}{'...' if item['gold_length'] > 100 else ''}\\\"\")\n",
    "            print(f\"         Pred: \\\"{item['pred_preview']}{'...' if item['pred_length'] > 100 else ''}\\\"\")\n",
    "\n",
    "# Error distribution analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ERROR TYPE DISTRIBUTION (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_errors = sum(d['total_errors'] for d in page_diagnostics_standard)\n",
    "total_subs = sum(d['substitutions'] for d in page_diagnostics_standard)\n",
    "total_dels = sum(d['deletions'] for d in page_diagnostics_standard)\n",
    "total_ins = sum(d['insertions'] for d in page_diagnostics_standard)\n",
    "\n",
    "print(f\"\\nTotal errors across all pages: {total_errors:,}\")\n",
    "print(f\"   Substitutions: {total_subs:,} ({total_subs/total_errors*100:.1f}%)\")\n",
    "print(f\"   Deletions:     {total_dels:,} ({total_dels/total_errors*100:.1f}%)\")\n",
    "print(f\"   Insertions:    {total_ins:,} ({total_ins/total_errors*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Average CER (standard): {df_standard['cer_%'].mean():.2f}%\")\n",
    "print(f\"- Pages with CER > 20%: {len(df_standard[df_standard['cer_%'] > 20])}\")\n",
    "print(f\"- Pages with CER < 5%: {len(df_standard[df_standard['cer_%'] < 5])}\")\n",
    "print(f\"- Most common error type: \" + \n",
    "      (\"Substitutions\" if total_subs > max(total_dels, total_ins) else \n",
    "       \"Deletions\" if total_dels > total_ins else \"Insertions\"))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b21705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing character-level confusions across all pages...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CHARACTER CONFUSION MATRIX\n",
      "================================================================================\n",
      "\n",
      "Total character substitutions: 130\n",
      "Unique confusion pairs: 115\n",
      "\n",
      "Top 30 Most Common Character Substitutions:\n",
      "Gold â†’ Pred                    Count     \n",
      "--------------------------------------------------------------------------------\n",
      "\" ; E\" â†’ \"; Ã‰\"                 8         \n",
      "'a' â†’ 'Ã '                      3         \n",
      "'E' â†’ 'Ã‰'                      2         \n",
      "'Ã®' â†’ 'i'                      2         \n",
      "'e' â†’ 'Ã¨'                      2         \n",
      "\"t) \" â†’ \"l)\"                   2         \n",
      "\".)\" â†’ \")\"                     2         \n",
      "'d' â†’ 'D'                      2         \n",
      "\"ance\" â†’ \"ir\"                  1         \n",
      "'l' â†’ 'L'                      1         \n",
      "'C' â†’ 'G'                      1         \n",
      "\"de\" â†’ \"DE\"                    1         \n",
      "'Ã©' â†’ 'e'                      1         \n",
      "\" dÃ©\" â†’ \"dÃ©- \"                 1         \n",
      "'e' â†’ 'Ã©'                      1         \n",
      "\"nec\" â†’ \"ce\"                   1         \n",
      "'i' â†’ 'I'                      1         \n",
      "\"Ã¨res luttes lit\" â†’ \"eres luttes lit- \" 1         \n",
      "\"y\" â†’ \"vous\"                   1         \n",
      "\"que\" â†’ \"ces- \"                1         \n",
      "\"t ce rÃ©gal comblait nos jeunes yeux de fous. \" â†’ \"R\" 1         \n",
      "\"e soir tombait, un soir Ã©quivoque d'automne : Les belles, se pendant rÃªveuses Ã  nos bras, Dirent alors des mots si spÃ©cieux, tout bas, Que notre Ã¢me depuis ce temps tremble et s'Ã©tonne. Paul Verlaine\" â†’ \"AINE\" 1         \n",
      "'t' â†’ 'l'                      1         \n",
      "\"Cent affic\" â†’ \"Si discord parmi l'exaltation de l'\" 1         \n",
      "\"s s'assimilant l'or incompris des\" â†’ \"ure, un cri faussa ce nom connu pour dÃ©ployer la conti- nitÃ© de cimes tard Ã©vanouies, Fontainebleau, que\" 1         \n",
      "\"ours, trahison de la lettre, ont fui, comm\" â†’ \"e pensai, la glace du compartiment vio- lentÃ©e, du poing aussi Ã©treindr\" 1         \n",
      "\"tous confins de la \" â†’ \"la gorge l'interrupteur : Tais-toi ! ne di\" 1         \n",
      "\"ille, mes ye\" â†’ \"ulgue pas du fait d'un aboi indiffÃ©rent l'ombre ici insinuÃ©e dans mon esprit, a\" 1         \n",
      "\"au ras de l'horizon par un dÃ©part sur le rail traÃ®nÃ©s a\" â†’ \"portiÃ¨res de wagons battant sous un \" 1         \n",
      "\"ant de se recueillir dans l'abstruse fiertÃ© que donne une approche de forÃªt en son temps d'apothÃ©ose. Si discord parmi l'exaltation de l'heure, un cri faussa ce nom connu pour dÃ©ployer la continuitÃ© de cimes tard Ã©vanouies, Fontainebleau, que je pensai, la \" â†’ \"ent inspirÃ© et Ã©\" 1         \n",
      "\n",
      "\n",
      "================================================================================\n",
      "SYSTEMATIC ERROR PATTERNS\n",
      "================================================================================\n",
      "\n",
      "Total confusions: 130\n",
      "Categorized: 85 (65.4%)\n",
      "Uncategorized: 45 (34.6%)\n",
      "\n",
      "Pattern Breakdown:\n",
      "   Space Issues                  64 ( 49.2%)\n",
      "   Case Errors                    9 (  6.9%)\n",
      "   Punctuation Errors             6 (  4.6%)\n",
      "   Accent Removal                 3 (  2.3%)\n",
      "   Accent Confusion               2 (  1.5%)\n",
      "   Ligature Issues                1 (  0.8%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ACCENT & DIACRITIC ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Accented character confusions: 5\n",
      "\n",
      "Most common accented character errors:\n",
      "   'Ã®' â†’ 'i': 2 times\n",
      "   'Ã©' â†’ 'e': 1 times\n",
      "   'Ã¨' â†’ 'Ã©': 1 times\n",
      "   'Ã ' â†’ 'Ã€': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LIGATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Ligature-related confusions: 1\n",
      "\n",
      "Ligature substitutions:\n",
      "   'Å“' â†’ 'oe': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CASE SENSITIVITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Case-only differences: 10\n",
      "\n",
      "Most common case errors:\n",
      "   'd' â†’ 'D': 2 times\n",
      "   'l' â†’ 'L': 1 times\n",
      "   'i' â†’ 'I': 1 times\n",
      "   'P' â†’ 'p': 1 times\n",
      "   'M' â†’ 'm': 1 times\n",
      "   'D' â†’ 'd': 1 times\n",
      "   'F' â†’ 'f': 1 times\n",
      "   's' â†’ 'S': 1 times\n",
      "   'Ã ' â†’ 'Ã€': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Based on the error analysis:\n",
      "\n",
      "âš  CASE SENSITIVITY ISSUES (6.9%)\n",
      "   - Model confusing upper/lowercase\n",
      "   - May indicate line/title detection problems\n",
      "\n",
      "âœ“ ERROR DIVERSITY IS HIGH (unique ratio: 0.88)\n",
      "   - Errors are diverse, not systematic\n",
      "   - Suggests random OCR noise rather than systematic bias\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cross-Page Error Analysis\n",
    "Character-level confusion matrix and systematic error pattern detection.\n",
    "Analyzes all pages together to identify recurring OCR issues.\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def get_character_confusions(reference: str, hypothesis: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract character-level substitutions from aligned strings.\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_char, pred_char) tuples for substitutions\n",
    "    \"\"\"\n",
    "    confusions = []\n",
    "    \n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Character substitution\n",
    "            gold_substr = reference[i1:i2]\n",
    "            pred_substr = hypothesis[j1:j2]\n",
    "            \n",
    "            # For single character replacements\n",
    "            if len(gold_substr) == 1 and len(pred_substr) == 1:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "            # For multi-character replacements (like Å“ -> oe)\n",
    "            elif len(gold_substr) > 0 and len(pred_substr) > 0:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "    \n",
    "    return confusions\n",
    "\n",
    "\n",
    "def analyze_character_patterns(confusions: list) -> dict:\n",
    "    \"\"\"\n",
    "    Detect systematic patterns in character confusions.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with pattern names and counts\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'accent_removal': 0,\n",
    "        'accent_confusion': 0,\n",
    "        'ligature_issues': 0,\n",
    "        'case_errors': 0,\n",
    "        'punctuation_errors': 0,\n",
    "        'similar_shape': 0,\n",
    "        'space_issues': 0\n",
    "    }\n",
    "    \n",
    "    accent_chars = 'Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´ÃµÃ¶Ã¹ÃºÃ»Ã¼Ã½Ã¿Ã±Ã§Ã€ÃÃ‚ÃƒÃ„Ã…ÃˆÃ‰ÃŠÃ‹ÃŒÃÃŽÃÃ’Ã“Ã”Ã•Ã–Ã™ÃšÃ›ÃœÃÅ¸Ã‘Ã‡'\n",
    "    ligatures = 'Å“Ã¦Å’Ã†'\n",
    "    \n",
    "    for gold, pred in confusions:\n",
    "        # Accent removal (Ã© -> e, Ã  -> a)\n",
    "        if len(gold) == 1 and len(pred) == 1:\n",
    "            gold_base = unicodedata.normalize('NFD', gold)[0]\n",
    "            pred_normalized = unicodedata.normalize('NFD', pred)[0]\n",
    "            if gold in accent_chars and gold_base == pred:\n",
    "                patterns['accent_removal'] += 1\n",
    "            elif gold in accent_chars and pred in accent_chars and gold != pred:\n",
    "                patterns['accent_confusion'] += 1\n",
    "            elif gold.lower() == pred.lower():\n",
    "                patterns['case_errors'] += 1\n",
    "        \n",
    "        # Ligature issues (Å“ -> oe, Ã¦ -> ae)\n",
    "        if gold in ligatures and pred not in ligatures:\n",
    "            patterns['ligature_issues'] += 1\n",
    "        \n",
    "        # Similar shape confusions (common OCR errors)\n",
    "        similar_pairs = [\n",
    "            ('l', 'i'), ('i', 'l'), ('rn', 'm'), ('m', 'rn'),\n",
    "            ('cl', 'd'), ('d', 'cl'), ('o', '0'), ('0', 'o'),\n",
    "            ('1', 'l'), ('l', '1'), ('s', '5'), ('5', 's')\n",
    "        ]\n",
    "        if (gold, pred) in similar_pairs:\n",
    "            patterns['similar_shape'] += 1\n",
    "        \n",
    "        # Punctuation confusion\n",
    "        if gold in '.,;:!?\\'\"' or pred in '.,;:!?\\'\"':\n",
    "            patterns['punctuation_errors'] += 1\n",
    "        \n",
    "        # Space-related issues\n",
    "        if ' ' in gold or ' ' in pred:\n",
    "            patterns['space_issues'] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Collect all character confusions across all pages\n",
    "print(\"Analyzing character-level confusions across all pages...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_confusions = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        pred_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        # Use standard normalization for fair comparison\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "        \n",
    "        confusions = get_character_confusions(gold_norm, pred_norm)\n",
    "        all_confusions.extend(confusions)\n",
    "\n",
    "# Count confusion frequencies\n",
    "confusion_counter = Counter(all_confusions)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHARACTER CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal character substitutions: {len(all_confusions):,}\")\n",
    "print(f\"Unique confusion pairs: {len(confusion_counter):,}\")\n",
    "\n",
    "# Top 30 most common confusions\n",
    "print(\"\\nTop 30 Most Common Character Substitutions:\")\n",
    "print(f\"{'Gold â†’ Pred':<30} {'Count':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for (gold, pred), count in confusion_counter.most_common(30):\n",
    "    # Escape special characters for display\n",
    "    gold_display = repr(gold)[1:-1] if gold in '\\n\\t\\r' else gold\n",
    "    pred_display = repr(pred)[1:-1] if pred in '\\n\\t\\r' else pred\n",
    "    \n",
    "    # Create display string\n",
    "    if len(gold) == 1 and len(pred) == 1:\n",
    "        display = f\"'{gold_display}' â†’ '{pred_display}'\"\n",
    "    else:\n",
    "        display = f'\"{gold_display}\" â†’ \"{pred_display}\"'\n",
    "    \n",
    "    print(f\"{display:<30} {count:<10}\")\n",
    "\n",
    "# Pattern analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC ERROR PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "patterns = analyze_character_patterns(all_confusions)\n",
    "total_categorized = sum(patterns.values())\n",
    "\n",
    "print(f\"\\nTotal confusions: {len(all_confusions):,}\")\n",
    "print(f\"Categorized: {total_categorized:,} ({total_categorized/len(all_confusions)*100:.1f}%)\")\n",
    "print(f\"Uncategorized: {len(all_confusions) - total_categorized:,} \" +\n",
    "      f\"({(len(all_confusions) - total_categorized)/len(all_confusions)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPattern Breakdown:\")\n",
    "for pattern, count in sorted(patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        pct = count / len(all_confusions) * 100\n",
    "        pattern_name = pattern.replace('_', ' ').title()\n",
    "        print(f\"   {pattern_name:<25} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Specific accent analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ACCENT & DIACRITIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accent_confusions = [(g, p) for g, p in all_confusions \n",
    "                     if len(g) == 1 and len(p) == 1 \n",
    "                     and any(c in 'Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´ÃµÃ¶Ã¹ÃºÃ»Ã¼Ã½Ã¿Ã±Ã§Ã€ÃÃ‚ÃƒÃ„Ã…ÃˆÃ‰ÃŠÃ‹ÃŒÃÃŽÃÃ’Ã“Ã”Ã•Ã–Ã™ÃšÃ›ÃœÃÅ¸Ã‘Ã‡' for c in g)]\n",
    "\n",
    "if accent_confusions:\n",
    "    accent_counter = Counter(accent_confusions)\n",
    "    print(f\"\\nAccented character confusions: {len(accent_confusions):,}\")\n",
    "    print(\"\\nMost common accented character errors:\")\n",
    "    for (gold, pred), count in accent_counter.most_common(15):\n",
    "        print(f\"   '{gold}' â†’ '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo accented character confusions detected.\")\n",
    "\n",
    "# Ligature analysis  \n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"LIGATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ligature_confusions = [(g, p) for g, p in all_confusions if g in 'Å“Ã¦Å’Ã†' or p in 'Å“Ã¦Å’Ã†']\n",
    "\n",
    "if ligature_confusions:\n",
    "    ligature_counter = Counter(ligature_confusions)\n",
    "    print(f\"\\nLigature-related confusions: {len(ligature_confusions):,}\")\n",
    "    print(\"\\nLigature substitutions:\")\n",
    "    for (gold, pred), count in ligature_counter.most_common(10):\n",
    "        print(f\"   '{gold}' â†’ '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo ligature confusions detected.\")\n",
    "\n",
    "# Case sensitivity analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CASE SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "case_confusions = [(g, p) for g, p in all_confusions \n",
    "                   if len(g) == 1 and len(p) == 1 and g.lower() == p.lower() and g != p]\n",
    "\n",
    "if case_confusions:\n",
    "    case_counter = Counter(case_confusions)\n",
    "    print(f\"\\nCase-only differences: {len(case_confusions):,}\")\n",
    "    print(\"\\nMost common case errors:\")\n",
    "    for (gold, pred), count in case_counter.most_common(10):\n",
    "        print(f\"   '{gold}' â†’ '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo case-only confusions detected.\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBased on the error analysis:\")\n",
    "\n",
    "# Check for high accent issues\n",
    "accent_pct = patterns['accent_removal'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if accent_pct > 5:\n",
    "    print(f\"\\nâš  HIGH ACCENT REMOVAL RATE ({accent_pct:.1f}%)\")\n",
    "    print(\"   - Consider post-processing to restore accents using dictionary lookup\")\n",
    "    print(\"   - May need model fine-tuning on accented French text\")\n",
    "\n",
    "# Check for ligature issues\n",
    "ligature_pct = patterns['ligature_issues'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if ligature_pct > 2:\n",
    "    print(f\"\\nâš  LIGATURE HANDLING ISSUES ({ligature_pct:.1f}%)\")\n",
    "    print(\"   - Ligatures (Å“, Ã¦) being split or confused\")\n",
    "    print(\"   - Common in historical French texts\")\n",
    "\n",
    "# Check for case errors\n",
    "case_pct = patterns['case_errors'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if case_pct > 3:\n",
    "    print(f\"\\nâš  CASE SENSITIVITY ISSUES ({case_pct:.1f}%)\")\n",
    "    print(\"   - Model confusing upper/lowercase\")\n",
    "    print(\"   - May indicate line/title detection problems\")\n",
    "\n",
    "# General observation\n",
    "if len(all_confusions) > 0:\n",
    "    unique_ratio = len(confusion_counter) / len(all_confusions)\n",
    "    if unique_ratio > 0.5:\n",
    "        print(f\"\\nâœ“ ERROR DIVERSITY IS HIGH (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Errors are diverse, not systematic\")\n",
    "        print(\"   - Suggests random OCR noise rather than systematic bias\")\n",
    "    else:\n",
    "        print(f\"\\nâš  ERROR CONCENTRATION DETECTED (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Same errors repeat frequently\")\n",
    "        print(\"   - Suggests systematic model bias that could be corrected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item classification...\n",
      "\n",
      "Item count mismatch: gold=8, pred=5\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   All items - Accuracy: 100.00% (5/5)\n",
      "\n",
      "Item count mismatch: gold=0, pred=1\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   All items - Accuracy: 50.00% (1/2)\n",
      "\n",
      "Item count mismatch: gold=1, pred=2\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   All items - Accuracy: 66.67% (2/3)\n",
      "   Contributions - Accuracy: 100.00% (1/1)\n",
      "\n",
      "Item count mismatch: gold=5, pred=4\n",
      "Item count mismatch: gold=3, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   All items - Accuracy: 0.00% (0/4)\n",
      "   Contributions - Accuracy: 33.33% (1/3)\n",
      "\n",
      "Item count mismatch: gold=5, pred=4\n",
      "Item count mismatch: gold=3, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "   Contributions - Accuracy: 100.00% (3/3)\n",
      "\n",
      "Item count mismatch: gold=6, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "   Contributions - Accuracy: 100.00% (4/4)\n",
      "\n",
      "Item count mismatch: gold=2, pred=3\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   All items - Accuracy: 100.00% (2/2)\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   All items - Accuracy: 0.00% (0/0)\n",
      "\n",
      "Item count mismatch: gold=7, pred=5\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   All items - Accuracy: 40.00% (2/5)\n",
      "   Contributions - Accuracy: 80.00% (4/5)\n",
      "\n",
      "Item count mismatch: gold=6, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   All items - Accuracy: 0.00% (0/4)\n",
      "   Contributions - Accuracy: 100.00% (4/4)\n",
      "\n",
      "Item count mismatch: gold=3, pred=1\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   All items - Accuracy: 0.00% (0/1)\n",
      "   Contributions - Accuracy: 100.00% (1/1)\n",
      "\n",
      "Item count mismatch: gold=8, pred=0\n",
      "Item count mismatch: gold=4, pred=0\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   All items - Accuracy: 0.00% (0/0)\n",
      "\n",
      "Item count mismatch: gold=0, pred=10\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   All items - Accuracy: 9.09% (1/11)\n",
      "\n",
      "Item count mismatch: gold=4, pred=6\n",
      "Item count mismatch: gold=0, pred=1\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION ACCURACY SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Overall Accuracy: 38.78% (19/49)\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Overall Accuracy: 85.71% (18/21)\n",
      "\n",
      "Confusion Matrix (All Items):\n",
      "Gold / Pred    ad          paratext    prose       verse       \n",
      "ad             0           1           0           0           \n",
      "paratext       0           12          20          4           \n",
      "prose          0           0           3           3           \n",
      "verse          0           0           2           4           \n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification(gold_path: Path, pred_path: Path,\n",
    "                           item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate item_class classification accuracy.\n",
    "    \n",
    "    Assumes items are in same order (or uses simple alignment).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with accuracy, per-class metrics, confusion matrix\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    # Simple alignment: assume same number and order\n",
    "    if len(gold_items) != len(pred_items):\n",
    "        print(f\"Item count mismatch: gold={len(gold_items)}, pred={len(pred_items)}\")\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    correct = 0\n",
    "    confusion = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_class = gold_items[i].get('item_class', 'unknown')\n",
    "        pred_class = pred_items[i].get('item_class', 'unknown')\n",
    "        \n",
    "        confusion[gold_class][pred_class] += 1\n",
    "        if gold_class == pred_class:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / min_len if min_len > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'total': min_len,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion': dict(confusion),\n",
    "        'gold_count': len(gold_items),\n",
    "        'pred_count': len(pred_items)\n",
    "    }\n",
    "\n",
    "# Evaluate classification\n",
    "print(\"Evaluating item classification...\\n\")\n",
    "\n",
    "classification_results_all = []\n",
    "classification_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_classification(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    classification_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_classification(gold_path, pred_path,\n",
    "                                            item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    classification_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   All items - Accuracy: {result_all['accuracy']:.2%} \"\n",
    "          f\"({result_all['correct']}/{result_all['total']})\")\n",
    "    if result_contrib['total'] > 0:\n",
    "        print(f\"   Contributions - Accuracy: {result_contrib['accuracy']:.2%} \"\n",
    "              f\"({result_contrib['correct']}/{result_contrib['total']})\")\n",
    "    print()\n",
    "\n",
    "# Compute overall accuracy\n",
    "total_correct_all = sum(r['correct'] for r in classification_results_all)\n",
    "total_items_all = sum(r['total'] for r in classification_results_all)\n",
    "overall_accuracy_all = total_correct_all / total_items_all if total_items_all > 0 else 0\n",
    "\n",
    "total_correct_contrib = sum(r['correct'] for r in classification_results_contrib)\n",
    "total_items_contrib = sum(r['total'] for r in classification_results_contrib)\n",
    "overall_accuracy_contrib = total_correct_contrib / total_items_contrib if total_items_contrib > 0 else 0\n",
    "\n",
    "# Aggregate confusion matrix\n",
    "all_confusion = defaultdict(lambda: defaultdict(int))\n",
    "for result in classification_results_all:\n",
    "    for gold_class, pred_dict in result['confusion'].items():\n",
    "        for pred_class, count in pred_dict.items():\n",
    "            all_confusion[gold_class][pred_class] += count\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CLASSIFICATION ACCURACY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_all:.2%} ({total_correct_all}/{total_items_all})\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_contrib:.2%} ({total_correct_contrib}/{total_items_contrib})\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (All Items):\")\n",
    "print(f\"{'Gold / Pred':<15}\", end=\"\")\n",
    "all_classes = sorted(set(list(all_confusion.keys()) + \n",
    "                        [pred for preds in all_confusion.values() for pred in preds.keys()]))\n",
    "for pred_class in all_classes:\n",
    "    print(f\"{pred_class:<12}\", end=\"\")\n",
    "print()\n",
    "for gold_class in all_classes:\n",
    "    print(f\"{gold_class:<15}\", end=\"\")\n",
    "    for pred_class in all_classes:\n",
    "        count = all_confusion[gold_class][pred_class]\n",
    "        print(f\"{count:<12}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c433e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metadata extraction...\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   Title F1: 1.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   Title F1: 0.667, Author F1: 0.667\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   Title F1: 0.667, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   Title F1: 0.500, Author F1: 0.400\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   Title F1: 0.667, Author F1: 0.571\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   Title F1: 0.500, Author F1: 0.400\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "\n",
      "============================================================\n",
      "METADATA EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Title - P: 43.75%, R: 87.50%, F1: 0.583\n",
      "           Exact matches: 2/7\n",
      "   Author - P: 38.46%, R: 45.45%, F1: 0.417\n",
      "            Exact matches: 0/5\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Title - P: 100.00%, R: 86.67%, F1: 0.929\n",
      "   Author - P: 92.31%, R: 85.71%, F1: 0.889\n"
     ]
    }
   ],
   "source": [
    "def evaluate_metadata(gold_path: Path, pred_path: Path,\n",
    "                     item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate title and author extraction accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with title/author presence detection and exact match metrics\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    title_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    author_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Title evaluation\n",
    "        gold_title = gold_item.get('item_title')\n",
    "        pred_title = pred_item.get('item_title')\n",
    "        \n",
    "        if gold_title and pred_title:\n",
    "            title_metrics['tp'] += 1\n",
    "            if gold_title == pred_title:\n",
    "                title_metrics['exact_match'] += 1\n",
    "        elif not gold_title and pred_title:\n",
    "            title_metrics['fp'] += 1\n",
    "        elif gold_title and not pred_title:\n",
    "            title_metrics['fn'] += 1\n",
    "        \n",
    "        # Author evaluation\n",
    "        gold_author = gold_item.get('item_author')\n",
    "        pred_author = pred_item.get('item_author')\n",
    "        \n",
    "        if gold_author and pred_author:\n",
    "            author_metrics['tp'] += 1\n",
    "            if gold_author == pred_author:\n",
    "                author_metrics['exact_match'] += 1\n",
    "        elif not gold_author and pred_author:\n",
    "            author_metrics['fp'] += 1\n",
    "        elif gold_author and not pred_author:\n",
    "            author_metrics['fn'] += 1\n",
    "    \n",
    "    # Compute F1 for title\n",
    "    title_p = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fp']) if (title_metrics['tp'] + title_metrics['fp']) > 0 else 0\n",
    "    title_r = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fn']) if (title_metrics['tp'] + title_metrics['fn']) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    # Compute F1 for author\n",
    "    author_p = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fp']) if (author_metrics['tp'] + author_metrics['fp']) > 0 else 0\n",
    "    author_r = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fn']) if (author_metrics['tp'] + author_metrics['fn']) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {\n",
    "            **title_metrics,\n",
    "            'precision': title_p,\n",
    "            'recall': title_r,\n",
    "            'f1': title_f1\n",
    "        },\n",
    "        'author': {\n",
    "            **author_metrics,\n",
    "            'precision': author_p,\n",
    "            'recall': author_r,\n",
    "            'f1': author_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate metadata extraction\n",
    "print(\"Evaluating metadata extraction...\\n\")\n",
    "\n",
    "metadata_results_all = []\n",
    "metadata_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_metadata(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    metadata_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_metadata(gold_path, pred_path,\n",
    "                                      item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    metadata_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   Title F1: {result_all['title']['f1']:.3f}, \"\n",
    "          f\"Author F1: {result_all['author']['f1']:.3f}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "def aggregate_metadata_metrics(results):\n",
    "    total_title_tp = sum(r['title']['tp'] for r in results)\n",
    "    total_title_fp = sum(r['title']['fp'] for r in results)\n",
    "    total_title_fn = sum(r['title']['fn'] for r in results)\n",
    "    total_title_exact = sum(r['title']['exact_match'] for r in results)\n",
    "    \n",
    "    title_p = total_title_tp / (total_title_tp + total_title_fp) if (total_title_tp + total_title_fp) > 0 else 0\n",
    "    title_r = total_title_tp / (total_title_tp + total_title_fn) if (total_title_tp + total_title_fn) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    total_author_tp = sum(r['author']['tp'] for r in results)\n",
    "    total_author_fp = sum(r['author']['fp'] for r in results)\n",
    "    total_author_fn = sum(r['author']['fn'] for r in results)\n",
    "    total_author_exact = sum(r['author']['exact_match'] for r in results)\n",
    "    \n",
    "    author_p = total_author_tp / (total_author_tp + total_author_fp) if (total_author_tp + total_author_fp) > 0 else 0\n",
    "    author_r = total_author_tp / (total_author_tp + total_author_fn) if (total_author_tp + total_author_fn) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {'precision': title_p, 'recall': title_r, 'f1': title_f1, 'exact_match': total_title_exact, 'tp': total_title_tp},\n",
    "        'author': {'precision': author_p, 'recall': author_r, 'f1': author_f1, 'exact_match': total_author_exact, 'tp': total_author_tp}\n",
    "    }\n",
    "\n",
    "agg_all = aggregate_metadata_metrics(metadata_results_all)\n",
    "agg_contrib = aggregate_metadata_metrics(metadata_results_contrib)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METADATA EXTRACTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Title - P: {agg_all['title']['precision']:.2%}, R: {agg_all['title']['recall']:.2%}, F1: {agg_all['title']['f1']:.3f}\")\n",
    "print(f\"           Exact matches: {agg_all['title']['exact_match']}/{agg_all['title']['tp']}\")\n",
    "print(f\"   Author - P: {agg_all['author']['precision']:.2%}, R: {agg_all['author']['recall']:.2%}, F1: {agg_all['author']['f1']:.3f}\")\n",
    "print(f\"            Exact matches: {agg_all['author']['exact_match']}/{agg_all['author']['tp']}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Title - P: {agg_contrib['title']['precision']:.2%}, R: {agg_contrib['title']['recall']:.2%}, F1: {agg_contrib['title']['f1']:.3f}\")\n",
    "print(f\"   Author - P: {agg_contrib['author']['precision']:.2%}, R: {agg_contrib['author']['recall']:.2%}, F1: {agg_contrib['author']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838969fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating continuation tracking...\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   is_continuation - F1: 1.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   is_continuation - F1: 1.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "============================================================\n",
      "CONTINUATION TRACKING SUMMARY (Contributions Only)\n",
      "============================================================\n",
      "\n",
      "is_continuation:\n",
      "   Precision: 100.00%\n",
      "   Recall: 33.33%\n",
      "   F1: 0.500\n",
      "\n",
      "continues_on_next_page:\n",
      "   Precision: 100.00%\n",
      "   Recall: 71.43%\n",
      "   F1: 0.833\n"
     ]
    }
   ],
   "source": [
    "def evaluate_continuation_tracking(gold_path: Path, pred_path: Path,\n",
    "                                  item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy (is_continuation, continues_on_next_page).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1 for each continuation field\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    # Metrics for is_continuation\n",
    "    is_cont_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    # Metrics for continues_on_next_page\n",
    "    continues_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Evaluate is_continuation (treat absent as False)\n",
    "        gold_is_cont = gold_item.get('is_continuation', False)\n",
    "        pred_is_cont = pred_item.get('is_continuation', False)\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['tp'] += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['fp'] += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_metrics['fn'] += 1\n",
    "        else:\n",
    "            is_cont_metrics['tn'] += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page', False)\n",
    "        pred_continues = pred_item.get('continues_on_next_page', False)\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_metrics['tp'] += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_metrics['fp'] += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_metrics['fn'] += 1\n",
    "        else:\n",
    "            continues_metrics['tn'] += 1\n",
    "    \n",
    "    # Compute metrics for is_continuation\n",
    "    is_cont_p = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fp']) if (is_cont_metrics['tp'] + is_cont_metrics['fp']) > 0 else 0\n",
    "    is_cont_r = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fn']) if (is_cont_metrics['tp'] + is_cont_metrics['fn']) > 0 else 0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "    \n",
    "    # Compute metrics for continues_on_next_page\n",
    "    continues_p = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fp']) if (continues_metrics['tp'] + continues_metrics['fp']) > 0 else 0\n",
    "    continues_r = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fn']) if (continues_metrics['tp'] + continues_metrics['fn']) > 0 else 0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            **is_cont_metrics,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            **continues_metrics,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate continuation tracking\n",
    "print(\"Evaluating continuation tracking...\\n\")\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    result = evaluate_continuation_tracking(gold_path, pred_path,\n",
    "                                           item_classes=['prose', 'verse'])\n",
    "    result['page'] = gold_path.name\n",
    "    continuation_results.append(result)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   is_continuation - F1: {result['is_continuation']['f1']:.3f}\")\n",
    "    print(f\"   continues_on_next - F1: {result['continues_on_next_page']['f1']:.3f}\\n\")\n",
    "\n",
    "# Aggregate continuation metrics\n",
    "total_is_cont_tp = sum(r['is_continuation']['tp'] for r in continuation_results)\n",
    "total_is_cont_fp = sum(r['is_continuation']['fp'] for r in continuation_results)\n",
    "total_is_cont_fn = sum(r['is_continuation']['fn'] for r in continuation_results)\n",
    "\n",
    "is_cont_p = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fp) if (total_is_cont_tp + total_is_cont_fp) > 0 else 0\n",
    "is_cont_r = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fn) if (total_is_cont_tp + total_is_cont_fn) > 0 else 0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "\n",
    "total_continues_tp = sum(r['continues_on_next_page']['tp'] for r in continuation_results)\n",
    "total_continues_fp = sum(r['continues_on_next_page']['fp'] for r in continuation_results)\n",
    "total_continues_fn = sum(r['continues_on_next_page']['fn'] for r in continuation_results)\n",
    "\n",
    "continues_p = total_continues_tp / (total_continues_tp + total_continues_fp) if (total_continues_tp + total_continues_fp) > 0 else 0\n",
    "continues_r = total_continues_tp / (total_continues_tp + total_continues_fn) if (total_continues_tp + total_continues_fn) > 0 else 0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CONTINUATION TRACKING SUMMARY (Contributions Only)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nis_continuation:\")\n",
    "print(f\"   Precision: {is_cont_p:.2%}\")\n",
    "print(f\"   Recall: {is_cont_r:.2%}\")\n",
    "print(f\"   F1: {is_cont_f1:.3f}\")\n",
    "print(f\"\\ncontinues_on_next_page:\")\n",
    "print(f\"   Precision: {continues_p:.2%}\")\n",
    "print(f\"   Recall: {continues_r:.2%}\")\n",
    "print(f\"   F1: {continues_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EVALUATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Second_try_revised vs Gold Standard\n",
      "Evaluated on 14 pages\n",
      "\n",
      "Metric                         All Items            Contributions       \n",
      "----------------------------------------------------------------------\n",
      "TEXT QUALITY                  \n",
      "  Character Error Rate                     12.63%             31.65%\n",
      "  Word Error Rate                          17.82%             34.04%\n",
      "\n",
      "STRUCTURE QUALITY             \n",
      "  Boundary Detection F1                     0.426              0.459\n",
      "  Classification Accuracy                  38.78%             85.71%\n",
      "\n",
      "METADATA EXTRACTION           \n",
      "  Title F1                                  0.583              0.929\n",
      "  Author F1                                 0.417              0.889\n",
      "\n",
      "CONTINUATION TRACKING          N/A                  Contributions       \n",
      "  is_continuation F1                                             0.500\n",
      "  continues_on_next F1                                           0.833\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSecond_try_revised vs Gold Standard\")\n",
    "print(f\"Evaluated on {len(page_pairs)} pages\\n\")\n",
    "\n",
    "print(f\"{'Metric':<30} {'All Items':<20} {'Contributions':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Text Quality\n",
    "print(f\"{'TEXT QUALITY':<30}\")\n",
    "print(f\"{'  Character Error Rate':<30} {avg_cer_all:>18.2%} {avg_cer_contrib:>18.2%}\")\n",
    "print(f\"{'  Word Error Rate':<30} {avg_wer_all:>18.2%} {avg_wer_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Structure Quality\n",
    "print(f\"{'STRUCTURE QUALITY':<30}\")\n",
    "print(f\"{'  Boundary Detection F1':<30} {f1_all:>18.3f} {f1_contrib:>18.3f}\")\n",
    "print(f\"{'  Classification Accuracy':<30} {overall_accuracy_all:>18.2%} {overall_accuracy_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Metadata Quality\n",
    "print(f\"{'METADATA EXTRACTION':<30}\")\n",
    "print(f\"{'  Title F1':<30} {agg_all['title']['f1']:>18.3f} {agg_contrib['title']['f1']:>18.3f}\")\n",
    "print(f\"{'  Author F1':<30} {agg_all['author']['f1']:>18.3f} {agg_contrib['author']['f1']:>18.3f}\")\n",
    "print()\n",
    "\n",
    "# Continuation Tracking\n",
    "print(f\"{'CONTINUATION TRACKING':<30} {'N/A':<20} {'Contributions':<20}\")\n",
    "print(f\"{'  is_continuation F1':<30} {'':<20} {is_cont_f1:>18.3f}\")\n",
    "print(f\"{'  continues_on_next F1':<30} {'':<20} {continues_f1:>18.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94687802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONTINUATION FIELD ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“š GOLD STANDARD:\n",
      "  Files processed: 14\n",
      "  Total items: 70\n",
      "\n",
      "  is_continuation:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   63\n",
      "    Absent: 0\n",
      "\n",
      "  continues_on_next_page:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   63\n",
      "    Absent: 0\n",
      "\n",
      "\n",
      "ðŸ¤– PREDICTIONS:\n",
      "  Files processed: 14\n",
      "  Total items: 52\n",
      "\n",
      "  is_continuation:\n",
      "    True:   3\n",
      "    False:  0\n",
      "    Null:   49\n",
      "    Absent: 0\n",
      "\n",
      "  continues_on_next_page:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   45\n",
      "    Absent: 0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "\n",
      "is_continuation=True:\n",
      "  Gold has: 7\n",
      "  Pred has: 3\n",
      "  Detection rate: 3/7 = 42.9%\n",
      "\n",
      "continues_on_next_page=True:\n",
      "  Gold has: 7\n",
      "  Pred has: 7\n",
      "  Detection rate: 7/7 = 100.0%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/fabian-ramirez/Documents/These/Code/magazine_graphs\")\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "def analyze_continuation_fields(directory, label):\n",
    "    \"\"\"Count continuation field usage across all files.\"\"\"\n",
    "    stats = {\n",
    "        'is_continuation': {'true': 0, 'false': 0, 'null': 0, 'absent': 0},\n",
    "        'continues_on_next_page': {'true': 0, 'false': 0, 'null': 0, 'absent': 0},\n",
    "        'total_items': 0,\n",
    "        'files_processed': 0\n",
    "    }\n",
    "    \n",
    "    for json_file in sorted(directory.glob(\"*.json\")):\n",
    "        try:\n",
    "            data = json.loads(json_file.read_text(encoding='utf-8'))\n",
    "            items = data.get('items', [])\n",
    "            stats['files_processed'] += 1\n",
    "            \n",
    "            for item in items:\n",
    "                stats['total_items'] += 1\n",
    "                \n",
    "                # Check is_continuation\n",
    "                is_cont = item.get('is_continuation')\n",
    "                if is_cont is True:\n",
    "                    stats['is_continuation']['true'] += 1\n",
    "                elif is_cont is False:\n",
    "                    stats['is_continuation']['false'] += 1\n",
    "                elif is_cont is None:\n",
    "                    stats['is_continuation']['null'] += 1\n",
    "                else:  # key not present\n",
    "                    stats['is_continuation']['absent'] += 1\n",
    "                \n",
    "                # Check continues_on_next_page\n",
    "                continues = item.get('continues_on_next_page')\n",
    "                if continues is True:\n",
    "                    stats['continues_on_next_page']['true'] += 1\n",
    "                elif continues is False:\n",
    "                    stats['continues_on_next_page']['false'] += 1\n",
    "                elif continues is None:\n",
    "                    stats['continues_on_next_page']['null'] += 1\n",
    "                else:  # key not present\n",
    "                    stats['continues_on_next_page']['absent'] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file.name}: {e}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTINUATION FIELD ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze gold standard\n",
    "print(\"\\nðŸ“š GOLD STANDARD:\")\n",
    "gold_stats = analyze_continuation_fields(GOLD_DIR, \"Gold\")\n",
    "print(f\"  Files processed: {gold_stats['files_processed']}\")\n",
    "print(f\"  Total items: {gold_stats['total_items']}\")\n",
    "print(f\"\\n  is_continuation:\")\n",
    "print(f\"    True:   {gold_stats['is_continuation']['true']}\")\n",
    "print(f\"    False:  {gold_stats['is_continuation']['false']}\")\n",
    "print(f\"    Null:   {gold_stats['is_continuation']['null']}\")\n",
    "print(f\"    Absent: {gold_stats['is_continuation']['absent']}\")\n",
    "print(f\"\\n  continues_on_next_page:\")\n",
    "print(f\"    True:   {gold_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"    False:  {gold_stats['continues_on_next_page']['false']}\")\n",
    "print(f\"    Null:   {gold_stats['continues_on_next_page']['null']}\")\n",
    "print(f\"    Absent: {gold_stats['continues_on_next_page']['absent']}\")\n",
    "\n",
    "# Analyze predictions\n",
    "print(\"\\n\\nðŸ¤– PREDICTIONS:\")\n",
    "pred_stats = analyze_continuation_fields(PRED_DIR, \"Predictions\")\n",
    "print(f\"  Files processed: {pred_stats['files_processed']}\")\n",
    "print(f\"  Total items: {pred_stats['total_items']}\")\n",
    "print(f\"\\n  is_continuation:\")\n",
    "print(f\"    True:   {pred_stats['is_continuation']['true']}\")\n",
    "print(f\"    False:  {pred_stats['is_continuation']['false']}\")\n",
    "print(f\"    Null:   {pred_stats['is_continuation']['null']}\")\n",
    "print(f\"    Absent: {pred_stats['is_continuation']['absent']}\")\n",
    "print(f\"\\n  continues_on_next_page:\")\n",
    "print(f\"    True:   {pred_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"    False:  {pred_stats['continues_on_next_page']['false']}\")\n",
    "print(f\"    Null:   {pred_stats['continues_on_next_page']['null']}\")\n",
    "print(f\"    Absent: {pred_stats['continues_on_next_page']['absent']}\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nis_continuation=True:\")\n",
    "print(f\"  Gold has: {gold_stats['is_continuation']['true']}\")\n",
    "print(f\"  Pred has: {pred_stats['is_continuation']['true']}\")\n",
    "print(f\"  Detection rate: {pred_stats['is_continuation']['true']}/{gold_stats['is_continuation']['true']} = \"\n",
    "      f\"{pred_stats['is_continuation']['true']/gold_stats['is_continuation']['true']*100:.1f}%\" \n",
    "      if gold_stats['is_continuation']['true'] > 0 else \"  N/A\")\n",
    "\n",
    "print(f\"\\ncontinues_on_next_page=True:\")\n",
    "print(f\"  Gold has: {gold_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"  Pred has: {pred_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"  Detection rate: {pred_stats['continues_on_next_page']['true']}/{gold_stats['continues_on_next_page']['true']} = \"\n",
    "      f\"{pred_stats['continues_on_next_page']['true']/gold_stats['continues_on_next_page']['true']*100:.1f}%\"\n",
    "      if gold_stats['continues_on_next_page']['true'] > 0 else \"  N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e01889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Worst 5 Pages by Character Error Rate:\n",
      "1. La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   CER: 100.00%, WER: 100.00%\n",
      "   Gold: 8 items, 3634 chars\n",
      "   Pred: 0 items, 0 chars\n",
      "\n",
      "2. La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   CER: 21.48%, WER: 27.55%\n",
      "   Gold: 5 items, 4745 chars\n",
      "   Pred: 4 items, 4648 chars\n",
      "\n",
      "3. La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   CER: 14.49%, WER: 18.18%\n",
      "   Gold: 2 items, 69 chars\n",
      "   Pred: 3 items, 69 chars\n",
      "\n",
      "4. La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   CER: 9.63%, WER: 12.56%\n",
      "   Gold: 3 items, 5236 chars\n",
      "   Pred: 1 items, 5174 chars\n",
      "\n",
      "5. La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   CER: 8.12%, WER: 10.99%\n",
      "   Gold: 4 items, 1158 chars\n",
      "   Pred: 6 items, 1190 chars\n",
      "\n",
      "\n",
      "Pages with Item Count Mismatches:\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-001.json: Gold=8, Pred=5 (-3)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-004.json: Gold=5, Pred=4 (-1)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-005.json: Gold=5, Pred=4 (-1)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-006.json: Gold=6, Pred=4 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-007.json: Gold=2, Pred=3 (+1)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-009.json: Gold=7, Pred=5 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-010.json: Gold=6, Pred=4 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-011.json: Gold=3, Pred=1 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-012.json: Gold=8, Pred=0 (-8)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-014.json: Gold=4, Pred=6 (+2)\n",
      "\n",
      "Most Common Classification Errors:\n",
      "  â€¢ paratext â†’ prose: 20 times\n",
      "  â€¢ paratext â†’ verse: 4 times\n",
      "  â€¢ prose â†’ verse: 3 times\n",
      "  â€¢ verse â†’ prose: 2 times\n",
      "  â€¢ ad â†’ paratext: 1 times\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find worst performing pages by CER\n",
    "worst_pages_cer = sorted(all_results, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nWorst 5 Pages by Character Error Rate:\")\n",
    "for i, result in enumerate(worst_pages_cer, 1):\n",
    "    print(f\"{i}. {result['page']}\")\n",
    "    print(f\"   CER: {result['cer']:.2%}, WER: {result['wer']:.2%}\")\n",
    "    print(f\"   Gold: {result['gold_items']} items, {result['gold_chars']} chars\")\n",
    "    print(f\"   Pred: {result['pred_items']} items, {result['pred_chars']} chars\")\n",
    "    print()\n",
    "\n",
    "# Find pages with item count mismatches\n",
    "print(\"\\nPages with Item Count Mismatches:\")\n",
    "mismatches = [r for r in classification_results_all if r['gold_count'] != r['pred_count']]\n",
    "if mismatches:\n",
    "    for result in mismatches:\n",
    "        diff = result['pred_count'] - result['gold_count']\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"  â€¢ {result['page']}: Gold={result['gold_count']}, Pred={result['pred_count']} ({sign}{diff})\")\n",
    "else:\n",
    "    print(\"  No mismatches found!\")\n",
    "\n",
    "# Classification errors\n",
    "print(\"\\nMost Common Classification Errors:\")\n",
    "errors = []\n",
    "for gold_class, pred_dict in all_confusion.items():\n",
    "    for pred_class, count in pred_dict.items():\n",
    "        if gold_class != pred_class and count > 0:\n",
    "            errors.append((count, gold_class, pred_class))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, gold_class, pred_class in errors[:10]:\n",
    "    print(f\"  â€¢ {gold_class} â†’ {pred_class}: {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
