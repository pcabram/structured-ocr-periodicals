{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Evaluation\n",
      "\n",
      "\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "Predictions: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/interim_pages/La_Plume_bpt6k1185893k_1_10_1889\n",
      "\n",
      "Dataset:\n",
      "  Gold files: 14\n",
      "  Pred files: 14\n",
      "  Matching pairs: 14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Path setup\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import schemas for validation\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "\n",
    "# Paths\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "print(\"Stage 1 OCR Evaluation\")\n",
    "print(\"\\n\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Gold standard: {GOLD_DIR}\")\n",
    "print(f\"Predictions: {PRED_DIR}\")\n",
    "\n",
    "# Find common files\n",
    "def load_page_pairs() -> List[Tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Match gold standard files with prediction files by filename.\n",
    "    Returns list of (gold_path, pred_path) tuples.\n",
    "    \"\"\"\n",
    "    gold_files = {f.name: f for f in GOLD_DIR.glob(\"*.json\")}\n",
    "    pred_files = {f.name: f for f in PRED_DIR.glob(\"*.json\")}\n",
    "    \n",
    "    common_names = set(gold_files.keys()) & set(pred_files.keys())\n",
    "    \n",
    "    pairs = [(gold_files[name], pred_files[name]) for name in sorted(common_names)]\n",
    "    \n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Gold files: {len(gold_files)}\")\n",
    "    print(f\"  Pred files: {len(pred_files)}\")\n",
    "    print(f\"  Matching pairs: {len(pairs)}\")\n",
    "    \n",
    "    if len(pairs) < len(gold_files):\n",
    "        missing = set(gold_files.keys()) - set(pred_files.keys())\n",
    "        print(f\"Warning: {len(missing)} gold standard pages without predictions:\")\n",
    "        for name in sorted(missing):\n",
    "            print(f\"   - {name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "page_pairs = load_page_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2a9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Item Matching Configuration\n",
      "\n",
      "\n",
      "Similarity threshold: 0.7\n",
      "\n",
      "\n",
      "Item Matching Test\n",
      "\n",
      "\n",
      "\n",
      "Test page: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Gold items: 8\n",
      "  Pred items: 5\n",
      "  Matches found: 4\n",
      "  Unmatched gold: 4\n",
      "  Unmatched pred: 1\n",
      "  Average match quality: 92.47%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Match gold items to predicted items using content-based text similarity.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_THRESHOLD = 0.7  # Minimum text similarity to consider a match (0.0-1.0)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Configuration\")\n",
    "print(\"\\n\")\n",
    "print(f\"Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "        \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item, find the best-matching unmatched pred item.\n",
    "        Accept the match if similarity exceeds threshold.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair and match items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "\n",
    "# Test matching on first page\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Test\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if page_pairs:\n",
    "    test_gold, test_pred = page_pairs[0]\n",
    "    test_result = load_and_match_page(test_gold, test_pred)\n",
    "    \n",
    "    print(f\"\\nTest page: {test_result['page_name']}\")\n",
    "    print(f\"  Gold items: {len(test_result['gold_items'])}\")\n",
    "    print(f\"  Pred items: {len(test_result['pred_items'])}\")\n",
    "    print(f\"  Matches found: {len(test_result['matches'])}\")\n",
    "    print(f\"  Unmatched gold: {len(test_result['unmatched_gold'])}\")\n",
    "    print(f\"  Unmatched pred: {len(test_result['unmatched_pred'])}\")\n",
    "    \n",
    "    if test_result['matches']:\n",
    "        avg_score = sum(score for _, _, score in test_result['matches']) / len(test_result['matches'])\n",
    "        print(f\"  Average match quality: {avg_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7322404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running diagnostics on all pages...\n",
      "\n",
      "\n",
      "\n",
      "SUMMARY TABLE\n",
      "\n",
      "\n",
      "                                   page_id  gold_items  pred_items  matched  match_rate_%  contrib_match_rate_%  avg_similarity  gold_cont_in  pred_cont_in  gold_cont_out  pred_cont_out                                                            flags\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001           8           5        4          50.0                   0.0           0.925             0             0              0              0                                                   COUNT_MISMATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002           2           2        2         100.0                   0.0           1.000             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003           3           3        1          33.3                   0.0           0.713             0             1              1              2                                           LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004           5           4        3          60.0                 100.0           0.942             1             1              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005           5           4        1          20.0                  33.3           0.755             1             0              1              1                                           LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006           6           4        3          50.0                  75.0           0.967             1             0              1              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007           2           3        1          50.0                   0.0           1.000             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008           0           0        0           0.0                   0.0           0.000             0             0              0              0                              ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009           7           5        4          57.1                  80.0           0.953             1             0              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010           6           4        4          66.7                 100.0           0.971             1             0              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011           3           1        0           0.0                   0.0           0.000             1             1              1              1                             ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012           8           0        0           0.0                   0.0           0.000             1             0              0              0 ZERO_PREDS, ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB, COUNT_MISMATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013          11          11       10          90.9                   0.0           0.951             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014           4           6        4         100.0                   0.0           0.968             0             0              0              0                                                                 \n",
      "\n",
      "\n",
      "================================================================================\n",
      "DETAILED REPORTS\n",
      "================================================================================\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-001 ===\n",
      "Items: 8 gold, 5 pred\n",
      "Matches: 4 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   8 gold, 5 pred, 4 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.925\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1, 2, 3, 7]\n",
      "Unmatched pred items: [1]\n",
      "\n",
      "FLAGS: COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-002 ===\n",
      "Items: 2 gold, 2 pred\n",
      "Matches: 2 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 2 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-003 ===\n",
      "Items: 3 gold, 3 pred\n",
      "Matches: 1 (33.3% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 1 matched (50.0%)\n",
      "  prose      1 gold, 2 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 2 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.713\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 2 pred\n",
      "\n",
      "Unmatched gold items: [1, 2]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-004 ===\n",
      "Items: 5 gold, 4 pred\n",
      "Matches: 3 (60.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 3 pred, 2 matched (100.0%)\n",
      "  verse      1 gold, 1 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 4 pred, 3 matched (100.0%)\n",
      "Avg similarity: 0.942\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1]\n",
      "Unmatched pred items: [1]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-005 ===\n",
      "Items: 5 gold, 4 pred\n",
      "Matches: 1 (20.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 0 matched (0.0%)\n",
      "  verse      1 gold, 2 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 4 pred, 1 matched (33.3%)\n",
      "Avg similarity: 0.755\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 4]\n",
      "Unmatched pred items: [0, 1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-006 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 3 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 3 matched (75.0%)\n",
      "Avg similarity: 0.967\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 5]\n",
      "Unmatched pred items: [3]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-007 ===\n",
      "Items: 2 gold, 3 pred\n",
      "Matches: 1 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 3 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-008 ===\n",
      "Items: 0 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-009 ===\n",
      "Items: 7 gold, 5 pred\n",
      "Matches: 4 (57.1% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 1 pred, 2 matched (100.0%)\n",
      "  verse      3 gold, 4 pred, 2 matched (66.7%)\n",
      "\n",
      "Contributions: 5 gold, 5 pred, 4 matched (80.0%)\n",
      "Avg similarity: 0.953\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 4]\n",
      "Unmatched pred items: [2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-010 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 4 (66.7% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 2 matched (100.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 4 matched (100.0%)\n",
      "Avg similarity: 0.971\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1]\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-011 ===\n",
      "Items: 3 gold, 1 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      1 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2]\n",
      "Unmatched pred items: [0]\n",
      "\n",
      "FLAGS: ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-012 ===\n",
      "Items: 8 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   4 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 0 pred, 0 matched (0.0%)\n",
      "  verse      2 gold, 0 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 4 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB, COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-013 ===\n",
      "Items: 11 gold, 11 pred\n",
      "Matches: 10 (90.9% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   11 gold, 1 pred, 10 matched (90.9%)\n",
      "  prose      0 gold, 10 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 10 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.951\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [7]\n",
      "Unmatched pred items: [7]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-014 ===\n",
      "Items: 4 gold, 6 pred\n",
      "Matches: 4 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  ad         1 gold, 0 pred, 1 matched (100.0%)\n",
      "  paratext   3 gold, 5 pred, 3 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.968\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-Level Diagnostics\n",
    "Generate diagnostic metrics for each page based on item matches.\n",
    "\"\"\"\n",
    "\n",
    "def diagnose_page(page_id: str, gold_items: list, pred_items: list, matches: list) -> dict:\n",
    "    \"\"\"\n",
    "    Generate diagnostic metrics for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page_id: Page identifier\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with diagnostic metrics\n",
    "    \"\"\"\n",
    "    # Count items by class\n",
    "    gold_by_class = {}\n",
    "    pred_by_class = {}\n",
    "    \n",
    "    for item in gold_items:\n",
    "        item_class = item['item_class']\n",
    "        gold_by_class[item_class] = gold_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    for item in pred_items:\n",
    "        item_class = item['item_class']\n",
    "        pred_by_class[item_class] = pred_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    # Count contributions (prose + verse)\n",
    "    gold_contrib = gold_by_class.get('prose', 0) + gold_by_class.get('verse', 0)\n",
    "    pred_contrib = pred_by_class.get('prose', 0) + pred_by_class.get('verse', 0)\n",
    "    \n",
    "    # Filter matches by contribution class\n",
    "    contrib_matches = [\n",
    "        (g_idx, p_idx, score) for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in ('prose', 'verse')\n",
    "    ]\n",
    "    \n",
    "    # Calculate match rates\n",
    "    match_rate = (len(matches) / len(gold_items) * 100) if gold_items else 0\n",
    "    contrib_match_rate = (len(contrib_matches) / gold_contrib * 100) if gold_contrib else 0\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    avg_similarity = (sum(score for _, _, score in matches) / len(matches)) if matches else 0\n",
    "    \n",
    "    # Count continuation flags\n",
    "    gold_cont_in = sum(1 for item in gold_items if item.get('is_continuation') is True)\n",
    "    pred_cont_in = sum(1 for item in pred_items if item.get('is_continuation') is True)\n",
    "    gold_cont_out = sum(1 for item in gold_items if item.get('continues_on_next_page') is True)\n",
    "    pred_cont_out = sum(1 for item in pred_items if item.get('continues_on_next_page') is True)\n",
    "    \n",
    "    # Track matched indices\n",
    "    matched_gold = {g_idx for g_idx, _, _ in matches}\n",
    "    matched_pred = {p_idx for _, p_idx, _ in matches}\n",
    "    \n",
    "    unmatched_gold = [i for i in range(len(gold_items)) if i not in matched_gold]\n",
    "    unmatched_pred = [i for i in range(len(pred_items)) if i not in matched_pred]\n",
    "    \n",
    "    # Count matches by class\n",
    "    matches_by_class = {}\n",
    "    for g_idx, p_idx, score in matches:\n",
    "        item_class = gold_items[g_idx]['item_class']\n",
    "        matches_by_class[item_class] = matches_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items),\n",
    "        'matched': len(matches),\n",
    "        'match_rate': match_rate,\n",
    "        'contrib_match_rate': contrib_match_rate,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'gold_cont_in': gold_cont_in,\n",
    "        'pred_cont_in': pred_cont_in,\n",
    "        'gold_cont_out': gold_cont_out,\n",
    "        'pred_cont_out': pred_cont_out,\n",
    "        'gold_by_class': gold_by_class,\n",
    "        'pred_by_class': pred_by_class,\n",
    "        'matches_by_class': matches_by_class,\n",
    "        'gold_contrib': gold_contrib,\n",
    "        'pred_contrib': pred_contrib,\n",
    "        'contrib_matched': len(contrib_matches),\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def flag_page(metrics: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate flags for problematic pages based on metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary from diagnose_page()\n",
    "        \n",
    "    Returns:\n",
    "        Comma-separated string of flags, or empty string if no issues\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    if metrics['pred_items'] == 0:\n",
    "        flags.append('ZERO_PREDS')\n",
    "    \n",
    "    if metrics['matched'] == 0:\n",
    "        flags.append('ZERO_MATCHES')\n",
    "    \n",
    "    if metrics['match_rate'] < 50:\n",
    "        flags.append('LOW_MATCH')\n",
    "    \n",
    "    if metrics['gold_contrib'] > 0 and metrics['contrib_match_rate'] < 60:\n",
    "        flags.append('LOW_CONTRIB')\n",
    "    \n",
    "    if abs(metrics['gold_items'] - metrics['pred_items']) >= 3:\n",
    "        flags.append('COUNT_MISMATCH')\n",
    "    \n",
    "    return ', '.join(flags)\n",
    "\n",
    "\n",
    "def run_diagnostics(page_pairs: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run diagnostics on all pages and generate summary table and detailed reports.\n",
    "    \n",
    "    Args:\n",
    "        page_pairs: List of (gold_path, pred_path) tuples from load_page_pairs()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary metrics for all pages\n",
    "    \"\"\"\n",
    "    print(\"Running diagnostics on all pages...\\n\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for gold_path, pred_path in page_pairs:\n",
    "        # Extract page_id from filename\n",
    "        page_id = gold_path.stem\n",
    "        \n",
    "        # Load and match page\n",
    "        result = load_and_match_page(gold_path, pred_path)\n",
    "        gold_items = result['gold_items']\n",
    "        pred_items = result['pred_items']\n",
    "        matches = result['matches']\n",
    "        \n",
    "        # Generate metrics\n",
    "        metrics = diagnose_page(page_id, gold_items, pred_items, matches)\n",
    "        metrics['flags'] = flag_page(metrics)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for m in all_metrics:\n",
    "        summary_data.append({\n",
    "            'page_id': m['page_id'],\n",
    "            'gold_items': m['gold_items'],\n",
    "            'pred_items': m['pred_items'],\n",
    "            'matched': m['matched'],\n",
    "            'match_rate_%': round(m['match_rate'], 1),\n",
    "            'contrib_match_rate_%': round(m['contrib_match_rate'], 1),\n",
    "            'avg_similarity': round(m['avg_similarity'], 3),\n",
    "            'gold_cont_in': m['gold_cont_in'],\n",
    "            'pred_cont_in': m['pred_cont_in'],\n",
    "            'gold_cont_out': m['gold_cont_out'],\n",
    "            'pred_cont_out': m['pred_cont_out'],\n",
    "            'flags': m['flags']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print detailed reports for all pages\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for m in all_metrics:\n",
    "        print(f\"\\n=== Page {m['page_id']} ===\")\n",
    "        print(f\"Items: {m['gold_items']} gold, {m['pred_items']} pred\")\n",
    "        print(f\"Matches: {m['matched']} ({m['match_rate']:.1f}% match rate)\")\n",
    "        \n",
    "        print(\"\\nBy class:\")\n",
    "        all_classes = sorted(set(m['gold_by_class'].keys()) | set(m['pred_by_class'].keys()))\n",
    "        for cls in all_classes:\n",
    "            gold_count = m['gold_by_class'].get(cls, 0)\n",
    "            pred_count = m['pred_by_class'].get(cls, 0)\n",
    "            matched_count = m['matches_by_class'].get(cls, 0)\n",
    "            match_pct = (matched_count / gold_count * 100) if gold_count > 0 else 0\n",
    "            print(f\"  {cls:10s} {gold_count} gold, {pred_count} pred, {matched_count} matched ({match_pct:.1f}%)\")\n",
    "\n",
    "        \n",
    "        print(f\"\\nContributions: {m['gold_contrib']} gold, {m['pred_contrib']} pred, \"\n",
    "              f\"{m['contrib_matched']} matched ({m['contrib_match_rate']:.1f}%)\")\n",
    "        print(f\"Avg similarity: {m['avg_similarity']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nContinuations:\")\n",
    "        print(f\"  is_continuation: {m['gold_cont_in']} gold, {m['pred_cont_in']} pred\")\n",
    "        print(f\"  continues_on_next_page: {m['gold_cont_out']} gold, {m['pred_cont_out']} pred\")\n",
    "        \n",
    "        print(f\"\\nUnmatched gold items: {m['unmatched_gold']}\")\n",
    "        print(f\"Unmatched pred items: {m['unmatched_pred']}\")\n",
    "        \n",
    "        if m['flags']:\n",
    "            print(f\"\\nFLAGS: {m['flags']}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostic_df = run_diagnostics(page_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fcf45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and matching all pages...\n",
      "Loaded 14 pages\n",
      "Total matches across all pages: 37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation Helpers\n",
    "Utility functions for filtering matches and loading all pages efficiently.\n",
    "These helpers are used by the evaluation cells that follow.\n",
    "\"\"\"\n",
    "\n",
    "def filter_matches_by_class(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    item_classes: List[str]\n",
    ") -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Filter matches to only include items of specified classes.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        item_classes: List of classes to include (e.g., ['prose', 'verse'])\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of matches\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (g_idx, p_idx, score) \n",
    "        for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in item_classes\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_matched_pairs(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict]\n",
    ") -> List[Tuple[Dict, Dict, float]]:\n",
    "    \"\"\"\n",
    "    Convert match indices to actual item pairs.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_item, pred_item, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (gold_items[g_idx], pred_items[p_idx], score)\n",
    "        for g_idx, p_idx, score in matches\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_all_pages(page_pairs: List[Tuple[Path, Path]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and match all pages at once for efficient batch evaluation.\n",
    "    \n",
    "    Args:\n",
    "        page_pairs: List of (gold_path, pred_path) tuples from load_page_pairs()\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, one per page, each containing:\n",
    "        - page_id: Page identifier\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "    \n",
    "    for gold_path, pred_path in page_pairs:\n",
    "        result = load_and_match_page(gold_path, pred_path)\n",
    "        result['page_id'] = gold_path.stem\n",
    "        all_pages.append(result)\n",
    "    \n",
    "    return all_pages\n",
    "\n",
    "\n",
    "# Load all pages once for reuse in subsequent evaluation cells\n",
    "print(\"Loading and matching all pages...\")\n",
    "all_pages = load_all_pages(page_pairs)\n",
    "print(f\"Loaded {len(all_pages)} pages\")\n",
    "print(f\"Total matches across all pages: {sum(len(page['matches']) for page in all_pages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text quality...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEXT QUALITY SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. ORDER-AGNOSTIC EVALUATION\n",
      "   (Pure OCR quality, reading order irrelevant)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 14.83%  |  WER: 17.66%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 14.83%  |  WER: 17.66%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 13.48%\n",
      "\n",
      "   Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 18.29%  |  WER: 20.85%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 18.29%  |  WER: 20.85%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 17.94%\n",
      "\n",
      "======================================================================\n",
      "2. STRUCTURE-AWARE EVALUATION\n",
      "   (OCR quality on matched content only)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   Matched Content - All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 15.13%  |  WER: 21.12%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 14.61%  |  WER: 21.12%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 13.81%\n",
      "      Coverage: 27,797 chars matched (59.0% of gold)\n",
      "      Unmatched: 19,350 chars (41.1% of gold)\n",
      "\n",
      "   Matched Content - Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 13.05%  |  WER: 17.63%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 12.42%  |  WER: 17.63%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 12.11%\n",
      "      Coverage: 17,510 chars matched (48.8% of gold)\n",
      "      Unmatched: 18,365 chars (51.2% of gold)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION GUIDE:\n",
      "----------------------------------------------------------------------\n",
      "Strict: Most conservative\n",
      "Standard: Fair baseline - normalizes whitespace\n",
      "Letters Only: Most lenient - pure character recognition quality\n",
      "\n",
      "======================================================================\n",
      "\n",
      "KEY INSIGHTS:\n",
      "- Pure OCR quality (standard normalization): 14.83%\n",
      "- Letter recognition quality: 13.48%\n",
      "- Structure failures (unmatched content): 41.1%\n",
      "- Contributions:\n",
      "    Standard CER: 12.42%\n",
      "    Successfully matched: 48.8%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Text Quality Evaluation\n",
    "Calculate CER and WER using two complementary approaches:\n",
    "1. Order-agnostic: Pure OCR quality regardless of reading order\n",
    "2. Structure-aware: OCR quality on properly aligned content via matching\n",
    "\n",
    "Each approach calculates three normalization levels:\n",
    "- Strict: Preserves all whitespace (including \\n vs \\n\\n differences)\n",
    "- Standard: Normalizes whitespace to single spaces (RECOMMENDED)\n",
    "- Letters Only: Removes all whitespace and punctuation (pure character recognition)\n",
    "\n",
    "References:\n",
    "- Flexible Character Accuracy (FCA) for handling reading order issues:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "- Token sort ratio for order-agnostic OCR comparison:\n",
    "  https://urban-institute.medium.com/choosing-the-right-ocr-service-for-extracting-text-data-d7830399ec5\n",
    "- Unicode normalization and whitespace handling in OCR evaluation:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_text_strict(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strict normalization: only Unicode NFC normalization.\n",
    "    Preserves all whitespace, punctuation, and capitalization.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "\n",
    "def normalize_text_standard(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standard normalization for fair OCR evaluation:\n",
    "    - Unicode NFC normalization\n",
    "    - All whitespace (spaces, tabs, newlines) â†’ single space\n",
    "    - Preserves punctuation and capitalization\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text_letters_only(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Letter-only normalization for pure character recognition quality:\n",
    "    - Unicode NFC normalization\n",
    "    - Remove all whitespace\n",
    "    - Remove all punctuation\n",
    "    - Preserves capitalization and diacritics\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'[^\\w]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def character_error_rate(reference: str, hypothesis: str, normalization: str = 'strict') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Character Error Rate using Levenshtein distance.\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: OCR output text\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        CER = (insertions + deletions + substitutions) / total_reference_chars\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        ref = normalize_text_letters_only(reference)\n",
    "        hyp = normalize_text_letters_only(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    if not ref:\n",
    "        return 1.0 if hyp else 0.0\n",
    "    distance = Levenshtein.distance(ref, hyp)\n",
    "    return distance / len(ref)\n",
    "\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str, normalization: str = 'strict') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate using Levenshtein distance on words.\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: OCR output text\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        WER = (insertions + deletions + substitutions) / total_reference_words\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        # For letters only, WER doesn't make sense without word boundaries\n",
    "        # So we use standard normalization\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    if not ref_words:\n",
    "        return 1.0 if hyp_words else 0.0\n",
    "    distance = Levenshtein.distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "\n",
    "def token_sort_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Sort tokens (words) alphabetically for order-agnostic comparison.\n",
    "    This removes the impact of reading order on text similarity.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return ' '.join(sorted(tokens))\n",
    "\n",
    "\n",
    "def evaluate_order_agnostic(gold_items: List[Dict], pred_items: List[Dict], \n",
    "                            item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality without considering reading order.\n",
    "    Uses token sort ratio approach - sorts all words before comparison.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        item_classes: If provided, filter to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER for each normalization level, and text statistics\n",
    "    \"\"\"\n",
    "    # Filter by class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items if item['item_class'] in item_classes]\n",
    "        pred_items = [item for item in pred_items if item['item_class'] in item_classes]\n",
    "    \n",
    "    # Concatenate all text\n",
    "    gold_text = ' '.join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = ' '.join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    # Sort tokens for order-agnostic comparison\n",
    "    gold_sorted = token_sort_text(gold_text)\n",
    "    pred_sorted = token_sort_text(pred_text)\n",
    "    \n",
    "    # Calculate for all three normalization levels\n",
    "    results = {\n",
    "        'cer_strict': character_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'wer_strict': word_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'cer_standard': character_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'wer_standard': word_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'cer_letters': character_error_rate(gold_sorted, pred_sorted, 'letters_only'),\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split())\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_structure_aware(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                             matches: List[Tuple[int, int, float]],\n",
    "                             item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality on matched pairs, respecting document structure.\n",
    "    Only compares content that was successfully aligned via matching.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        item_classes: If provided, filter matches to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with matched CER/WER for each normalization level and unmatched content statistics\n",
    "    \"\"\"\n",
    "    # Filter matches by class if specified\n",
    "    if item_classes:\n",
    "        filtered_matches = filter_matches_by_class(matches, gold_items, item_classes)\n",
    "    else:\n",
    "        filtered_matches = matches\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(filtered_matches, gold_items, pred_items)\n",
    "    \n",
    "    # Calculate CER/WER on matched content for all normalization levels\n",
    "    if matched_pairs:\n",
    "        # Concatenate matched texts in gold order\n",
    "        gold_matched_text = ' '.join(gold_item.get('item_text_raw', '') \n",
    "                                     for gold_item, _, _ in matched_pairs)\n",
    "        pred_matched_text = ' '.join(pred_item.get('item_text_raw', '') \n",
    "                                     for _, pred_item, _ in matched_pairs)\n",
    "        \n",
    "        cer_strict = character_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        wer_strict = word_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        cer_standard = character_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        wer_standard = word_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        cer_letters = character_error_rate(gold_matched_text, pred_matched_text, 'letters_only')\n",
    "        \n",
    "        matched_gold_chars = len(gold_matched_text)\n",
    "        matched_pred_chars = len(pred_matched_text)\n",
    "    else:\n",
    "        cer_strict = 0.0\n",
    "        wer_strict = 0.0\n",
    "        cer_standard = 0.0\n",
    "        wer_standard = 0.0\n",
    "        cer_letters = 0.0\n",
    "        matched_gold_chars = 0\n",
    "        matched_pred_chars = 0\n",
    "    \n",
    "    # Calculate unmatched content\n",
    "    matched_gold_indices = {g_idx for g_idx, _, _ in filtered_matches}\n",
    "    matched_pred_indices = {p_idx for _, p_idx, _ in filtered_matches}\n",
    "    \n",
    "    if item_classes:\n",
    "        # Only count unmatched items of the specified classes\n",
    "        unmatched_gold_items = [\n",
    "            gold_items[i] for i in range(len(gold_items))\n",
    "            if i not in matched_gold_indices and gold_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        unmatched_pred_items = [\n",
    "            pred_items[i] for i in range(len(pred_items))\n",
    "            if i not in matched_pred_indices and pred_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                              for item in gold_items if item['item_class'] in item_classes)\n",
    "    else:\n",
    "        unmatched_gold_items = [gold_items[i] for i in range(len(gold_items)) \n",
    "                               if i not in matched_gold_indices]\n",
    "        unmatched_pred_items = [pred_items[i] for i in range(len(pred_items)) \n",
    "                               if i not in matched_pred_indices]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    unmatched_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_gold_items)\n",
    "    unmatched_pred_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_pred_items)\n",
    "    \n",
    "    return {\n",
    "        'cer_strict': cer_strict,\n",
    "        'wer_strict': wer_strict,\n",
    "        'cer_standard': cer_standard,\n",
    "        'wer_standard': wer_standard,\n",
    "        'cer_letters': cer_letters,\n",
    "        'matched_gold_chars': matched_gold_chars,\n",
    "        'matched_pred_chars': matched_pred_chars,\n",
    "        'unmatched_gold_chars': unmatched_gold_chars,\n",
    "        'unmatched_pred_chars': unmatched_pred_chars,\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'matched_percentage': (matched_gold_chars / total_gold_chars * 100) if total_gold_chars else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate text quality across all pages\n",
    "print(\"Evaluating text quality...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "order_agnostic_all = []\n",
    "order_agnostic_contrib = []\n",
    "structure_aware_all = []\n",
    "structure_aware_contrib = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Order-agnostic evaluation\n",
    "    oa_all = evaluate_order_agnostic(gold_items, pred_items)\n",
    "    oa_all['page_id'] = page_id\n",
    "    order_agnostic_all.append(oa_all)\n",
    "    \n",
    "    oa_contrib = evaluate_order_agnostic(gold_items, pred_items, \n",
    "                                         item_classes=['prose', 'verse'])\n",
    "    oa_contrib['page_id'] = page_id\n",
    "    order_agnostic_contrib.append(oa_contrib)\n",
    "    \n",
    "    # Structure-aware evaluation\n",
    "    sa_all = evaluate_structure_aware(gold_items, pred_items, matches)\n",
    "    sa_all['page_id'] = page_id\n",
    "    structure_aware_all.append(sa_all)\n",
    "    \n",
    "    sa_contrib = evaluate_structure_aware(gold_items, pred_items, matches,\n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    sa_contrib['page_id'] = page_id\n",
    "    structure_aware_contrib.append(sa_contrib)\n",
    "\n",
    "# Calculate averages for order-agnostic evaluation\n",
    "avg_oa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in order_agnostic_all) / len(order_agnostic_all)\n",
    "}\n",
    "\n",
    "contrib_with_content = [r for r in order_agnostic_contrib if r['gold_chars'] > 0]\n",
    "avg_oa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in contrib_with_content) / len(contrib_with_content)\n",
    "}\n",
    "\n",
    "# Calculate averages for structure-aware evaluation\n",
    "sa_all_with_matches = [r for r in structure_aware_all if r['matched_gold_chars'] > 0]\n",
    "avg_sa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_all_with_matches) / len(sa_all_with_matches)\n",
    "}\n",
    "\n",
    "sa_contrib_with_matches = [r for r in structure_aware_contrib if r['matched_gold_chars'] > 0]\n",
    "avg_sa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches)\n",
    "}\n",
    "\n",
    "# Calculate total matched percentages\n",
    "total_sa_all_matched = sum(r['matched_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_gold = sum(r['total_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_all)\n",
    "\n",
    "total_sa_contrib_matched = sum(r['matched_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_gold = sum(r['total_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_contrib)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORDER-AGNOSTIC EVALUATION\")\n",
    "print(\"   (Pure OCR quality, reading order irrelevant)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_strict']:.2%}  |  WER: {avg_oa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_standard']:.2%}  |  WER: {avg_oa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_letters']:.2%}\")\n",
    "\n",
    "print(f\"\\n   Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_strict']:.2%}  |  WER: {avg_oa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_standard']:.2%}  |  WER: {avg_oa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_letters']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STRUCTURE-AWARE EVALUATION\")\n",
    "print(\"   (OCR quality on matched content only)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   Matched Content - All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_strict']:.2%}  |  WER: {avg_sa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_standard']:.2%}  |  WER: {avg_sa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_all_matched:,} chars matched \" +\n",
    "      f\"({total_sa_all_matched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_all_unmatched:,} chars \" +\n",
    "      f\"({total_sa_all_unmatched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(f\"\\n   Matched Content - Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_strict']:.2%}  |  WER: {avg_sa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_standard']:.2%}  |  WER: {avg_sa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_contrib_matched:,} chars matched \" +\n",
    "      f\"({total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_contrib_unmatched:,} chars \" +\n",
    "      f\"({total_sa_contrib_unmatched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Strict: Most conservative\")\n",
    "print(\"Standard: Fair baseline - normalizes whitespace\")\n",
    "print(\"Letters Only: Most lenient - pure character recognition quality\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(f\"- Pure OCR quality (standard normalization): {avg_oa_all['cer_standard']:.2%}\")\n",
    "print(f\"- Letter recognition quality: {avg_oa_all['cer_letters']:.2%}\")\n",
    "print(f\"- Structure failures (unmatched content): {total_sa_all_unmatched/total_sa_all_gold*100:.1f}%\")\n",
    "print(f\"- Contributions:\")\n",
    "print(f\"    Standard CER: {avg_sa_contrib['cer_standard']:.2%}\")\n",
    "print(f\"    Successfully matched: {total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e807f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item boundary detection...\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   All items - P: 40.00%, R: 25.00%, F1: 0.308\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   All items - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   All items - P: 66.67%, R: 66.67%, F1: 0.667\n",
      "   Contributions - P: 50.00%, R: 100.00%, F1: 0.667\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   All items - P: 75.00%, R: 60.00%, F1: 0.667\n",
      "   Contributions - P: 75.00%, R: 100.00%, F1: 0.857\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   All items - P: 25.00%, R: 20.00%, F1: 0.222\n",
      "   Contributions - P: 25.00%, R: 33.33%, F1: 0.286\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   All items - P: 50.00%, R: 33.33%, F1: 0.400\n",
      "   Contributions - P: 50.00%, R: 50.00%, F1: 0.500\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   All items - P: 100.00%, R: 150.00%, F1: 1.200\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   All items - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   All items - P: 100.00%, R: 71.43%, F1: 0.833\n",
      "   Contributions - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   All items - P: 25.00%, R: 16.67%, F1: 0.200\n",
      "   Contributions - P: 25.00%, R: 25.00%, F1: 0.250\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   All items - P: 100.00%, R: 33.33%, F1: 0.500\n",
      "   Contributions - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   All items - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   All items - P: 27.27%, R: 27.27%, F1: 0.273\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   All items - P: 16.67%, R: 25.00%, F1: 0.200\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "============================================================\n",
      "ITEM BOUNDARY DETECTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Precision: 50.00%\n",
      "   Recall: 37.14%\n",
      "   F1: 0.426\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Precision: 38.89%\n",
      "   Recall: 56.00%\n",
      "   F1: 0.459\n"
     ]
    }
   ],
   "source": [
    "def evaluate_item_boundaries(gold_path: Path, pred_path: Path, \n",
    "                            item_classes: Optional[List[str]] = None,\n",
    "                            tolerance: int = 20) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate whether item boundaries are correctly detected.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        item_classes: If provided, only evaluate these item classes\n",
    "        tolerance: Character tolerance window for boundary matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    def get_boundaries(items, filter_classes=None):\n",
    "        \"\"\"Get character positions where items start.\"\"\"\n",
    "        boundaries = []\n",
    "        pos = 0\n",
    "        for item in items:\n",
    "            if filter_classes is None or item.get('item_class') in filter_classes:\n",
    "                boundaries.append(pos)\n",
    "            # Add text length + separator\n",
    "            pos += len(item.get('item_text_raw', '')) + 2\n",
    "        return set(boundaries)\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    gold_bounds = get_boundaries(gold_items, item_classes)\n",
    "    pred_bounds = get_boundaries(pred_items, item_classes)\n",
    "    \n",
    "    # Match boundaries within tolerance\n",
    "    tp = 0\n",
    "    for pred_b in pred_bounds:\n",
    "        if any(abs(pred_b - gold_b) <= tolerance for gold_b in gold_bounds):\n",
    "            tp += 1\n",
    "    \n",
    "    fp = len(pred_bounds) - tp\n",
    "    fn = len(gold_bounds) - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'gold_boundaries': len(gold_bounds),\n",
    "        'pred_boundaries': len(pred_bounds)\n",
    "    }\n",
    "\n",
    "# Evaluate item boundaries\n",
    "print(\"Evaluating item boundary detection...\\n\")\n",
    "\n",
    "boundary_results_all = []\n",
    "boundary_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_item_boundaries(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    boundary_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_item_boundaries(gold_path, pred_path,\n",
    "                                             item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    boundary_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   All items - P: {result_all['precision']:.2%}, \"\n",
    "          f\"R: {result_all['recall']:.2%}, F1: {result_all['f1']:.3f}\")\n",
    "    print(f\"   Contributions - P: {result_contrib['precision']:.2%}, \"\n",
    "          f\"R: {result_contrib['recall']:.2%}, F1: {result_contrib['f1']:.3f}\\n\")\n",
    "\n",
    "# Compute micro-averages (sum all TP/FP/FN, then compute metrics)\n",
    "total_tp_all = sum(r['tp'] for r in boundary_results_all)\n",
    "total_fp_all = sum(r['fp'] for r in boundary_results_all)\n",
    "total_fn_all = sum(r['fn'] for r in boundary_results_all)\n",
    "\n",
    "precision_all = total_tp_all / (total_tp_all + total_fp_all) if (total_tp_all + total_fp_all) > 0 else 0\n",
    "recall_all = total_tp_all / (total_tp_all + total_fn_all) if (total_tp_all + total_fn_all) > 0 else 0\n",
    "f1_all = 2 * precision_all * recall_all / (precision_all + recall_all) if (precision_all + recall_all) > 0 else 0\n",
    "\n",
    "total_tp_contrib = sum(r['tp'] for r in boundary_results_contrib)\n",
    "total_fp_contrib = sum(r['fp'] for r in boundary_results_contrib)\n",
    "total_fn_contrib = sum(r['fn'] for r in boundary_results_contrib)\n",
    "\n",
    "precision_contrib = total_tp_contrib / (total_tp_contrib + total_fp_contrib) if (total_tp_contrib + total_fp_contrib) > 0 else 0\n",
    "recall_contrib = total_tp_contrib / (total_tp_contrib + total_fn_contrib) if (total_tp_contrib + total_fn_contrib) > 0 else 0\n",
    "f1_contrib = 2 * precision_contrib * recall_contrib / (precision_contrib + recall_contrib) if (precision_contrib + recall_contrib) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ITEM BOUNDARY DETECTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Precision: {precision_all:.2%}\")\n",
    "print(f\"   Recall: {recall_all:.2%}\")\n",
    "print(f\"   F1: {f1_all:.3f}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Precision: {precision_contrib:.2%}\")\n",
    "print(f\"   Recall: {recall_contrib:.2%}\")\n",
    "print(f\"   F1: {f1_contrib:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item classification...\n",
      "\n",
      "Item count mismatch: gold=8, pred=5\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   All items - Accuracy: 100.00% (5/5)\n",
      "\n",
      "Item count mismatch: gold=0, pred=1\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   All items - Accuracy: 50.00% (1/2)\n",
      "\n",
      "Item count mismatch: gold=1, pred=2\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   All items - Accuracy: 66.67% (2/3)\n",
      "   Contributions - Accuracy: 100.00% (1/1)\n",
      "\n",
      "Item count mismatch: gold=5, pred=4\n",
      "Item count mismatch: gold=3, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   All items - Accuracy: 0.00% (0/4)\n",
      "   Contributions - Accuracy: 33.33% (1/3)\n",
      "\n",
      "Item count mismatch: gold=5, pred=4\n",
      "Item count mismatch: gold=3, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "   Contributions - Accuracy: 100.00% (3/3)\n",
      "\n",
      "Item count mismatch: gold=6, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "   Contributions - Accuracy: 100.00% (4/4)\n",
      "\n",
      "Item count mismatch: gold=2, pred=3\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   All items - Accuracy: 100.00% (2/2)\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   All items - Accuracy: 0.00% (0/0)\n",
      "\n",
      "Item count mismatch: gold=7, pred=5\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   All items - Accuracy: 40.00% (2/5)\n",
      "   Contributions - Accuracy: 80.00% (4/5)\n",
      "\n",
      "Item count mismatch: gold=6, pred=4\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   All items - Accuracy: 0.00% (0/4)\n",
      "   Contributions - Accuracy: 100.00% (4/4)\n",
      "\n",
      "Item count mismatch: gold=3, pred=1\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   All items - Accuracy: 0.00% (0/1)\n",
      "   Contributions - Accuracy: 100.00% (1/1)\n",
      "\n",
      "Item count mismatch: gold=8, pred=0\n",
      "Item count mismatch: gold=4, pred=0\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   All items - Accuracy: 0.00% (0/0)\n",
      "\n",
      "Item count mismatch: gold=0, pred=10\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   All items - Accuracy: 9.09% (1/11)\n",
      "\n",
      "Item count mismatch: gold=4, pred=6\n",
      "Item count mismatch: gold=0, pred=1\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION ACCURACY SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Overall Accuracy: 38.78% (19/49)\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Overall Accuracy: 85.71% (18/21)\n",
      "\n",
      "Confusion Matrix (All Items):\n",
      "Gold / Pred    ad          paratext    prose       verse       \n",
      "ad             0           1           0           0           \n",
      "paratext       0           12          20          4           \n",
      "prose          0           0           3           3           \n",
      "verse          0           0           2           4           \n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification(gold_path: Path, pred_path: Path,\n",
    "                           item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate item_class classification accuracy.\n",
    "    \n",
    "    Assumes items are in same order (or uses simple alignment).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with accuracy, per-class metrics, confusion matrix\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    # Simple alignment: assume same number and order\n",
    "    if len(gold_items) != len(pred_items):\n",
    "        print(f\"Item count mismatch: gold={len(gold_items)}, pred={len(pred_items)}\")\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    correct = 0\n",
    "    confusion = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_class = gold_items[i].get('item_class', 'unknown')\n",
    "        pred_class = pred_items[i].get('item_class', 'unknown')\n",
    "        \n",
    "        confusion[gold_class][pred_class] += 1\n",
    "        if gold_class == pred_class:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / min_len if min_len > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'total': min_len,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion': dict(confusion),\n",
    "        'gold_count': len(gold_items),\n",
    "        'pred_count': len(pred_items)\n",
    "    }\n",
    "\n",
    "# Evaluate classification\n",
    "print(\"Evaluating item classification...\\n\")\n",
    "\n",
    "classification_results_all = []\n",
    "classification_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_classification(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    classification_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_classification(gold_path, pred_path,\n",
    "                                            item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    classification_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   All items - Accuracy: {result_all['accuracy']:.2%} \"\n",
    "          f\"({result_all['correct']}/{result_all['total']})\")\n",
    "    if result_contrib['total'] > 0:\n",
    "        print(f\"   Contributions - Accuracy: {result_contrib['accuracy']:.2%} \"\n",
    "              f\"({result_contrib['correct']}/{result_contrib['total']})\")\n",
    "    print()\n",
    "\n",
    "# Compute overall accuracy\n",
    "total_correct_all = sum(r['correct'] for r in classification_results_all)\n",
    "total_items_all = sum(r['total'] for r in classification_results_all)\n",
    "overall_accuracy_all = total_correct_all / total_items_all if total_items_all > 0 else 0\n",
    "\n",
    "total_correct_contrib = sum(r['correct'] for r in classification_results_contrib)\n",
    "total_items_contrib = sum(r['total'] for r in classification_results_contrib)\n",
    "overall_accuracy_contrib = total_correct_contrib / total_items_contrib if total_items_contrib > 0 else 0\n",
    "\n",
    "# Aggregate confusion matrix\n",
    "all_confusion = defaultdict(lambda: defaultdict(int))\n",
    "for result in classification_results_all:\n",
    "    for gold_class, pred_dict in result['confusion'].items():\n",
    "        for pred_class, count in pred_dict.items():\n",
    "            all_confusion[gold_class][pred_class] += count\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CLASSIFICATION ACCURACY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_all:.2%} ({total_correct_all}/{total_items_all})\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_contrib:.2%} ({total_correct_contrib}/{total_items_contrib})\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (All Items):\")\n",
    "print(f\"{'Gold / Pred':<15}\", end=\"\")\n",
    "all_classes = sorted(set(list(all_confusion.keys()) + \n",
    "                        [pred for preds in all_confusion.values() for pred in preds.keys()]))\n",
    "for pred_class in all_classes:\n",
    "    print(f\"{pred_class:<12}\", end=\"\")\n",
    "print()\n",
    "for gold_class in all_classes:\n",
    "    print(f\"{gold_class:<15}\", end=\"\")\n",
    "    for pred_class in all_classes:\n",
    "        count = all_confusion[gold_class][pred_class]\n",
    "        print(f\"{count:<12}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c433e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metadata extraction...\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   Title F1: 1.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   Title F1: 0.667, Author F1: 0.667\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   Title F1: 0.667, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   Title F1: 0.500, Author F1: 0.400\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   Title F1: 0.667, Author F1: 0.571\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   Title F1: 0.500, Author F1: 0.400\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "\n",
      "============================================================\n",
      "METADATA EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Title - P: 43.75%, R: 87.50%, F1: 0.583\n",
      "           Exact matches: 2/7\n",
      "   Author - P: 38.46%, R: 45.45%, F1: 0.417\n",
      "            Exact matches: 0/5\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Title - P: 100.00%, R: 86.67%, F1: 0.929\n",
      "   Author - P: 92.31%, R: 85.71%, F1: 0.889\n"
     ]
    }
   ],
   "source": [
    "def evaluate_metadata(gold_path: Path, pred_path: Path,\n",
    "                     item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate title and author extraction accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with title/author presence detection and exact match metrics\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    title_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    author_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Title evaluation\n",
    "        gold_title = gold_item.get('item_title')\n",
    "        pred_title = pred_item.get('item_title')\n",
    "        \n",
    "        if gold_title and pred_title:\n",
    "            title_metrics['tp'] += 1\n",
    "            if gold_title == pred_title:\n",
    "                title_metrics['exact_match'] += 1\n",
    "        elif not gold_title and pred_title:\n",
    "            title_metrics['fp'] += 1\n",
    "        elif gold_title and not pred_title:\n",
    "            title_metrics['fn'] += 1\n",
    "        \n",
    "        # Author evaluation\n",
    "        gold_author = gold_item.get('item_author')\n",
    "        pred_author = pred_item.get('item_author')\n",
    "        \n",
    "        if gold_author and pred_author:\n",
    "            author_metrics['tp'] += 1\n",
    "            if gold_author == pred_author:\n",
    "                author_metrics['exact_match'] += 1\n",
    "        elif not gold_author and pred_author:\n",
    "            author_metrics['fp'] += 1\n",
    "        elif gold_author and not pred_author:\n",
    "            author_metrics['fn'] += 1\n",
    "    \n",
    "    # Compute F1 for title\n",
    "    title_p = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fp']) if (title_metrics['tp'] + title_metrics['fp']) > 0 else 0\n",
    "    title_r = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fn']) if (title_metrics['tp'] + title_metrics['fn']) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    # Compute F1 for author\n",
    "    author_p = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fp']) if (author_metrics['tp'] + author_metrics['fp']) > 0 else 0\n",
    "    author_r = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fn']) if (author_metrics['tp'] + author_metrics['fn']) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {\n",
    "            **title_metrics,\n",
    "            'precision': title_p,\n",
    "            'recall': title_r,\n",
    "            'f1': title_f1\n",
    "        },\n",
    "        'author': {\n",
    "            **author_metrics,\n",
    "            'precision': author_p,\n",
    "            'recall': author_r,\n",
    "            'f1': author_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate metadata extraction\n",
    "print(\"Evaluating metadata extraction...\\n\")\n",
    "\n",
    "metadata_results_all = []\n",
    "metadata_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_metadata(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    metadata_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_metadata(gold_path, pred_path,\n",
    "                                      item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    metadata_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   Title F1: {result_all['title']['f1']:.3f}, \"\n",
    "          f\"Author F1: {result_all['author']['f1']:.3f}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "def aggregate_metadata_metrics(results):\n",
    "    total_title_tp = sum(r['title']['tp'] for r in results)\n",
    "    total_title_fp = sum(r['title']['fp'] for r in results)\n",
    "    total_title_fn = sum(r['title']['fn'] for r in results)\n",
    "    total_title_exact = sum(r['title']['exact_match'] for r in results)\n",
    "    \n",
    "    title_p = total_title_tp / (total_title_tp + total_title_fp) if (total_title_tp + total_title_fp) > 0 else 0\n",
    "    title_r = total_title_tp / (total_title_tp + total_title_fn) if (total_title_tp + total_title_fn) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    total_author_tp = sum(r['author']['tp'] for r in results)\n",
    "    total_author_fp = sum(r['author']['fp'] for r in results)\n",
    "    total_author_fn = sum(r['author']['fn'] for r in results)\n",
    "    total_author_exact = sum(r['author']['exact_match'] for r in results)\n",
    "    \n",
    "    author_p = total_author_tp / (total_author_tp + total_author_fp) if (total_author_tp + total_author_fp) > 0 else 0\n",
    "    author_r = total_author_tp / (total_author_tp + total_author_fn) if (total_author_tp + total_author_fn) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {'precision': title_p, 'recall': title_r, 'f1': title_f1, 'exact_match': total_title_exact, 'tp': total_title_tp},\n",
    "        'author': {'precision': author_p, 'recall': author_r, 'f1': author_f1, 'exact_match': total_author_exact, 'tp': total_author_tp}\n",
    "    }\n",
    "\n",
    "agg_all = aggregate_metadata_metrics(metadata_results_all)\n",
    "agg_contrib = aggregate_metadata_metrics(metadata_results_contrib)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METADATA EXTRACTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Title - P: {agg_all['title']['precision']:.2%}, R: {agg_all['title']['recall']:.2%}, F1: {agg_all['title']['f1']:.3f}\")\n",
    "print(f\"           Exact matches: {agg_all['title']['exact_match']}/{agg_all['title']['tp']}\")\n",
    "print(f\"   Author - P: {agg_all['author']['precision']:.2%}, R: {agg_all['author']['recall']:.2%}, F1: {agg_all['author']['f1']:.3f}\")\n",
    "print(f\"            Exact matches: {agg_all['author']['exact_match']}/{agg_all['author']['tp']}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Title - P: {agg_contrib['title']['precision']:.2%}, R: {agg_contrib['title']['recall']:.2%}, F1: {agg_contrib['title']['f1']:.3f}\")\n",
    "print(f\"   Author - P: {agg_contrib['author']['precision']:.2%}, R: {agg_contrib['author']['recall']:.2%}, F1: {agg_contrib['author']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838969fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating continuation tracking...\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   is_continuation - F1: 1.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   is_continuation - F1: 1.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "âœ“ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "============================================================\n",
      "CONTINUATION TRACKING SUMMARY (Contributions Only)\n",
      "============================================================\n",
      "\n",
      "is_continuation:\n",
      "   Precision: 100.00%\n",
      "   Recall: 33.33%\n",
      "   F1: 0.500\n",
      "\n",
      "continues_on_next_page:\n",
      "   Precision: 100.00%\n",
      "   Recall: 71.43%\n",
      "   F1: 0.833\n"
     ]
    }
   ],
   "source": [
    "def evaluate_continuation_tracking(gold_path: Path, pred_path: Path,\n",
    "                                  item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy (is_continuation, continues_on_next_page).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1 for each continuation field\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    # Metrics for is_continuation\n",
    "    is_cont_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    # Metrics for continues_on_next_page\n",
    "    continues_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Evaluate is_continuation (treat absent as False)\n",
    "        gold_is_cont = gold_item.get('is_continuation', False)\n",
    "        pred_is_cont = pred_item.get('is_continuation', False)\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['tp'] += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['fp'] += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_metrics['fn'] += 1\n",
    "        else:\n",
    "            is_cont_metrics['tn'] += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page', False)\n",
    "        pred_continues = pred_item.get('continues_on_next_page', False)\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_metrics['tp'] += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_metrics['fp'] += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_metrics['fn'] += 1\n",
    "        else:\n",
    "            continues_metrics['tn'] += 1\n",
    "    \n",
    "    # Compute metrics for is_continuation\n",
    "    is_cont_p = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fp']) if (is_cont_metrics['tp'] + is_cont_metrics['fp']) > 0 else 0\n",
    "    is_cont_r = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fn']) if (is_cont_metrics['tp'] + is_cont_metrics['fn']) > 0 else 0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "    \n",
    "    # Compute metrics for continues_on_next_page\n",
    "    continues_p = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fp']) if (continues_metrics['tp'] + continues_metrics['fp']) > 0 else 0\n",
    "    continues_r = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fn']) if (continues_metrics['tp'] + continues_metrics['fn']) > 0 else 0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            **is_cont_metrics,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            **continues_metrics,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate continuation tracking\n",
    "print(\"Evaluating continuation tracking...\\n\")\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    result = evaluate_continuation_tracking(gold_path, pred_path,\n",
    "                                           item_classes=['prose', 'verse'])\n",
    "    result['page'] = gold_path.name\n",
    "    continuation_results.append(result)\n",
    "    \n",
    "    print(f\"âœ“ {gold_path.name}\")\n",
    "    print(f\"   is_continuation - F1: {result['is_continuation']['f1']:.3f}\")\n",
    "    print(f\"   continues_on_next - F1: {result['continues_on_next_page']['f1']:.3f}\\n\")\n",
    "\n",
    "# Aggregate continuation metrics\n",
    "total_is_cont_tp = sum(r['is_continuation']['tp'] for r in continuation_results)\n",
    "total_is_cont_fp = sum(r['is_continuation']['fp'] for r in continuation_results)\n",
    "total_is_cont_fn = sum(r['is_continuation']['fn'] for r in continuation_results)\n",
    "\n",
    "is_cont_p = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fp) if (total_is_cont_tp + total_is_cont_fp) > 0 else 0\n",
    "is_cont_r = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fn) if (total_is_cont_tp + total_is_cont_fn) > 0 else 0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "\n",
    "total_continues_tp = sum(r['continues_on_next_page']['tp'] for r in continuation_results)\n",
    "total_continues_fp = sum(r['continues_on_next_page']['fp'] for r in continuation_results)\n",
    "total_continues_fn = sum(r['continues_on_next_page']['fn'] for r in continuation_results)\n",
    "\n",
    "continues_p = total_continues_tp / (total_continues_tp + total_continues_fp) if (total_continues_tp + total_continues_fp) > 0 else 0\n",
    "continues_r = total_continues_tp / (total_continues_tp + total_continues_fn) if (total_continues_tp + total_continues_fn) > 0 else 0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CONTINUATION TRACKING SUMMARY (Contributions Only)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nis_continuation:\")\n",
    "print(f\"   Precision: {is_cont_p:.2%}\")\n",
    "print(f\"   Recall: {is_cont_r:.2%}\")\n",
    "print(f\"   F1: {is_cont_f1:.3f}\")\n",
    "print(f\"\\ncontinues_on_next_page:\")\n",
    "print(f\"   Precision: {continues_p:.2%}\")\n",
    "print(f\"   Recall: {continues_r:.2%}\")\n",
    "print(f\"   F1: {continues_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EVALUATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Second_try_revised vs Gold Standard\n",
      "Evaluated on 14 pages\n",
      "\n",
      "Metric                         All Items            Contributions       \n",
      "----------------------------------------------------------------------\n",
      "TEXT QUALITY                  \n",
      "  Character Error Rate                     12.63%             31.65%\n",
      "  Word Error Rate                          17.82%             34.04%\n",
      "\n",
      "STRUCTURE QUALITY             \n",
      "  Boundary Detection F1                     0.426              0.459\n",
      "  Classification Accuracy                  38.78%             85.71%\n",
      "\n",
      "METADATA EXTRACTION           \n",
      "  Title F1                                  0.583              0.929\n",
      "  Author F1                                 0.417              0.889\n",
      "\n",
      "CONTINUATION TRACKING          N/A                  Contributions       \n",
      "  is_continuation F1                                             0.500\n",
      "  continues_on_next F1                                           0.833\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSecond_try_revised vs Gold Standard\")\n",
    "print(f\"Evaluated on {len(page_pairs)} pages\\n\")\n",
    "\n",
    "print(f\"{'Metric':<30} {'All Items':<20} {'Contributions':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Text Quality\n",
    "print(f\"{'TEXT QUALITY':<30}\")\n",
    "print(f\"{'  Character Error Rate':<30} {avg_cer_all:>18.2%} {avg_cer_contrib:>18.2%}\")\n",
    "print(f\"{'  Word Error Rate':<30} {avg_wer_all:>18.2%} {avg_wer_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Structure Quality\n",
    "print(f\"{'STRUCTURE QUALITY':<30}\")\n",
    "print(f\"{'  Boundary Detection F1':<30} {f1_all:>18.3f} {f1_contrib:>18.3f}\")\n",
    "print(f\"{'  Classification Accuracy':<30} {overall_accuracy_all:>18.2%} {overall_accuracy_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Metadata Quality\n",
    "print(f\"{'METADATA EXTRACTION':<30}\")\n",
    "print(f\"{'  Title F1':<30} {agg_all['title']['f1']:>18.3f} {agg_contrib['title']['f1']:>18.3f}\")\n",
    "print(f\"{'  Author F1':<30} {agg_all['author']['f1']:>18.3f} {agg_contrib['author']['f1']:>18.3f}\")\n",
    "print()\n",
    "\n",
    "# Continuation Tracking\n",
    "print(f\"{'CONTINUATION TRACKING':<30} {'N/A':<20} {'Contributions':<20}\")\n",
    "print(f\"{'  is_continuation F1':<30} {'':<20} {is_cont_f1:>18.3f}\")\n",
    "print(f\"{'  continues_on_next F1':<30} {'':<20} {continues_f1:>18.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94687802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONTINUATION FIELD ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“š GOLD STANDARD:\n",
      "  Files processed: 14\n",
      "  Total items: 70\n",
      "\n",
      "  is_continuation:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   63\n",
      "    Absent: 0\n",
      "\n",
      "  continues_on_next_page:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   63\n",
      "    Absent: 0\n",
      "\n",
      "\n",
      "ðŸ¤– PREDICTIONS:\n",
      "  Files processed: 14\n",
      "  Total items: 52\n",
      "\n",
      "  is_continuation:\n",
      "    True:   3\n",
      "    False:  0\n",
      "    Null:   49\n",
      "    Absent: 0\n",
      "\n",
      "  continues_on_next_page:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   45\n",
      "    Absent: 0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "\n",
      "is_continuation=True:\n",
      "  Gold has: 7\n",
      "  Pred has: 3\n",
      "  Detection rate: 3/7 = 42.9%\n",
      "\n",
      "continues_on_next_page=True:\n",
      "  Gold has: 7\n",
      "  Pred has: 7\n",
      "  Detection rate: 7/7 = 100.0%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/fabian-ramirez/Documents/These/Code/magazine_graphs\")\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "def analyze_continuation_fields(directory, label):\n",
    "    \"\"\"Count continuation field usage across all files.\"\"\"\n",
    "    stats = {\n",
    "        'is_continuation': {'true': 0, 'false': 0, 'null': 0, 'absent': 0},\n",
    "        'continues_on_next_page': {'true': 0, 'false': 0, 'null': 0, 'absent': 0},\n",
    "        'total_items': 0,\n",
    "        'files_processed': 0\n",
    "    }\n",
    "    \n",
    "    for json_file in sorted(directory.glob(\"*.json\")):\n",
    "        try:\n",
    "            data = json.loads(json_file.read_text(encoding='utf-8'))\n",
    "            items = data.get('items', [])\n",
    "            stats['files_processed'] += 1\n",
    "            \n",
    "            for item in items:\n",
    "                stats['total_items'] += 1\n",
    "                \n",
    "                # Check is_continuation\n",
    "                is_cont = item.get('is_continuation')\n",
    "                if is_cont is True:\n",
    "                    stats['is_continuation']['true'] += 1\n",
    "                elif is_cont is False:\n",
    "                    stats['is_continuation']['false'] += 1\n",
    "                elif is_cont is None:\n",
    "                    stats['is_continuation']['null'] += 1\n",
    "                else:  # key not present\n",
    "                    stats['is_continuation']['absent'] += 1\n",
    "                \n",
    "                # Check continues_on_next_page\n",
    "                continues = item.get('continues_on_next_page')\n",
    "                if continues is True:\n",
    "                    stats['continues_on_next_page']['true'] += 1\n",
    "                elif continues is False:\n",
    "                    stats['continues_on_next_page']['false'] += 1\n",
    "                elif continues is None:\n",
    "                    stats['continues_on_next_page']['null'] += 1\n",
    "                else:  # key not present\n",
    "                    stats['continues_on_next_page']['absent'] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file.name}: {e}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTINUATION FIELD ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze gold standard\n",
    "print(\"\\nðŸ“š GOLD STANDARD:\")\n",
    "gold_stats = analyze_continuation_fields(GOLD_DIR, \"Gold\")\n",
    "print(f\"  Files processed: {gold_stats['files_processed']}\")\n",
    "print(f\"  Total items: {gold_stats['total_items']}\")\n",
    "print(f\"\\n  is_continuation:\")\n",
    "print(f\"    True:   {gold_stats['is_continuation']['true']}\")\n",
    "print(f\"    False:  {gold_stats['is_continuation']['false']}\")\n",
    "print(f\"    Null:   {gold_stats['is_continuation']['null']}\")\n",
    "print(f\"    Absent: {gold_stats['is_continuation']['absent']}\")\n",
    "print(f\"\\n  continues_on_next_page:\")\n",
    "print(f\"    True:   {gold_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"    False:  {gold_stats['continues_on_next_page']['false']}\")\n",
    "print(f\"    Null:   {gold_stats['continues_on_next_page']['null']}\")\n",
    "print(f\"    Absent: {gold_stats['continues_on_next_page']['absent']}\")\n",
    "\n",
    "# Analyze predictions\n",
    "print(\"\\n\\nðŸ¤– PREDICTIONS:\")\n",
    "pred_stats = analyze_continuation_fields(PRED_DIR, \"Predictions\")\n",
    "print(f\"  Files processed: {pred_stats['files_processed']}\")\n",
    "print(f\"  Total items: {pred_stats['total_items']}\")\n",
    "print(f\"\\n  is_continuation:\")\n",
    "print(f\"    True:   {pred_stats['is_continuation']['true']}\")\n",
    "print(f\"    False:  {pred_stats['is_continuation']['false']}\")\n",
    "print(f\"    Null:   {pred_stats['is_continuation']['null']}\")\n",
    "print(f\"    Absent: {pred_stats['is_continuation']['absent']}\")\n",
    "print(f\"\\n  continues_on_next_page:\")\n",
    "print(f\"    True:   {pred_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"    False:  {pred_stats['continues_on_next_page']['false']}\")\n",
    "print(f\"    Null:   {pred_stats['continues_on_next_page']['null']}\")\n",
    "print(f\"    Absent: {pred_stats['continues_on_next_page']['absent']}\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nis_continuation=True:\")\n",
    "print(f\"  Gold has: {gold_stats['is_continuation']['true']}\")\n",
    "print(f\"  Pred has: {pred_stats['is_continuation']['true']}\")\n",
    "print(f\"  Detection rate: {pred_stats['is_continuation']['true']}/{gold_stats['is_continuation']['true']} = \"\n",
    "      f\"{pred_stats['is_continuation']['true']/gold_stats['is_continuation']['true']*100:.1f}%\" \n",
    "      if gold_stats['is_continuation']['true'] > 0 else \"  N/A\")\n",
    "\n",
    "print(f\"\\ncontinues_on_next_page=True:\")\n",
    "print(f\"  Gold has: {gold_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"  Pred has: {pred_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"  Detection rate: {pred_stats['continues_on_next_page']['true']}/{gold_stats['continues_on_next_page']['true']} = \"\n",
    "      f\"{pred_stats['continues_on_next_page']['true']/gold_stats['continues_on_next_page']['true']*100:.1f}%\"\n",
    "      if gold_stats['continues_on_next_page']['true'] > 0 else \"  N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e01889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Worst 5 Pages by Character Error Rate:\n",
      "1. La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   CER: 100.00%, WER: 100.00%\n",
      "   Gold: 8 items, 3634 chars\n",
      "   Pred: 0 items, 0 chars\n",
      "\n",
      "2. La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   CER: 21.48%, WER: 27.55%\n",
      "   Gold: 5 items, 4745 chars\n",
      "   Pred: 4 items, 4648 chars\n",
      "\n",
      "3. La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   CER: 14.49%, WER: 18.18%\n",
      "   Gold: 2 items, 69 chars\n",
      "   Pred: 3 items, 69 chars\n",
      "\n",
      "4. La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   CER: 9.63%, WER: 12.56%\n",
      "   Gold: 3 items, 5236 chars\n",
      "   Pred: 1 items, 5174 chars\n",
      "\n",
      "5. La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   CER: 8.12%, WER: 10.99%\n",
      "   Gold: 4 items, 1158 chars\n",
      "   Pred: 6 items, 1190 chars\n",
      "\n",
      "\n",
      "Pages with Item Count Mismatches:\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-001.json: Gold=8, Pred=5 (-3)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-004.json: Gold=5, Pred=4 (-1)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-005.json: Gold=5, Pred=4 (-1)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-006.json: Gold=6, Pred=4 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-007.json: Gold=2, Pred=3 (+1)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-009.json: Gold=7, Pred=5 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-010.json: Gold=6, Pred=4 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-011.json: Gold=3, Pred=1 (-2)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-012.json: Gold=8, Pred=0 (-8)\n",
      "  â€¢ La_Plume_bpt6k1185893k_1_10_1889__page-014.json: Gold=4, Pred=6 (+2)\n",
      "\n",
      "Most Common Classification Errors:\n",
      "  â€¢ paratext â†’ prose: 20 times\n",
      "  â€¢ paratext â†’ verse: 4 times\n",
      "  â€¢ prose â†’ verse: 3 times\n",
      "  â€¢ verse â†’ prose: 2 times\n",
      "  â€¢ ad â†’ paratext: 1 times\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find worst performing pages by CER\n",
    "worst_pages_cer = sorted(all_results, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nWorst 5 Pages by Character Error Rate:\")\n",
    "for i, result in enumerate(worst_pages_cer, 1):\n",
    "    print(f\"{i}. {result['page']}\")\n",
    "    print(f\"   CER: {result['cer']:.2%}, WER: {result['wer']:.2%}\")\n",
    "    print(f\"   Gold: {result['gold_items']} items, {result['gold_chars']} chars\")\n",
    "    print(f\"   Pred: {result['pred_items']} items, {result['pred_chars']} chars\")\n",
    "    print()\n",
    "\n",
    "# Find pages with item count mismatches\n",
    "print(\"\\nPages with Item Count Mismatches:\")\n",
    "mismatches = [r for r in classification_results_all if r['gold_count'] != r['pred_count']]\n",
    "if mismatches:\n",
    "    for result in mismatches:\n",
    "        diff = result['pred_count'] - result['gold_count']\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"  â€¢ {result['page']}: Gold={result['gold_count']}, Pred={result['pred_count']} ({sign}{diff})\")\n",
    "else:\n",
    "    print(\"  No mismatches found!\")\n",
    "\n",
    "# Classification errors\n",
    "print(\"\\nMost Common Classification Errors:\")\n",
    "errors = []\n",
    "for gold_class, pred_dict in all_confusion.items():\n",
    "    for pred_class, count in pred_dict.items():\n",
    "        if gold_class != pred_class and count > 0:\n",
    "            errors.append((count, gold_class, pred_class))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, gold_class, pred_class in errors[:10]:\n",
    "    print(f\"  â€¢ {gold_class} â†’ {pred_class}: {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aebfe96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_gold_standard_names():\n",
    "#     \"\"\"\n",
    "#     Rename gold standard files to match the PDF-based naming convention.\n",
    "#     Old: La_Plume___revue_littÃ©raire_[...]_bpt6k1185893k__page-001.json\n",
    "#     New: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "    \n",
    "#     # The standard name from your PDF\n",
    "#     STANDARD_BASE = \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "    \n",
    "#     for old_path in GOLD_DIR.glob(\"*.json\"):\n",
    "#         # Extract page number\n",
    "#         match = re.search(r'page-(\\d+)\\.json$', old_path.name)\n",
    "#         if not match:\n",
    "#             print(f\"âš ï¸  Skipping (no page number): {old_path.name}\")\n",
    "#             continue\n",
    "        \n",
    "#         page_num = match.group(1)\n",
    "#         new_name = f\"{STANDARD_BASE}__page-{page_num}.json\"\n",
    "#         new_path = old_path.parent / new_name\n",
    "        \n",
    "#         if old_path.name != new_name:\n",
    "#             print(f\"Renaming: {old_path.name}\")\n",
    "#             print(f\"      â†’  {new_name}\")\n",
    "#             old_path.rename(new_path)\n",
    "    \n",
    "#     print(\"\\nâœ“ Gold standard filenames standardized!\")\n",
    "\n",
    "# # Uncomment to run:\n",
    "# standardize_gold_standard_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
