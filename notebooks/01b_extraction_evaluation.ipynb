{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Evaluation\n",
      "\n",
      "\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "Predictions: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/interim_pages/La_Plume_bpt6k1185893k_1_10_1889\n",
      "\n",
      "Dataset:\n",
      "  Gold files: 14\n",
      "  Pred files: 14\n",
      "  Matching pairs: 14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "\n",
    "# Path setup\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import schemas for validation\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "\n",
    "# Paths\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "print(\"Stage 1 OCR Evaluation\")\n",
    "print(\"\\n\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Gold standard: {GOLD_DIR}\")\n",
    "print(f\"Predictions: {PRED_DIR}\")\n",
    "\n",
    "# Find common files\n",
    "def load_page_pairs() -> List[Tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Match gold standard files with prediction files by filename.\n",
    "    Returns list of (gold_path, pred_path) tuples.\n",
    "    \"\"\"\n",
    "    gold_files = {f.name: f for f in GOLD_DIR.glob(\"*.json\")}\n",
    "    pred_files = {f.name: f for f in PRED_DIR.glob(\"*.json\")}\n",
    "    \n",
    "    common_names = set(gold_files.keys()) & set(pred_files.keys())\n",
    "    \n",
    "    pairs = [(gold_files[name], pred_files[name]) for name in sorted(common_names)]\n",
    "    \n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Gold files: {len(gold_files)}\")\n",
    "    print(f\"  Pred files: {len(pred_files)}\")\n",
    "    print(f\"  Matching pairs: {len(pairs)}\")\n",
    "    \n",
    "    if len(pairs) < len(gold_files):\n",
    "        missing = set(gold_files.keys()) - set(pred_files.keys())\n",
    "        print(f\"Warning: {len(missing)} gold standard pages without predictions:\")\n",
    "        for name in sorted(missing):\n",
    "            print(f\"   - {name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "page_pairs = load_page_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd2a9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Item Matching Configuration\n",
      "\n",
      "\n",
      "Similarity threshold: 0.7\n",
      "\n",
      "\n",
      "Item Matching Test\n",
      "\n",
      "\n",
      "\n",
      "Test page: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Gold items: 8\n",
      "  Pred items: 5\n",
      "  Matches found: 4\n",
      "  Unmatched gold: 4\n",
      "  Unmatched pred: 1\n",
      "  Average match quality: 92.47%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Match gold items to predicted items using content-based text similarity.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_THRESHOLD = 0.7  # Minimum text similarity to consider a match (0.0-1.0)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Configuration\")\n",
    "print(\"\\n\")\n",
    "print(f\"Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "        \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item, find the best-matching unmatched pred item.\n",
    "        Accept the match if similarity exceeds threshold.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair and match items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "\n",
    "# Test matching on first page\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Test\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if page_pairs:\n",
    "    test_gold, test_pred = page_pairs[0]\n",
    "    test_result = load_and_match_page(test_gold, test_pred)\n",
    "    \n",
    "    print(f\"\\nTest page: {test_result['page_name']}\")\n",
    "    print(f\"  Gold items: {len(test_result['gold_items'])}\")\n",
    "    print(f\"  Pred items: {len(test_result['pred_items'])}\")\n",
    "    print(f\"  Matches found: {len(test_result['matches'])}\")\n",
    "    print(f\"  Unmatched gold: {len(test_result['unmatched_gold'])}\")\n",
    "    print(f\"  Unmatched pred: {len(test_result['unmatched_pred'])}\")\n",
    "    \n",
    "    if test_result['matches']:\n",
    "        avg_score = sum(score for _, _, score in test_result['matches']) / len(test_result['matches'])\n",
    "        print(f\"  Average match quality: {avg_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text quality...\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   All items - CER: 4.04%, WER: 6.28%\n",
      "   Contributions - CER: 0.00%, WER: 0.00%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   All items - CER: 0.61%, WER: 0.25%\n",
      "   Contributions - CER: 100.00%, WER: 100.00%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   All items - CER: 1.22%, WER: 3.34%\n",
      "   Contributions - CER: 1.21%, WER: 3.25%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   All items - CER: 2.94%, WER: 7.67%\n",
      "   Contributions - CER: 2.68%, WER: 7.35%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   All items - CER: 21.48%, WER: 27.55%\n",
      "   Contributions - CER: 21.24%, WER: 27.27%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   All items - CER: 2.94%, WER: 6.84%\n",
      "   Contributions - CER: 2.64%, WER: 6.48%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   All items - CER: 14.49%, WER: 18.18%\n",
      "   Contributions - CER: 0.00%, WER: 0.00%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   All items - CER: 0.00%, WER: 0.00%\n",
      "   Contributions - CER: 0.00%, WER: 0.00%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   All items - CER: 3.96%, WER: 6.50%\n",
      "   Contributions - CER: 3.58%, WER: 6.03%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   All items - CER: 2.58%, WER: 14.24%\n",
      "   Contributions - CER: 2.32%, WER: 13.95%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   All items - CER: 9.63%, WER: 12.56%\n",
      "   Contributions - CER: 9.38%, WER: 12.26%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   All items - CER: 100.00%, WER: 100.00%\n",
      "   Contributions - CER: 100.00%, WER: 100.00%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   All items - CER: 4.77%, WER: 35.02%\n",
      "   Contributions - CER: 100.00%, WER: 100.00%\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   All items - CER: 8.12%, WER: 10.99%\n",
      "   Contributions - CER: 100.00%, WER: 100.00%\n",
      "\n",
      "============================================================\n",
      "TEXT QUALITY SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Average CER: 12.63%\n",
      "   Average WER: 17.82%\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Average CER: 31.65%\n",
      "   Average WER: 34.04%\n"
     ]
    }
   ],
   "source": [
    "def character_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate Character Error Rate using Levenshtein distance.\"\"\"\n",
    "    if not reference:\n",
    "        return 1.0 if hypothesis else 0.0\n",
    "    distance = Levenshtein.distance(reference, hypothesis)\n",
    "    return distance / len(reference)\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate Word Error Rate using Levenshtein distance on words.\"\"\"\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    if not ref_words:\n",
    "        return 1.0 if hyp_words else 0.0\n",
    "    distance = Levenshtein.distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "def evaluate_text_quality(gold_path: Path, pred_path: Path, \n",
    "                         item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare text quality between gold and prediction.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        item_classes: If provided, only evaluate these item classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER, and per-item metrics\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    # Extract text blocks\n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    # Flatten to text\n",
    "    gold_text = \"\\n\\n\".join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = \"\\n\\n\".join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    cer = character_error_rate(gold_text, pred_text)\n",
    "    wer = word_error_rate(gold_text, pred_text)\n",
    "    \n",
    "    return {\n",
    "        'cer': cer,\n",
    "        'wer': wer,\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split()),\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items)\n",
    "    }\n",
    "\n",
    "# Evaluate text quality for all pages\n",
    "print(\"Evaluating text quality...\\n\")\n",
    "\n",
    "all_results = []\n",
    "contributions_results = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_text_quality(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    all_results.append(result_all)\n",
    "    \n",
    "    # Contributions only (prose + verse)\n",
    "    result_contrib = evaluate_text_quality(gold_path, pred_path, \n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    contributions_results.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   All items - CER: {result_all['cer']:.2%}, WER: {result_all['wer']:.2%}\")\n",
    "    print(f\"   Contributions - CER: {result_contrib['cer']:.2%}, WER: {result_contrib['wer']:.2%}\\n\")\n",
    "\n",
    "# Compute averages\n",
    "avg_cer_all = sum(r['cer'] for r in all_results) / len(all_results) if all_results else 0\n",
    "avg_wer_all = sum(r['wer'] for r in all_results) / len(all_results) if all_results else 0\n",
    "avg_cer_contrib = sum(r['cer'] for r in contributions_results) / len(contributions_results) if contributions_results else 0\n",
    "avg_wer_contrib = sum(r['wer'] for r in contributions_results) / len(contributions_results) if contributions_results else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"TEXT QUALITY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Average CER: {avg_cer_all:.2%}\")\n",
    "print(f\"   Average WER: {avg_wer_all:.2%}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Average CER: {avg_cer_contrib:.2%}\")\n",
    "print(f\"   Average WER: {avg_wer_contrib:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e807f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item boundary detection...\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   All items - P: 40.00%, R: 25.00%, F1: 0.308\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   All items - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   All items - P: 66.67%, R: 66.67%, F1: 0.667\n",
      "   Contributions - P: 50.00%, R: 100.00%, F1: 0.667\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   All items - P: 75.00%, R: 60.00%, F1: 0.667\n",
      "   Contributions - P: 75.00%, R: 100.00%, F1: 0.857\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   All items - P: 25.00%, R: 20.00%, F1: 0.222\n",
      "   Contributions - P: 25.00%, R: 33.33%, F1: 0.286\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   All items - P: 50.00%, R: 33.33%, F1: 0.400\n",
      "   Contributions - P: 50.00%, R: 50.00%, F1: 0.500\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   All items - P: 100.00%, R: 150.00%, F1: 1.200\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   All items - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   All items - P: 100.00%, R: 71.43%, F1: 0.833\n",
      "   Contributions - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   All items - P: 25.00%, R: 16.67%, F1: 0.200\n",
      "   Contributions - P: 25.00%, R: 25.00%, F1: 0.250\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   All items - P: 100.00%, R: 33.33%, F1: 0.500\n",
      "   Contributions - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   All items - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   All items - P: 27.27%, R: 27.27%, F1: 0.273\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   All items - P: 16.67%, R: 25.00%, F1: 0.200\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "============================================================\n",
      "ITEM BOUNDARY DETECTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Precision: 50.00%\n",
      "   Recall: 37.14%\n",
      "   F1: 0.426\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Precision: 38.89%\n",
      "   Recall: 56.00%\n",
      "   F1: 0.459\n"
     ]
    }
   ],
   "source": [
    "def evaluate_item_boundaries(gold_path: Path, pred_path: Path, \n",
    "                            item_classes: Optional[List[str]] = None,\n",
    "                            tolerance: int = 20) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate whether item boundaries are correctly detected.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        item_classes: If provided, only evaluate these item classes\n",
    "        tolerance: Character tolerance window for boundary matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    def get_boundaries(items, filter_classes=None):\n",
    "        \"\"\"Get character positions where items start.\"\"\"\n",
    "        boundaries = []\n",
    "        pos = 0\n",
    "        for item in items:\n",
    "            if filter_classes is None or item.get('item_class') in filter_classes:\n",
    "                boundaries.append(pos)\n",
    "            # Add text length + separator\n",
    "            pos += len(item.get('item_text_raw', '')) + 2\n",
    "        return set(boundaries)\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    gold_bounds = get_boundaries(gold_items, item_classes)\n",
    "    pred_bounds = get_boundaries(pred_items, item_classes)\n",
    "    \n",
    "    # Match boundaries within tolerance\n",
    "    tp = 0\n",
    "    for pred_b in pred_bounds:\n",
    "        if any(abs(pred_b - gold_b) <= tolerance for gold_b in gold_bounds):\n",
    "            tp += 1\n",
    "    \n",
    "    fp = len(pred_bounds) - tp\n",
    "    fn = len(gold_bounds) - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'gold_boundaries': len(gold_bounds),\n",
    "        'pred_boundaries': len(pred_bounds)\n",
    "    }\n",
    "\n",
    "# Evaluate item boundaries\n",
    "print(\"Evaluating item boundary detection...\\n\")\n",
    "\n",
    "boundary_results_all = []\n",
    "boundary_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_item_boundaries(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    boundary_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_item_boundaries(gold_path, pred_path,\n",
    "                                             item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    boundary_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   All items - P: {result_all['precision']:.2%}, \"\n",
    "          f\"R: {result_all['recall']:.2%}, F1: {result_all['f1']:.3f}\")\n",
    "    print(f\"   Contributions - P: {result_contrib['precision']:.2%}, \"\n",
    "          f\"R: {result_contrib['recall']:.2%}, F1: {result_contrib['f1']:.3f}\\n\")\n",
    "\n",
    "# Compute micro-averages (sum all TP/FP/FN, then compute metrics)\n",
    "total_tp_all = sum(r['tp'] for r in boundary_results_all)\n",
    "total_fp_all = sum(r['fp'] for r in boundary_results_all)\n",
    "total_fn_all = sum(r['fn'] for r in boundary_results_all)\n",
    "\n",
    "precision_all = total_tp_all / (total_tp_all + total_fp_all) if (total_tp_all + total_fp_all) > 0 else 0\n",
    "recall_all = total_tp_all / (total_tp_all + total_fn_all) if (total_tp_all + total_fn_all) > 0 else 0\n",
    "f1_all = 2 * precision_all * recall_all / (precision_all + recall_all) if (precision_all + recall_all) > 0 else 0\n",
    "\n",
    "total_tp_contrib = sum(r['tp'] for r in boundary_results_contrib)\n",
    "total_fp_contrib = sum(r['fp'] for r in boundary_results_contrib)\n",
    "total_fn_contrib = sum(r['fn'] for r in boundary_results_contrib)\n",
    "\n",
    "precision_contrib = total_tp_contrib / (total_tp_contrib + total_fp_contrib) if (total_tp_contrib + total_fp_contrib) > 0 else 0\n",
    "recall_contrib = total_tp_contrib / (total_tp_contrib + total_fn_contrib) if (total_tp_contrib + total_fn_contrib) > 0 else 0\n",
    "f1_contrib = 2 * precision_contrib * recall_contrib / (precision_contrib + recall_contrib) if (precision_contrib + recall_contrib) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ITEM BOUNDARY DETECTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Precision: {precision_all:.2%}\")\n",
    "print(f\"   Recall: {recall_all:.2%}\")\n",
    "print(f\"   F1: {f1_all:.3f}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Precision: {precision_contrib:.2%}\")\n",
    "print(f\"   Recall: {recall_contrib:.2%}\")\n",
    "print(f\"   F1: {f1_contrib:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item classification...\n",
      "\n",
      "Item count mismatch: gold=8, pred=5\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   All items - Accuracy: 100.00% (5/5)\n",
      "\n",
      "Item count mismatch: gold=0, pred=1\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   All items - Accuracy: 50.00% (1/2)\n",
      "\n",
      "Item count mismatch: gold=1, pred=2\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   All items - Accuracy: 66.67% (2/3)\n",
      "   Contributions - Accuracy: 100.00% (1/1)\n",
      "\n",
      "Item count mismatch: gold=5, pred=4\n",
      "Item count mismatch: gold=3, pred=4\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   All items - Accuracy: 0.00% (0/4)\n",
      "   Contributions - Accuracy: 33.33% (1/3)\n",
      "\n",
      "Item count mismatch: gold=5, pred=4\n",
      "Item count mismatch: gold=3, pred=4\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "   Contributions - Accuracy: 100.00% (3/3)\n",
      "\n",
      "Item count mismatch: gold=6, pred=4\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "   Contributions - Accuracy: 100.00% (4/4)\n",
      "\n",
      "Item count mismatch: gold=2, pred=3\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   All items - Accuracy: 100.00% (2/2)\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   All items - Accuracy: 0.00% (0/0)\n",
      "\n",
      "Item count mismatch: gold=7, pred=5\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   All items - Accuracy: 40.00% (2/5)\n",
      "   Contributions - Accuracy: 80.00% (4/5)\n",
      "\n",
      "Item count mismatch: gold=6, pred=4\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   All items - Accuracy: 0.00% (0/4)\n",
      "   Contributions - Accuracy: 100.00% (4/4)\n",
      "\n",
      "Item count mismatch: gold=3, pred=1\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   All items - Accuracy: 0.00% (0/1)\n",
      "   Contributions - Accuracy: 100.00% (1/1)\n",
      "\n",
      "Item count mismatch: gold=8, pred=0\n",
      "Item count mismatch: gold=4, pred=0\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   All items - Accuracy: 0.00% (0/0)\n",
      "\n",
      "Item count mismatch: gold=0, pred=10\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   All items - Accuracy: 9.09% (1/11)\n",
      "\n",
      "Item count mismatch: gold=4, pred=6\n",
      "Item count mismatch: gold=0, pred=1\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   All items - Accuracy: 50.00% (2/4)\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION ACCURACY SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Overall Accuracy: 38.78% (19/49)\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Overall Accuracy: 85.71% (18/21)\n",
      "\n",
      "Confusion Matrix (All Items):\n",
      "Gold / Pred    ad          paratext    prose       verse       \n",
      "ad             0           1           0           0           \n",
      "paratext       0           12          20          4           \n",
      "prose          0           0           3           3           \n",
      "verse          0           0           2           4           \n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification(gold_path: Path, pred_path: Path,\n",
    "                           item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate item_class classification accuracy.\n",
    "    \n",
    "    Assumes items are in same order (or uses simple alignment).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with accuracy, per-class metrics, confusion matrix\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    # Simple alignment: assume same number and order\n",
    "    if len(gold_items) != len(pred_items):\n",
    "        print(f\"Item count mismatch: gold={len(gold_items)}, pred={len(pred_items)}\")\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    correct = 0\n",
    "    confusion = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_class = gold_items[i].get('item_class', 'unknown')\n",
    "        pred_class = pred_items[i].get('item_class', 'unknown')\n",
    "        \n",
    "        confusion[gold_class][pred_class] += 1\n",
    "        if gold_class == pred_class:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / min_len if min_len > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'total': min_len,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion': dict(confusion),\n",
    "        'gold_count': len(gold_items),\n",
    "        'pred_count': len(pred_items)\n",
    "    }\n",
    "\n",
    "# Evaluate classification\n",
    "print(\"Evaluating item classification...\\n\")\n",
    "\n",
    "classification_results_all = []\n",
    "classification_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_classification(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    classification_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_classification(gold_path, pred_path,\n",
    "                                            item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    classification_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   All items - Accuracy: {result_all['accuracy']:.2%} \"\n",
    "          f\"({result_all['correct']}/{result_all['total']})\")\n",
    "    if result_contrib['total'] > 0:\n",
    "        print(f\"   Contributions - Accuracy: {result_contrib['accuracy']:.2%} \"\n",
    "              f\"({result_contrib['correct']}/{result_contrib['total']})\")\n",
    "    print()\n",
    "\n",
    "# Compute overall accuracy\n",
    "total_correct_all = sum(r['correct'] for r in classification_results_all)\n",
    "total_items_all = sum(r['total'] for r in classification_results_all)\n",
    "overall_accuracy_all = total_correct_all / total_items_all if total_items_all > 0 else 0\n",
    "\n",
    "total_correct_contrib = sum(r['correct'] for r in classification_results_contrib)\n",
    "total_items_contrib = sum(r['total'] for r in classification_results_contrib)\n",
    "overall_accuracy_contrib = total_correct_contrib / total_items_contrib if total_items_contrib > 0 else 0\n",
    "\n",
    "# Aggregate confusion matrix\n",
    "all_confusion = defaultdict(lambda: defaultdict(int))\n",
    "for result in classification_results_all:\n",
    "    for gold_class, pred_dict in result['confusion'].items():\n",
    "        for pred_class, count in pred_dict.items():\n",
    "            all_confusion[gold_class][pred_class] += count\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CLASSIFICATION ACCURACY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_all:.2%} ({total_correct_all}/{total_items_all})\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_contrib:.2%} ({total_correct_contrib}/{total_items_contrib})\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (All Items):\")\n",
    "print(f\"{'Gold / Pred':<15}\", end=\"\")\n",
    "all_classes = sorted(set(list(all_confusion.keys()) + \n",
    "                        [pred for preds in all_confusion.values() for pred in preds.keys()]))\n",
    "for pred_class in all_classes:\n",
    "    print(f\"{pred_class:<12}\", end=\"\")\n",
    "print()\n",
    "for gold_class in all_classes:\n",
    "    print(f\"{gold_class:<15}\", end=\"\")\n",
    "    for pred_class in all_classes:\n",
    "        count = all_confusion[gold_class][pred_class]\n",
    "        print(f\"{count:<12}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c433e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metadata extraction...\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   Title F1: 1.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   Title F1: 0.667, Author F1: 0.667\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   Title F1: 0.667, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   Title F1: 0.500, Author F1: 0.400\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   Title F1: 0.667, Author F1: 0.571\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   Title F1: 0.500, Author F1: 0.400\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   Title F1: 0.000, Author F1: 0.000\n",
      "\n",
      "============================================================\n",
      "METADATA EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Title - P: 43.75%, R: 87.50%, F1: 0.583\n",
      "           Exact matches: 2/7\n",
      "   Author - P: 38.46%, R: 45.45%, F1: 0.417\n",
      "            Exact matches: 0/5\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Title - P: 100.00%, R: 86.67%, F1: 0.929\n",
      "   Author - P: 92.31%, R: 85.71%, F1: 0.889\n"
     ]
    }
   ],
   "source": [
    "def evaluate_metadata(gold_path: Path, pred_path: Path,\n",
    "                     item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate title and author extraction accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with title/author presence detection and exact match metrics\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    title_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    author_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Title evaluation\n",
    "        gold_title = gold_item.get('item_title')\n",
    "        pred_title = pred_item.get('item_title')\n",
    "        \n",
    "        if gold_title and pred_title:\n",
    "            title_metrics['tp'] += 1\n",
    "            if gold_title == pred_title:\n",
    "                title_metrics['exact_match'] += 1\n",
    "        elif not gold_title and pred_title:\n",
    "            title_metrics['fp'] += 1\n",
    "        elif gold_title and not pred_title:\n",
    "            title_metrics['fn'] += 1\n",
    "        \n",
    "        # Author evaluation\n",
    "        gold_author = gold_item.get('item_author')\n",
    "        pred_author = pred_item.get('item_author')\n",
    "        \n",
    "        if gold_author and pred_author:\n",
    "            author_metrics['tp'] += 1\n",
    "            if gold_author == pred_author:\n",
    "                author_metrics['exact_match'] += 1\n",
    "        elif not gold_author and pred_author:\n",
    "            author_metrics['fp'] += 1\n",
    "        elif gold_author and not pred_author:\n",
    "            author_metrics['fn'] += 1\n",
    "    \n",
    "    # Compute F1 for title\n",
    "    title_p = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fp']) if (title_metrics['tp'] + title_metrics['fp']) > 0 else 0\n",
    "    title_r = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fn']) if (title_metrics['tp'] + title_metrics['fn']) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    # Compute F1 for author\n",
    "    author_p = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fp']) if (author_metrics['tp'] + author_metrics['fp']) > 0 else 0\n",
    "    author_r = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fn']) if (author_metrics['tp'] + author_metrics['fn']) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {\n",
    "            **title_metrics,\n",
    "            'precision': title_p,\n",
    "            'recall': title_r,\n",
    "            'f1': title_f1\n",
    "        },\n",
    "        'author': {\n",
    "            **author_metrics,\n",
    "            'precision': author_p,\n",
    "            'recall': author_r,\n",
    "            'f1': author_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate metadata extraction\n",
    "print(\"Evaluating metadata extraction...\\n\")\n",
    "\n",
    "metadata_results_all = []\n",
    "metadata_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_metadata(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    metadata_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_metadata(gold_path, pred_path,\n",
    "                                      item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    metadata_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   Title F1: {result_all['title']['f1']:.3f}, \"\n",
    "          f\"Author F1: {result_all['author']['f1']:.3f}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "def aggregate_metadata_metrics(results):\n",
    "    total_title_tp = sum(r['title']['tp'] for r in results)\n",
    "    total_title_fp = sum(r['title']['fp'] for r in results)\n",
    "    total_title_fn = sum(r['title']['fn'] for r in results)\n",
    "    total_title_exact = sum(r['title']['exact_match'] for r in results)\n",
    "    \n",
    "    title_p = total_title_tp / (total_title_tp + total_title_fp) if (total_title_tp + total_title_fp) > 0 else 0\n",
    "    title_r = total_title_tp / (total_title_tp + total_title_fn) if (total_title_tp + total_title_fn) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    total_author_tp = sum(r['author']['tp'] for r in results)\n",
    "    total_author_fp = sum(r['author']['fp'] for r in results)\n",
    "    total_author_fn = sum(r['author']['fn'] for r in results)\n",
    "    total_author_exact = sum(r['author']['exact_match'] for r in results)\n",
    "    \n",
    "    author_p = total_author_tp / (total_author_tp + total_author_fp) if (total_author_tp + total_author_fp) > 0 else 0\n",
    "    author_r = total_author_tp / (total_author_tp + total_author_fn) if (total_author_tp + total_author_fn) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {'precision': title_p, 'recall': title_r, 'f1': title_f1, 'exact_match': total_title_exact, 'tp': total_title_tp},\n",
    "        'author': {'precision': author_p, 'recall': author_r, 'f1': author_f1, 'exact_match': total_author_exact, 'tp': total_author_tp}\n",
    "    }\n",
    "\n",
    "agg_all = aggregate_metadata_metrics(metadata_results_all)\n",
    "agg_contrib = aggregate_metadata_metrics(metadata_results_contrib)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METADATA EXTRACTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Title - P: {agg_all['title']['precision']:.2%}, R: {agg_all['title']['recall']:.2%}, F1: {agg_all['title']['f1']:.3f}\")\n",
    "print(f\"           Exact matches: {agg_all['title']['exact_match']}/{agg_all['title']['tp']}\")\n",
    "print(f\"   Author - P: {agg_all['author']['precision']:.2%}, R: {agg_all['author']['recall']:.2%}, F1: {agg_all['author']['f1']:.3f}\")\n",
    "print(f\"            Exact matches: {agg_all['author']['exact_match']}/{agg_all['author']['tp']}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Title - P: {agg_contrib['title']['precision']:.2%}, R: {agg_contrib['title']['recall']:.2%}, F1: {agg_contrib['title']['f1']:.3f}\")\n",
    "print(f\"   Author - P: {agg_contrib['author']['precision']:.2%}, R: {agg_contrib['author']['recall']:.2%}, F1: {agg_contrib['author']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838969fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating continuation tracking...\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "   is_continuation - F1: 1.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   is_continuation - F1: 1.000\n",
      "   continues_on_next - F1: 1.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "✓ La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "============================================================\n",
      "CONTINUATION TRACKING SUMMARY (Contributions Only)\n",
      "============================================================\n",
      "\n",
      "is_continuation:\n",
      "   Precision: 100.00%\n",
      "   Recall: 33.33%\n",
      "   F1: 0.500\n",
      "\n",
      "continues_on_next_page:\n",
      "   Precision: 100.00%\n",
      "   Recall: 71.43%\n",
      "   F1: 0.833\n"
     ]
    }
   ],
   "source": [
    "def evaluate_continuation_tracking(gold_path: Path, pred_path: Path,\n",
    "                                  item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy (is_continuation, continues_on_next_page).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1 for each continuation field\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate prediction\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    # Metrics for is_continuation\n",
    "    is_cont_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    # Metrics for continues_on_next_page\n",
    "    continues_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Evaluate is_continuation (treat absent as False)\n",
    "        gold_is_cont = gold_item.get('is_continuation', False)\n",
    "        pred_is_cont = pred_item.get('is_continuation', False)\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['tp'] += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['fp'] += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_metrics['fn'] += 1\n",
    "        else:\n",
    "            is_cont_metrics['tn'] += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page', False)\n",
    "        pred_continues = pred_item.get('continues_on_next_page', False)\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_metrics['tp'] += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_metrics['fp'] += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_metrics['fn'] += 1\n",
    "        else:\n",
    "            continues_metrics['tn'] += 1\n",
    "    \n",
    "    # Compute metrics for is_continuation\n",
    "    is_cont_p = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fp']) if (is_cont_metrics['tp'] + is_cont_metrics['fp']) > 0 else 0\n",
    "    is_cont_r = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fn']) if (is_cont_metrics['tp'] + is_cont_metrics['fn']) > 0 else 0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "    \n",
    "    # Compute metrics for continues_on_next_page\n",
    "    continues_p = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fp']) if (continues_metrics['tp'] + continues_metrics['fp']) > 0 else 0\n",
    "    continues_r = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fn']) if (continues_metrics['tp'] + continues_metrics['fn']) > 0 else 0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            **is_cont_metrics,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            **continues_metrics,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate continuation tracking\n",
    "print(\"Evaluating continuation tracking...\\n\")\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    result = evaluate_continuation_tracking(gold_path, pred_path,\n",
    "                                           item_classes=['prose', 'verse'])\n",
    "    result['page'] = gold_path.name\n",
    "    continuation_results.append(result)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   is_continuation - F1: {result['is_continuation']['f1']:.3f}\")\n",
    "    print(f\"   continues_on_next - F1: {result['continues_on_next_page']['f1']:.3f}\\n\")\n",
    "\n",
    "# Aggregate continuation metrics\n",
    "total_is_cont_tp = sum(r['is_continuation']['tp'] for r in continuation_results)\n",
    "total_is_cont_fp = sum(r['is_continuation']['fp'] for r in continuation_results)\n",
    "total_is_cont_fn = sum(r['is_continuation']['fn'] for r in continuation_results)\n",
    "\n",
    "is_cont_p = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fp) if (total_is_cont_tp + total_is_cont_fp) > 0 else 0\n",
    "is_cont_r = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fn) if (total_is_cont_tp + total_is_cont_fn) > 0 else 0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "\n",
    "total_continues_tp = sum(r['continues_on_next_page']['tp'] for r in continuation_results)\n",
    "total_continues_fp = sum(r['continues_on_next_page']['fp'] for r in continuation_results)\n",
    "total_continues_fn = sum(r['continues_on_next_page']['fn'] for r in continuation_results)\n",
    "\n",
    "continues_p = total_continues_tp / (total_continues_tp + total_continues_fp) if (total_continues_tp + total_continues_fp) > 0 else 0\n",
    "continues_r = total_continues_tp / (total_continues_tp + total_continues_fn) if (total_continues_tp + total_continues_fn) > 0 else 0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CONTINUATION TRACKING SUMMARY (Contributions Only)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nis_continuation:\")\n",
    "print(f\"   Precision: {is_cont_p:.2%}\")\n",
    "print(f\"   Recall: {is_cont_r:.2%}\")\n",
    "print(f\"   F1: {is_cont_f1:.3f}\")\n",
    "print(f\"\\ncontinues_on_next_page:\")\n",
    "print(f\"   Precision: {continues_p:.2%}\")\n",
    "print(f\"   Recall: {continues_r:.2%}\")\n",
    "print(f\"   F1: {continues_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EVALUATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Second_try_revised vs Gold Standard\n",
      "Evaluated on 14 pages\n",
      "\n",
      "Metric                         All Items            Contributions       \n",
      "----------------------------------------------------------------------\n",
      "TEXT QUALITY                  \n",
      "  Character Error Rate                     12.63%             31.65%\n",
      "  Word Error Rate                          17.82%             34.04%\n",
      "\n",
      "STRUCTURE QUALITY             \n",
      "  Boundary Detection F1                     0.426              0.459\n",
      "  Classification Accuracy                  38.78%             85.71%\n",
      "\n",
      "METADATA EXTRACTION           \n",
      "  Title F1                                  0.583              0.929\n",
      "  Author F1                                 0.417              0.889\n",
      "\n",
      "CONTINUATION TRACKING          N/A                  Contributions       \n",
      "  is_continuation F1                                             0.500\n",
      "  continues_on_next F1                                           0.833\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSecond_try_revised vs Gold Standard\")\n",
    "print(f\"Evaluated on {len(page_pairs)} pages\\n\")\n",
    "\n",
    "print(f\"{'Metric':<30} {'All Items':<20} {'Contributions':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Text Quality\n",
    "print(f\"{'TEXT QUALITY':<30}\")\n",
    "print(f\"{'  Character Error Rate':<30} {avg_cer_all:>18.2%} {avg_cer_contrib:>18.2%}\")\n",
    "print(f\"{'  Word Error Rate':<30} {avg_wer_all:>18.2%} {avg_wer_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Structure Quality\n",
    "print(f\"{'STRUCTURE QUALITY':<30}\")\n",
    "print(f\"{'  Boundary Detection F1':<30} {f1_all:>18.3f} {f1_contrib:>18.3f}\")\n",
    "print(f\"{'  Classification Accuracy':<30} {overall_accuracy_all:>18.2%} {overall_accuracy_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Metadata Quality\n",
    "print(f\"{'METADATA EXTRACTION':<30}\")\n",
    "print(f\"{'  Title F1':<30} {agg_all['title']['f1']:>18.3f} {agg_contrib['title']['f1']:>18.3f}\")\n",
    "print(f\"{'  Author F1':<30} {agg_all['author']['f1']:>18.3f} {agg_contrib['author']['f1']:>18.3f}\")\n",
    "print()\n",
    "\n",
    "# Continuation Tracking\n",
    "print(f\"{'CONTINUATION TRACKING':<30} {'N/A':<20} {'Contributions':<20}\")\n",
    "print(f\"{'  is_continuation F1':<30} {'':<20} {is_cont_f1:>18.3f}\")\n",
    "print(f\"{'  continues_on_next F1':<30} {'':<20} {continues_f1:>18.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94687802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONTINUATION FIELD ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "📚 GOLD STANDARD:\n",
      "  Files processed: 14\n",
      "  Total items: 70\n",
      "\n",
      "  is_continuation:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   63\n",
      "    Absent: 0\n",
      "\n",
      "  continues_on_next_page:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   63\n",
      "    Absent: 0\n",
      "\n",
      "\n",
      "🤖 PREDICTIONS:\n",
      "  Files processed: 14\n",
      "  Total items: 52\n",
      "\n",
      "  is_continuation:\n",
      "    True:   3\n",
      "    False:  0\n",
      "    Null:   49\n",
      "    Absent: 0\n",
      "\n",
      "  continues_on_next_page:\n",
      "    True:   7\n",
      "    False:  0\n",
      "    Null:   45\n",
      "    Absent: 0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "\n",
      "is_continuation=True:\n",
      "  Gold has: 7\n",
      "  Pred has: 3\n",
      "  Detection rate: 3/7 = 42.9%\n",
      "\n",
      "continues_on_next_page=True:\n",
      "  Gold has: 7\n",
      "  Pred has: 7\n",
      "  Detection rate: 7/7 = 100.0%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/fabian-ramirez/Documents/These/Code/magazine_graphs\")\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "def analyze_continuation_fields(directory, label):\n",
    "    \"\"\"Count continuation field usage across all files.\"\"\"\n",
    "    stats = {\n",
    "        'is_continuation': {'true': 0, 'false': 0, 'null': 0, 'absent': 0},\n",
    "        'continues_on_next_page': {'true': 0, 'false': 0, 'null': 0, 'absent': 0},\n",
    "        'total_items': 0,\n",
    "        'files_processed': 0\n",
    "    }\n",
    "    \n",
    "    for json_file in sorted(directory.glob(\"*.json\")):\n",
    "        try:\n",
    "            data = json.loads(json_file.read_text(encoding='utf-8'))\n",
    "            items = data.get('items', [])\n",
    "            stats['files_processed'] += 1\n",
    "            \n",
    "            for item in items:\n",
    "                stats['total_items'] += 1\n",
    "                \n",
    "                # Check is_continuation\n",
    "                is_cont = item.get('is_continuation')\n",
    "                if is_cont is True:\n",
    "                    stats['is_continuation']['true'] += 1\n",
    "                elif is_cont is False:\n",
    "                    stats['is_continuation']['false'] += 1\n",
    "                elif is_cont is None:\n",
    "                    stats['is_continuation']['null'] += 1\n",
    "                else:  # key not present\n",
    "                    stats['is_continuation']['absent'] += 1\n",
    "                \n",
    "                # Check continues_on_next_page\n",
    "                continues = item.get('continues_on_next_page')\n",
    "                if continues is True:\n",
    "                    stats['continues_on_next_page']['true'] += 1\n",
    "                elif continues is False:\n",
    "                    stats['continues_on_next_page']['false'] += 1\n",
    "                elif continues is None:\n",
    "                    stats['continues_on_next_page']['null'] += 1\n",
    "                else:  # key not present\n",
    "                    stats['continues_on_next_page']['absent'] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file.name}: {e}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTINUATION FIELD ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze gold standard\n",
    "print(\"\\n📚 GOLD STANDARD:\")\n",
    "gold_stats = analyze_continuation_fields(GOLD_DIR, \"Gold\")\n",
    "print(f\"  Files processed: {gold_stats['files_processed']}\")\n",
    "print(f\"  Total items: {gold_stats['total_items']}\")\n",
    "print(f\"\\n  is_continuation:\")\n",
    "print(f\"    True:   {gold_stats['is_continuation']['true']}\")\n",
    "print(f\"    False:  {gold_stats['is_continuation']['false']}\")\n",
    "print(f\"    Null:   {gold_stats['is_continuation']['null']}\")\n",
    "print(f\"    Absent: {gold_stats['is_continuation']['absent']}\")\n",
    "print(f\"\\n  continues_on_next_page:\")\n",
    "print(f\"    True:   {gold_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"    False:  {gold_stats['continues_on_next_page']['false']}\")\n",
    "print(f\"    Null:   {gold_stats['continues_on_next_page']['null']}\")\n",
    "print(f\"    Absent: {gold_stats['continues_on_next_page']['absent']}\")\n",
    "\n",
    "# Analyze predictions\n",
    "print(\"\\n\\n🤖 PREDICTIONS:\")\n",
    "pred_stats = analyze_continuation_fields(PRED_DIR, \"Predictions\")\n",
    "print(f\"  Files processed: {pred_stats['files_processed']}\")\n",
    "print(f\"  Total items: {pred_stats['total_items']}\")\n",
    "print(f\"\\n  is_continuation:\")\n",
    "print(f\"    True:   {pred_stats['is_continuation']['true']}\")\n",
    "print(f\"    False:  {pred_stats['is_continuation']['false']}\")\n",
    "print(f\"    Null:   {pred_stats['is_continuation']['null']}\")\n",
    "print(f\"    Absent: {pred_stats['is_continuation']['absent']}\")\n",
    "print(f\"\\n  continues_on_next_page:\")\n",
    "print(f\"    True:   {pred_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"    False:  {pred_stats['continues_on_next_page']['false']}\")\n",
    "print(f\"    Null:   {pred_stats['continues_on_next_page']['null']}\")\n",
    "print(f\"    Absent: {pred_stats['continues_on_next_page']['absent']}\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nis_continuation=True:\")\n",
    "print(f\"  Gold has: {gold_stats['is_continuation']['true']}\")\n",
    "print(f\"  Pred has: {pred_stats['is_continuation']['true']}\")\n",
    "print(f\"  Detection rate: {pred_stats['is_continuation']['true']}/{gold_stats['is_continuation']['true']} = \"\n",
    "      f\"{pred_stats['is_continuation']['true']/gold_stats['is_continuation']['true']*100:.1f}%\" \n",
    "      if gold_stats['is_continuation']['true'] > 0 else \"  N/A\")\n",
    "\n",
    "print(f\"\\ncontinues_on_next_page=True:\")\n",
    "print(f\"  Gold has: {gold_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"  Pred has: {pred_stats['continues_on_next_page']['true']}\")\n",
    "print(f\"  Detection rate: {pred_stats['continues_on_next_page']['true']}/{gold_stats['continues_on_next_page']['true']} = \"\n",
    "      f\"{pred_stats['continues_on_next_page']['true']/gold_stats['continues_on_next_page']['true']*100:.1f}%\"\n",
    "      if gold_stats['continues_on_next_page']['true'] > 0 else \"  N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e01889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Worst 5 Pages by Character Error Rate:\n",
      "1. La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "   CER: 100.00%, WER: 100.00%\n",
      "   Gold: 8 items, 3634 chars\n",
      "   Pred: 0 items, 0 chars\n",
      "\n",
      "2. La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "   CER: 21.48%, WER: 27.55%\n",
      "   Gold: 5 items, 4745 chars\n",
      "   Pred: 4 items, 4648 chars\n",
      "\n",
      "3. La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "   CER: 14.49%, WER: 18.18%\n",
      "   Gold: 2 items, 69 chars\n",
      "   Pred: 3 items, 69 chars\n",
      "\n",
      "4. La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "   CER: 9.63%, WER: 12.56%\n",
      "   Gold: 3 items, 5236 chars\n",
      "   Pred: 1 items, 5174 chars\n",
      "\n",
      "5. La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "   CER: 8.12%, WER: 10.99%\n",
      "   Gold: 4 items, 1158 chars\n",
      "   Pred: 6 items, 1190 chars\n",
      "\n",
      "\n",
      "Pages with Item Count Mismatches:\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-001.json: Gold=8, Pred=5 (-3)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-004.json: Gold=5, Pred=4 (-1)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-005.json: Gold=5, Pred=4 (-1)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-006.json: Gold=6, Pred=4 (-2)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-007.json: Gold=2, Pred=3 (+1)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-009.json: Gold=7, Pred=5 (-2)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-010.json: Gold=6, Pred=4 (-2)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-011.json: Gold=3, Pred=1 (-2)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-012.json: Gold=8, Pred=0 (-8)\n",
      "  • La_Plume_bpt6k1185893k_1_10_1889__page-014.json: Gold=4, Pred=6 (+2)\n",
      "\n",
      "Most Common Classification Errors:\n",
      "  • paratext → prose: 20 times\n",
      "  • paratext → verse: 4 times\n",
      "  • prose → verse: 3 times\n",
      "  • verse → prose: 2 times\n",
      "  • ad → paratext: 1 times\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find worst performing pages by CER\n",
    "worst_pages_cer = sorted(all_results, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nWorst 5 Pages by Character Error Rate:\")\n",
    "for i, result in enumerate(worst_pages_cer, 1):\n",
    "    print(f\"{i}. {result['page']}\")\n",
    "    print(f\"   CER: {result['cer']:.2%}, WER: {result['wer']:.2%}\")\n",
    "    print(f\"   Gold: {result['gold_items']} items, {result['gold_chars']} chars\")\n",
    "    print(f\"   Pred: {result['pred_items']} items, {result['pred_chars']} chars\")\n",
    "    print()\n",
    "\n",
    "# Find pages with item count mismatches\n",
    "print(\"\\nPages with Item Count Mismatches:\")\n",
    "mismatches = [r for r in classification_results_all if r['gold_count'] != r['pred_count']]\n",
    "if mismatches:\n",
    "    for result in mismatches:\n",
    "        diff = result['pred_count'] - result['gold_count']\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"  • {result['page']}: Gold={result['gold_count']}, Pred={result['pred_count']} ({sign}{diff})\")\n",
    "else:\n",
    "    print(\"  No mismatches found!\")\n",
    "\n",
    "# Classification errors\n",
    "print(\"\\nMost Common Classification Errors:\")\n",
    "errors = []\n",
    "for gold_class, pred_dict in all_confusion.items():\n",
    "    for pred_class, count in pred_dict.items():\n",
    "        if gold_class != pred_class and count > 0:\n",
    "            errors.append((count, gold_class, pred_class))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, gold_class, pred_class in errors[:10]:\n",
    "    print(f\"  • {gold_class} → {pred_class}: {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aebfe96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_gold_standard_names():\n",
    "#     \"\"\"\n",
    "#     Rename gold standard files to match the PDF-based naming convention.\n",
    "#     Old: La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
    "#     New: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "    \n",
    "#     # The standard name from your PDF\n",
    "#     STANDARD_BASE = \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "    \n",
    "#     for old_path in GOLD_DIR.glob(\"*.json\"):\n",
    "#         # Extract page number\n",
    "#         match = re.search(r'page-(\\d+)\\.json$', old_path.name)\n",
    "#         if not match:\n",
    "#             print(f\"⚠️  Skipping (no page number): {old_path.name}\")\n",
    "#             continue\n",
    "        \n",
    "#         page_num = match.group(1)\n",
    "#         new_name = f\"{STANDARD_BASE}__page-{page_num}.json\"\n",
    "#         new_path = old_path.parent / new_name\n",
    "        \n",
    "#         if old_path.name != new_name:\n",
    "#             print(f\"Renaming: {old_path.name}\")\n",
    "#             print(f\"      →  {new_name}\")\n",
    "#             old_path.rename(new_path)\n",
    "    \n",
    "#     print(\"\\n✓ Gold standard filenames standardized!\")\n",
    "\n",
    "# # Uncomment to run:\n",
    "# standardize_gold_standard_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
