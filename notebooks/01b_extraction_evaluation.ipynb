{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Evaluation\n",
      "\n",
      "\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "Predictions: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/interim_pages/La_Plume_bpt6k1185893k_1_10_1889\n",
      "\n",
      "Dataset:\n",
      "  Gold files: 14\n",
      "  Pred files: 14\n",
      "  Matching pairs: 14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Path setup\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import schemas for validation\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "\n",
    "# Paths\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"La_Plume_bpt6k1185893k_1_10_1889\"\n",
    "\n",
    "print(\"Stage 1 OCR Evaluation\")\n",
    "print(\"\\n\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Gold standard: {GOLD_DIR}\")\n",
    "print(f\"Predictions: {PRED_DIR}\")\n",
    "\n",
    "# Find common files\n",
    "def load_page_pairs() -> List[Tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Match gold standard files with prediction files by filename.\n",
    "    Returns list of (gold_path, pred_path) tuples.\n",
    "    \"\"\"\n",
    "    gold_files = {f.name: f for f in GOLD_DIR.glob(\"*.json\")}\n",
    "    pred_files = {f.name: f for f in PRED_DIR.glob(\"*.json\")}\n",
    "    \n",
    "    common_names = set(gold_files.keys()) & set(pred_files.keys())\n",
    "    \n",
    "    pairs = [(gold_files[name], pred_files[name]) for name in sorted(common_names)]\n",
    "    \n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Gold files: {len(gold_files)}\")\n",
    "    print(f\"  Pred files: {len(pred_files)}\")\n",
    "    print(f\"  Matching pairs: {len(pairs)}\")\n",
    "    \n",
    "    if len(pairs) < len(gold_files):\n",
    "        missing = set(gold_files.keys()) - set(pred_files.keys())\n",
    "        print(f\"Warning: {len(missing)} gold standard pages without predictions:\")\n",
    "        for name in sorted(missing):\n",
    "            print(f\"   - {name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "page_pairs = load_page_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2a9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Item Matching Configuration\n",
      "\n",
      "\n",
      "Similarity threshold: 0.7\n",
      "\n",
      "\n",
      "Item Matching Test\n",
      "\n",
      "\n",
      "\n",
      "Test page: La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Gold items: 8\n",
      "  Pred items: 5\n",
      "  Matches found: 4\n",
      "  Unmatched gold: 4\n",
      "  Unmatched pred: 1\n",
      "  Average match quality: 92.47%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Match gold items to predicted items using content-based text similarity.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "SIMILARITY_THRESHOLD = 0.7  # Minimum text similarity to consider a match (0.0-1.0)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Configuration\")\n",
    "print(\"\\n\")\n",
    "print(f\"Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "        \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item, find the best-matching unmatched pred item.\n",
    "        Accept the match if similarity exceeds threshold.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair and match items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "\n",
    "# Test matching on first page\n",
    "print(\"\\n\")\n",
    "print(\"Item Matching Test\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if page_pairs:\n",
    "    test_gold, test_pred = page_pairs[0]\n",
    "    test_result = load_and_match_page(test_gold, test_pred)\n",
    "    \n",
    "    print(f\"\\nTest page: {test_result['page_name']}\")\n",
    "    print(f\"  Gold items: {len(test_result['gold_items'])}\")\n",
    "    print(f\"  Pred items: {len(test_result['pred_items'])}\")\n",
    "    print(f\"  Matches found: {len(test_result['matches'])}\")\n",
    "    print(f\"  Unmatched gold: {len(test_result['unmatched_gold'])}\")\n",
    "    print(f\"  Unmatched pred: {len(test_result['unmatched_pred'])}\")\n",
    "    \n",
    "    if test_result['matches']:\n",
    "        avg_score = sum(score for _, _, score in test_result['matches']) / len(test_result['matches'])\n",
    "        print(f\"  Average match quality: {avg_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7322404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running diagnostics on all pages...\n",
      "\n",
      "\n",
      "\n",
      "SUMMARY TABLE\n",
      "\n",
      "\n",
      "                                   page_id  gold_items  pred_items  matched  match_rate_%  contrib_match_rate_%  avg_similarity  gold_cont_in  pred_cont_in  gold_cont_out  pred_cont_out                                                            flags\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001           8           5        4          50.0                   0.0           0.925             0             0              0              0                                                   COUNT_MISMATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002           2           2        2         100.0                   0.0           1.000             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003           3           3        1          33.3                   0.0           0.713             0             1              1              2                                           LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004           5           4        3          60.0                 100.0           0.942             1             1              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005           5           4        1          20.0                  33.3           0.755             1             0              1              1                                           LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006           6           4        3          50.0                  75.0           0.967             1             0              1              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007           2           3        1          50.0                   0.0           1.000             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008           0           0        0           0.0                   0.0           0.000             0             0              0              0                              ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009           7           5        4          57.1                  80.0           0.953             1             0              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010           6           4        4          66.7                 100.0           0.971             1             0              1              1                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011           3           1        0           0.0                   0.0           0.000             1             1              1              1                             ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012           8           0        0           0.0                   0.0           0.000             1             0              0              0 ZERO_PREDS, ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB, COUNT_MISMATCH\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013          11          11       10          90.9                   0.0           0.951             0             0              0              0                                                                 \n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014           4           6        4         100.0                   0.0           0.968             0             0              0              0                                                                 \n",
      "\n",
      "\n",
      "================================================================================\n",
      "DETAILED REPORTS\n",
      "================================================================================\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-001 ===\n",
      "Items: 8 gold, 5 pred\n",
      "Matches: 4 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   8 gold, 5 pred, 4 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.925\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1, 2, 3, 7]\n",
      "Unmatched pred items: [1]\n",
      "\n",
      "FLAGS: COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-002 ===\n",
      "Items: 2 gold, 2 pred\n",
      "Matches: 2 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 2 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-003 ===\n",
      "Items: 3 gold, 3 pred\n",
      "Matches: 1 (33.3% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 1 pred, 1 matched (50.0%)\n",
      "  prose      1 gold, 2 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 2 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.713\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 2 pred\n",
      "\n",
      "Unmatched gold items: [1, 2]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-004 ===\n",
      "Items: 5 gold, 4 pred\n",
      "Matches: 3 (60.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 3 pred, 2 matched (100.0%)\n",
      "  verse      1 gold, 1 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 4 pred, 3 matched (100.0%)\n",
      "Avg similarity: 0.942\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1]\n",
      "Unmatched pred items: [1]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-005 ===\n",
      "Items: 5 gold, 4 pred\n",
      "Matches: 1 (20.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 0 matched (0.0%)\n",
      "  verse      1 gold, 2 pred, 1 matched (100.0%)\n",
      "\n",
      "Contributions: 3 gold, 4 pred, 1 matched (33.3%)\n",
      "Avg similarity: 0.755\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 4]\n",
      "Unmatched pred items: [0, 1, 2]\n",
      "\n",
      "FLAGS: LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-006 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 3 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 3 matched (75.0%)\n",
      "Avg similarity: 0.967\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 5]\n",
      "Unmatched pred items: [3]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-007 ===\n",
      "Items: 2 gold, 3 pred\n",
      "Matches: 1 (50.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 3 pred, 1 matched (50.0%)\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 1.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [1]\n",
      "Unmatched pred items: [1, 2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-008 ===\n",
      "Items: 0 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "\n",
      "Contributions: 0 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-009 ===\n",
      "Items: 7 gold, 5 pred\n",
      "Matches: 4 (57.1% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 1 pred, 2 matched (100.0%)\n",
      "  verse      3 gold, 4 pred, 2 matched (66.7%)\n",
      "\n",
      "Contributions: 5 gold, 5 pred, 4 matched (80.0%)\n",
      "Avg similarity: 0.953\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 4]\n",
      "Unmatched pred items: [2]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-010 ===\n",
      "Items: 6 gold, 4 pred\n",
      "Matches: 4 (66.7% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 2 pred, 2 matched (100.0%)\n",
      "  verse      2 gold, 2 pred, 2 matched (100.0%)\n",
      "\n",
      "Contributions: 4 gold, 4 pred, 4 matched (100.0%)\n",
      "Avg similarity: 0.971\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1]\n",
      "Unmatched pred items: []\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-011 ===\n",
      "Items: 3 gold, 1 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   2 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      1 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 1 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 1 pred\n",
      "  continues_on_next_page: 1 gold, 1 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2]\n",
      "Unmatched pred items: [0]\n",
      "\n",
      "FLAGS: ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-012 ===\n",
      "Items: 8 gold, 0 pred\n",
      "Matches: 0 (0.0% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   4 gold, 0 pred, 0 matched (0.0%)\n",
      "  prose      2 gold, 0 pred, 0 matched (0.0%)\n",
      "  verse      2 gold, 0 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 4 gold, 0 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.000\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 1 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Unmatched pred items: []\n",
      "\n",
      "FLAGS: ZERO_PREDS, ZERO_MATCHES, LOW_MATCH, LOW_CONTRIB, COUNT_MISMATCH\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-013 ===\n",
      "Items: 11 gold, 11 pred\n",
      "Matches: 10 (90.9% match rate)\n",
      "\n",
      "By class:\n",
      "  paratext   11 gold, 1 pred, 10 matched (90.9%)\n",
      "  prose      0 gold, 10 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 10 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.951\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: [7]\n",
      "Unmatched pred items: [7]\n",
      "\n",
      "=== Page La_Plume_bpt6k1185893k_1_10_1889__page-014 ===\n",
      "Items: 4 gold, 6 pred\n",
      "Matches: 4 (100.0% match rate)\n",
      "\n",
      "By class:\n",
      "  ad         1 gold, 0 pred, 1 matched (100.0%)\n",
      "  paratext   3 gold, 5 pred, 3 matched (100.0%)\n",
      "  prose      0 gold, 1 pred, 0 matched (0.0%)\n",
      "\n",
      "Contributions: 0 gold, 1 pred, 0 matched (0.0%)\n",
      "Avg similarity: 0.968\n",
      "\n",
      "Continuations:\n",
      "  is_continuation: 0 gold, 0 pred\n",
      "  continues_on_next_page: 0 gold, 0 pred\n",
      "\n",
      "Unmatched gold items: []\n",
      "Unmatched pred items: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-Level Diagnostics\n",
    "Generate diagnostic metrics for each page based on item matches.\n",
    "\"\"\"\n",
    "\n",
    "def diagnose_page(page_id: str, gold_items: list, pred_items: list, matches: list) -> dict:\n",
    "    \"\"\"\n",
    "    Generate diagnostic metrics for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page_id: Page identifier\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with diagnostic metrics\n",
    "    \"\"\"\n",
    "    # Count items by class\n",
    "    gold_by_class = {}\n",
    "    pred_by_class = {}\n",
    "    \n",
    "    for item in gold_items:\n",
    "        item_class = item['item_class']\n",
    "        gold_by_class[item_class] = gold_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    for item in pred_items:\n",
    "        item_class = item['item_class']\n",
    "        pred_by_class[item_class] = pred_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    # Count contributions (prose + verse)\n",
    "    gold_contrib = gold_by_class.get('prose', 0) + gold_by_class.get('verse', 0)\n",
    "    pred_contrib = pred_by_class.get('prose', 0) + pred_by_class.get('verse', 0)\n",
    "    \n",
    "    # Filter matches by contribution class\n",
    "    contrib_matches = [\n",
    "        (g_idx, p_idx, score) for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in ('prose', 'verse')\n",
    "    ]\n",
    "    \n",
    "    # Calculate match rates\n",
    "    match_rate = (len(matches) / len(gold_items) * 100) if gold_items else 0\n",
    "    contrib_match_rate = (len(contrib_matches) / gold_contrib * 100) if gold_contrib else 0\n",
    "    \n",
    "    # Calculate average similarity\n",
    "    avg_similarity = (sum(score for _, _, score in matches) / len(matches)) if matches else 0\n",
    "    \n",
    "    # Count continuation flags\n",
    "    gold_cont_in = sum(1 for item in gold_items if item.get('is_continuation') is True)\n",
    "    pred_cont_in = sum(1 for item in pred_items if item.get('is_continuation') is True)\n",
    "    gold_cont_out = sum(1 for item in gold_items if item.get('continues_on_next_page') is True)\n",
    "    pred_cont_out = sum(1 for item in pred_items if item.get('continues_on_next_page') is True)\n",
    "    \n",
    "    # Track matched indices\n",
    "    matched_gold = {g_idx for g_idx, _, _ in matches}\n",
    "    matched_pred = {p_idx for _, p_idx, _ in matches}\n",
    "    \n",
    "    unmatched_gold = [i for i in range(len(gold_items)) if i not in matched_gold]\n",
    "    unmatched_pred = [i for i in range(len(pred_items)) if i not in matched_pred]\n",
    "    \n",
    "    # Count matches by class\n",
    "    matches_by_class = {}\n",
    "    for g_idx, p_idx, score in matches:\n",
    "        item_class = gold_items[g_idx]['item_class']\n",
    "        matches_by_class[item_class] = matches_by_class.get(item_class, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items),\n",
    "        'matched': len(matches),\n",
    "        'match_rate': match_rate,\n",
    "        'contrib_match_rate': contrib_match_rate,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'gold_cont_in': gold_cont_in,\n",
    "        'pred_cont_in': pred_cont_in,\n",
    "        'gold_cont_out': gold_cont_out,\n",
    "        'pred_cont_out': pred_cont_out,\n",
    "        'gold_by_class': gold_by_class,\n",
    "        'pred_by_class': pred_by_class,\n",
    "        'matches_by_class': matches_by_class,\n",
    "        'gold_contrib': gold_contrib,\n",
    "        'pred_contrib': pred_contrib,\n",
    "        'contrib_matched': len(contrib_matches),\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def flag_page(metrics: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate flags for problematic pages based on metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary from diagnose_page()\n",
    "        \n",
    "    Returns:\n",
    "        Comma-separated string of flags, or empty string if no issues\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    if metrics['pred_items'] == 0:\n",
    "        flags.append('ZERO_PREDS')\n",
    "    \n",
    "    if metrics['matched'] == 0:\n",
    "        flags.append('ZERO_MATCHES')\n",
    "    \n",
    "    if metrics['match_rate'] < 50:\n",
    "        flags.append('LOW_MATCH')\n",
    "    \n",
    "    if metrics['gold_contrib'] > 0 and metrics['contrib_match_rate'] < 60:\n",
    "        flags.append('LOW_CONTRIB')\n",
    "    \n",
    "    if abs(metrics['gold_items'] - metrics['pred_items']) >= 3:\n",
    "        flags.append('COUNT_MISMATCH')\n",
    "    \n",
    "    return ', '.join(flags)\n",
    "\n",
    "\n",
    "def run_diagnostics(page_pairs: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run diagnostics on all pages and generate summary table and detailed reports.\n",
    "    \n",
    "    Args:\n",
    "        page_pairs: List of (gold_path, pred_path) tuples from load_page_pairs()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary metrics for all pages\n",
    "    \"\"\"\n",
    "    print(\"Running diagnostics on all pages...\\n\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for gold_path, pred_path in page_pairs:\n",
    "        # Extract page_id from filename\n",
    "        page_id = gold_path.stem\n",
    "        \n",
    "        # Load and match page\n",
    "        result = load_and_match_page(gold_path, pred_path)\n",
    "        gold_items = result['gold_items']\n",
    "        pred_items = result['pred_items']\n",
    "        matches = result['matches']\n",
    "        \n",
    "        # Generate metrics\n",
    "        metrics = diagnose_page(page_id, gold_items, pred_items, matches)\n",
    "        metrics['flags'] = flag_page(metrics)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for m in all_metrics:\n",
    "        summary_data.append({\n",
    "            'page_id': m['page_id'],\n",
    "            'gold_items': m['gold_items'],\n",
    "            'pred_items': m['pred_items'],\n",
    "            'matched': m['matched'],\n",
    "            'match_rate_%': round(m['match_rate'], 1),\n",
    "            'contrib_match_rate_%': round(m['contrib_match_rate'], 1),\n",
    "            'avg_similarity': round(m['avg_similarity'], 3),\n",
    "            'gold_cont_in': m['gold_cont_in'],\n",
    "            'pred_cont_in': m['pred_cont_in'],\n",
    "            'gold_cont_out': m['gold_cont_out'],\n",
    "            'pred_cont_out': m['pred_cont_out'],\n",
    "            'flags': m['flags']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print detailed reports for all pages\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for m in all_metrics:\n",
    "        print(f\"\\n=== Page {m['page_id']} ===\")\n",
    "        print(f\"Items: {m['gold_items']} gold, {m['pred_items']} pred\")\n",
    "        print(f\"Matches: {m['matched']} ({m['match_rate']:.1f}% match rate)\")\n",
    "        \n",
    "        print(\"\\nBy class:\")\n",
    "        all_classes = sorted(set(m['gold_by_class'].keys()) | set(m['pred_by_class'].keys()))\n",
    "        for cls in all_classes:\n",
    "            gold_count = m['gold_by_class'].get(cls, 0)\n",
    "            pred_count = m['pred_by_class'].get(cls, 0)\n",
    "            matched_count = m['matches_by_class'].get(cls, 0)\n",
    "            match_pct = (matched_count / gold_count * 100) if gold_count > 0 else 0\n",
    "            print(f\"  {cls:10s} {gold_count} gold, {pred_count} pred, {matched_count} matched ({match_pct:.1f}%)\")\n",
    "\n",
    "        \n",
    "        print(f\"\\nContributions: {m['gold_contrib']} gold, {m['pred_contrib']} pred, \"\n",
    "              f\"{m['contrib_matched']} matched ({m['contrib_match_rate']:.1f}%)\")\n",
    "        print(f\"Avg similarity: {m['avg_similarity']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nContinuations:\")\n",
    "        print(f\"  is_continuation: {m['gold_cont_in']} gold, {m['pred_cont_in']} pred\")\n",
    "        print(f\"  continues_on_next_page: {m['gold_cont_out']} gold, {m['pred_cont_out']} pred\")\n",
    "        \n",
    "        print(f\"\\nUnmatched gold items: {m['unmatched_gold']}\")\n",
    "        print(f\"Unmatched pred items: {m['unmatched_pred']}\")\n",
    "        \n",
    "        if m['flags']:\n",
    "            print(f\"\\nFLAGS: {m['flags']}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostic_df = run_diagnostics(page_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fcf45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and matching all pages...\n",
      "Loaded 14 pages\n",
      "Total matches across all pages: 37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation Helpers\n",
    "Utility functions for filtering matches and loading all pages efficiently.\n",
    "These helpers are used by the evaluation cells that follow.\n",
    "\"\"\"\n",
    "\n",
    "def filter_matches_by_class(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    item_classes: List[str]\n",
    ") -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Filter matches to only include items of specified classes.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        item_classes: List of classes to include (e.g., ['prose', 'verse'])\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of matches\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (g_idx, p_idx, score) \n",
    "        for g_idx, p_idx, score in matches\n",
    "        if gold_items[g_idx]['item_class'] in item_classes\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_matched_pairs(\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict]\n",
    ") -> List[Tuple[Dict, Dict, float]]:\n",
    "    \"\"\"\n",
    "    Convert match indices to actual item pairs.\n",
    "    \n",
    "    Args:\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_item, pred_item, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (gold_items[g_idx], pred_items[p_idx], score)\n",
    "        for g_idx, p_idx, score in matches\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_all_pages(page_pairs: List[Tuple[Path, Path]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and match all pages at once for efficient batch evaluation.\n",
    "    \n",
    "    Args:\n",
    "        page_pairs: List of (gold_path, pred_path) tuples from load_page_pairs()\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, one per page, each containing:\n",
    "        - page_id: Page identifier\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "    \n",
    "    for gold_path, pred_path in page_pairs:\n",
    "        result = load_and_match_page(gold_path, pred_path)\n",
    "        result['page_id'] = gold_path.stem\n",
    "        all_pages.append(result)\n",
    "    \n",
    "    return all_pages\n",
    "\n",
    "\n",
    "# Load all pages once for reuse in subsequent evaluation cells\n",
    "print(\"Loading and matching all pages...\")\n",
    "all_pages = load_all_pages(page_pairs)\n",
    "print(f\"Loaded {len(all_pages)} pages\")\n",
    "print(f\"Total matches across all pages: {sum(len(page['matches']) for page in all_pages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text quality...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEXT QUALITY SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. ORDER-AGNOSTIC EVALUATION\n",
      "   (Pure OCR quality, reading order irrelevant)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 14.83%  |  WER: 17.66%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 14.83%  |  WER: 17.66%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 13.48%\n",
      "\n",
      "   Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 18.29%  |  WER: 20.85%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 18.29%  |  WER: 20.85%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 17.94%\n",
      "\n",
      "======================================================================\n",
      "2. STRUCTURE-AWARE EVALUATION\n",
      "   (OCR quality on matched content only)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   Matched Content - All Items:\n",
      "      Strict (with all whitespace):\n",
      "         CER: 15.13%  |  WER: 21.12%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 14.61%  |  WER: 21.12%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 13.81%\n",
      "      Coverage: 27,797 chars matched (59.0% of gold)\n",
      "      Unmatched: 19,350 chars (41.1% of gold)\n",
      "\n",
      "   Matched Content - Contributions Only (prose + verse):\n",
      "      Strict (with all whitespace):\n",
      "         CER: 13.05%  |  WER: 17.63%\n",
      "      Standard (normalized whitespace):\n",
      "         CER: 12.42%  |  WER: 17.63%\n",
      "      Letters Only (no whitespace/punctuation):\n",
      "         CER: 12.11%\n",
      "      Coverage: 17,510 chars matched (48.8% of gold)\n",
      "      Unmatched: 18,365 chars (51.2% of gold)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION GUIDE:\n",
      "----------------------------------------------------------------------\n",
      "Strict: Most conservative\n",
      "Standard: Fair baseline - normalizes whitespace\n",
      "Letters Only: Most lenient - pure character recognition quality\n",
      "\n",
      "======================================================================\n",
      "\n",
      "KEY INSIGHTS:\n",
      "- Pure OCR quality (standard normalization): 14.83%\n",
      "- Letter recognition quality: 13.48%\n",
      "- Structure failures (unmatched content): 41.1%\n",
      "- Contributions:\n",
      "    Standard CER: 12.42%\n",
      "    Successfully matched: 48.8%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Quality Evaluation\n",
    "Calculate CER and WER using two complementary approaches:\n",
    "1. Order-agnostic: Pure OCR quality regardless of reading order\n",
    "2. Structure-aware: OCR quality on properly aligned content via matching\n",
    "\n",
    "Each approach calculates three normalization levels:\n",
    "- Strict: Preserves all whitespace (including \\n vs \\n\\n differences)\n",
    "- Standard: Normalizes whitespace to single spaces (RECOMMENDED)\n",
    "- Letters Only: Removes all whitespace and punctuation (pure character recognition)\n",
    "\n",
    "References:\n",
    "- Flexible Character Accuracy (FCA) for handling reading order issues:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "- Token sort ratio for order-agnostic OCR comparison:\n",
    "  https://urban-institute.medium.com/choosing-the-right-ocr-service-for-extracting-text-data-d7830399ec5\n",
    "- Unicode normalization and whitespace handling in OCR evaluation:\n",
    "  https://ocr-d.de/en/spec/ocrd_eval.html\n",
    "\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_text_strict(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strict normalization: only Unicode NFC normalization.\n",
    "    Preserves all whitespace, punctuation, and capitalization.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "\n",
    "def normalize_text_standard(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standard normalization for fair OCR evaluation:\n",
    "    - Unicode NFC normalization\n",
    "    - All whitespace (spaces, tabs, newlines) â†’ single space\n",
    "    - Preserves punctuation and capitalization\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text_letters_only(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Letter-only normalization for pure character recognition quality:\n",
    "    - Unicode NFC normalization\n",
    "    - Remove all whitespace\n",
    "    - Remove all punctuation\n",
    "    - Preserves capitalization and diacritics\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'[^\\w]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def character_error_rate(reference: str, hypothesis: str, normalization: str = 'strict') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Character Error Rate using Levenshtein distance.\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: OCR output text\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        CER = (insertions + deletions + substitutions) / total_reference_chars\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        ref = normalize_text_letters_only(reference)\n",
    "        hyp = normalize_text_letters_only(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    if not ref:\n",
    "        return 1.0 if hyp else 0.0\n",
    "    distance = Levenshtein.distance(ref, hyp)\n",
    "    return distance / len(ref)\n",
    "\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str, normalization: str = 'strict') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate using Levenshtein distance on words.\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: OCR output text\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        WER = (insertions + deletions + substitutions) / total_reference_words\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        # For letters only, WER doesn't make sense without word boundaries\n",
    "        # So we use standard normalization\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    if not ref_words:\n",
    "        return 1.0 if hyp_words else 0.0\n",
    "    distance = Levenshtein.distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "\n",
    "def token_sort_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Sort tokens (words) alphabetically for order-agnostic comparison.\n",
    "    This removes the impact of reading order on text similarity.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return ' '.join(sorted(tokens))\n",
    "\n",
    "\n",
    "def evaluate_order_agnostic(gold_items: List[Dict], pred_items: List[Dict], \n",
    "                            item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality without considering reading order.\n",
    "    Uses token sort ratio approach - sorts all words before comparison.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        item_classes: If provided, filter to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER for each normalization level, and text statistics\n",
    "    \"\"\"\n",
    "    # Filter by class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items if item['item_class'] in item_classes]\n",
    "        pred_items = [item for item in pred_items if item['item_class'] in item_classes]\n",
    "    \n",
    "    # Concatenate all text\n",
    "    gold_text = ' '.join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = ' '.join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    # Sort tokens for order-agnostic comparison\n",
    "    gold_sorted = token_sort_text(gold_text)\n",
    "    pred_sorted = token_sort_text(pred_text)\n",
    "    \n",
    "    # Calculate for all three normalization levels\n",
    "    results = {\n",
    "        'cer_strict': character_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'wer_strict': word_error_rate(gold_sorted, pred_sorted, 'strict'),\n",
    "        'cer_standard': character_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'wer_standard': word_error_rate(gold_sorted, pred_sorted, 'standard'),\n",
    "        'cer_letters': character_error_rate(gold_sorted, pred_sorted, 'letters_only'),\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split())\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_structure_aware(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                             matches: List[Tuple[int, int, float]],\n",
    "                             item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate text quality on matched pairs, respecting document structure.\n",
    "    Only compares content that was successfully aligned via matching.\n",
    "    Calculates three normalization levels: strict, standard, letters_only.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        item_classes: If provided, filter matches to only these classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with matched CER/WER for each normalization level and unmatched content statistics\n",
    "    \"\"\"\n",
    "    # Filter matches by class if specified\n",
    "    if item_classes:\n",
    "        filtered_matches = filter_matches_by_class(matches, gold_items, item_classes)\n",
    "    else:\n",
    "        filtered_matches = matches\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(filtered_matches, gold_items, pred_items)\n",
    "    \n",
    "    # Calculate CER/WER on matched content for all normalization levels\n",
    "    if matched_pairs:\n",
    "        # Concatenate matched texts in gold order\n",
    "        gold_matched_text = ' '.join(gold_item.get('item_text_raw', '') \n",
    "                                     for gold_item, _, _ in matched_pairs)\n",
    "        pred_matched_text = ' '.join(pred_item.get('item_text_raw', '') \n",
    "                                     for _, pred_item, _ in matched_pairs)\n",
    "        \n",
    "        cer_strict = character_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        wer_strict = word_error_rate(gold_matched_text, pred_matched_text, 'strict')\n",
    "        cer_standard = character_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        wer_standard = word_error_rate(gold_matched_text, pred_matched_text, 'standard')\n",
    "        cer_letters = character_error_rate(gold_matched_text, pred_matched_text, 'letters_only')\n",
    "        \n",
    "        matched_gold_chars = len(gold_matched_text)\n",
    "        matched_pred_chars = len(pred_matched_text)\n",
    "    else:\n",
    "        cer_strict = 0.0\n",
    "        wer_strict = 0.0\n",
    "        cer_standard = 0.0\n",
    "        wer_standard = 0.0\n",
    "        cer_letters = 0.0\n",
    "        matched_gold_chars = 0\n",
    "        matched_pred_chars = 0\n",
    "    \n",
    "    # Calculate unmatched content\n",
    "    matched_gold_indices = {g_idx for g_idx, _, _ in filtered_matches}\n",
    "    matched_pred_indices = {p_idx for _, p_idx, _ in filtered_matches}\n",
    "    \n",
    "    if item_classes:\n",
    "        # Only count unmatched items of the specified classes\n",
    "        unmatched_gold_items = [\n",
    "            gold_items[i] for i in range(len(gold_items))\n",
    "            if i not in matched_gold_indices and gold_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        unmatched_pred_items = [\n",
    "            pred_items[i] for i in range(len(pred_items))\n",
    "            if i not in matched_pred_indices and pred_items[i]['item_class'] in item_classes\n",
    "        ]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                              for item in gold_items if item['item_class'] in item_classes)\n",
    "    else:\n",
    "        unmatched_gold_items = [gold_items[i] for i in range(len(gold_items)) \n",
    "                               if i not in matched_gold_indices]\n",
    "        unmatched_pred_items = [pred_items[i] for i in range(len(pred_items)) \n",
    "                               if i not in matched_pred_indices]\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    unmatched_gold_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_gold_items)\n",
    "    unmatched_pred_chars = sum(len(item.get('item_text_raw', '')) \n",
    "                               for item in unmatched_pred_items)\n",
    "    \n",
    "    return {\n",
    "        'cer_strict': cer_strict,\n",
    "        'wer_strict': wer_strict,\n",
    "        'cer_standard': cer_standard,\n",
    "        'wer_standard': wer_standard,\n",
    "        'cer_letters': cer_letters,\n",
    "        'matched_gold_chars': matched_gold_chars,\n",
    "        'matched_pred_chars': matched_pred_chars,\n",
    "        'unmatched_gold_chars': unmatched_gold_chars,\n",
    "        'unmatched_pred_chars': unmatched_pred_chars,\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'matched_percentage': (matched_gold_chars / total_gold_chars * 100) if total_gold_chars else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate text quality across all pages\n",
    "print(\"Evaluating text quality...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "order_agnostic_all = []\n",
    "order_agnostic_contrib = []\n",
    "structure_aware_all = []\n",
    "structure_aware_contrib = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Order-agnostic evaluation\n",
    "    oa_all = evaluate_order_agnostic(gold_items, pred_items)\n",
    "    oa_all['page_id'] = page_id\n",
    "    order_agnostic_all.append(oa_all)\n",
    "    \n",
    "    oa_contrib = evaluate_order_agnostic(gold_items, pred_items, \n",
    "                                         item_classes=['prose', 'verse'])\n",
    "    oa_contrib['page_id'] = page_id\n",
    "    order_agnostic_contrib.append(oa_contrib)\n",
    "    \n",
    "    # Structure-aware evaluation\n",
    "    sa_all = evaluate_structure_aware(gold_items, pred_items, matches)\n",
    "    sa_all['page_id'] = page_id\n",
    "    structure_aware_all.append(sa_all)\n",
    "    \n",
    "    sa_contrib = evaluate_structure_aware(gold_items, pred_items, matches,\n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    sa_contrib['page_id'] = page_id\n",
    "    structure_aware_contrib.append(sa_contrib)\n",
    "\n",
    "# Calculate averages for order-agnostic evaluation\n",
    "avg_oa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in order_agnostic_all) / len(order_agnostic_all),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in order_agnostic_all) / len(order_agnostic_all)\n",
    "}\n",
    "\n",
    "contrib_with_content = [r for r in order_agnostic_contrib if r['gold_chars'] > 0]\n",
    "avg_oa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in contrib_with_content) / len(contrib_with_content),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in contrib_with_content) / len(contrib_with_content)\n",
    "}\n",
    "\n",
    "# Calculate averages for structure-aware evaluation\n",
    "sa_all_with_matches = [r for r in structure_aware_all if r['matched_gold_chars'] > 0]\n",
    "avg_sa_all = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_all_with_matches) / len(sa_all_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_all_with_matches) / len(sa_all_with_matches)\n",
    "}\n",
    "\n",
    "sa_contrib_with_matches = [r for r in structure_aware_contrib if r['matched_gold_chars'] > 0]\n",
    "avg_sa_contrib = {\n",
    "    'cer_strict': sum(r['cer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_strict': sum(r['wer_strict'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_standard': sum(r['cer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'wer_standard': sum(r['wer_standard'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches),\n",
    "    'cer_letters': sum(r['cer_letters'] for r in sa_contrib_with_matches) / len(sa_contrib_with_matches)\n",
    "}\n",
    "\n",
    "# Calculate total matched percentages\n",
    "total_sa_all_matched = sum(r['matched_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_gold = sum(r['total_gold_chars'] for r in structure_aware_all)\n",
    "total_sa_all_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_all)\n",
    "\n",
    "total_sa_contrib_matched = sum(r['matched_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_gold = sum(r['total_gold_chars'] for r in structure_aware_contrib)\n",
    "total_sa_contrib_unmatched = sum(r['unmatched_gold_chars'] for r in structure_aware_contrib)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORDER-AGNOSTIC EVALUATION\")\n",
    "print(\"   (Pure OCR quality, reading order irrelevant)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_strict']:.2%}  |  WER: {avg_oa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_standard']:.2%}  |  WER: {avg_oa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_all['cer_letters']:.2%}\")\n",
    "\n",
    "print(f\"\\n   Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_strict']:.2%}  |  WER: {avg_oa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_standard']:.2%}  |  WER: {avg_oa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_oa_contrib['cer_letters']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STRUCTURE-AWARE EVALUATION\")\n",
    "print(\"   (OCR quality on matched content only)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n   Matched Content - All Items:\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_strict']:.2%}  |  WER: {avg_sa_all['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_standard']:.2%}  |  WER: {avg_sa_all['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_all['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_all_matched:,} chars matched \" +\n",
    "      f\"({total_sa_all_matched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_all_unmatched:,} chars \" +\n",
    "      f\"({total_sa_all_unmatched/total_sa_all_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(f\"\\n   Matched Content - Contributions Only (prose + verse):\")\n",
    "print(f\"      Strict (with all whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_strict']:.2%}  |  WER: {avg_sa_contrib['wer_strict']:.2%}\")\n",
    "print(f\"      Standard (normalized whitespace):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_standard']:.2%}  |  WER: {avg_sa_contrib['wer_standard']:.2%}\")\n",
    "print(f\"      Letters Only (no whitespace/punctuation):\")\n",
    "print(f\"         CER: {avg_sa_contrib['cer_letters']:.2%}\")\n",
    "print(f\"      Coverage: {total_sa_contrib_matched:,} chars matched \" +\n",
    "      f\"({total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "print(f\"      Unmatched: {total_sa_contrib_unmatched:,} chars \" +\n",
    "      f\"({total_sa_contrib_unmatched/total_sa_contrib_gold*100:.1f}% of gold)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Strict: Most conservative\")\n",
    "print(\"Standard: Fair baseline - normalizes whitespace\")\n",
    "print(\"Letters Only: Most lenient - pure character recognition quality\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(f\"- Pure OCR quality (standard normalization): {avg_oa_all['cer_standard']:.2%}\")\n",
    "print(f\"- Letter recognition quality: {avg_oa_all['cer_letters']:.2%}\")\n",
    "print(f\"- Structure failures (unmatched content): {total_sa_all_unmatched/total_sa_all_gold*100:.1f}%\")\n",
    "print(f\"- Contributions:\")\n",
    "print(f\"    Standard CER: {avg_sa_contrib['cer_standard']:.2%}\")\n",
    "print(f\"    Successfully matched: {total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb143df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detailed page-by-page diagnostics...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PAGE-BY-PAGE TEXT QUALITY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "--- STRICT NORMALIZATION (preserves all whitespace) ---\n",
      "                                   page_id  cer_%  wer_%  coverage_%  subs_%  dels_%  ins_%  matched_chars  total_errors\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001   6.17   8.70        87.8    1.90    0.76   3.70           1054            67\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002   0.61   0.25       100.0    0.08    0.00   0.53           2475            15\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003  76.32  83.33         1.0    0.00    0.00  76.32             38            29\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004  17.17  20.07        99.8    9.81   15.21   1.36           5219          1377\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005  39.80  42.02        15.1    0.00   39.80   0.00            716           285\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006   2.12   5.83        78.2    6.03   10.75   9.55           3583           943\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00   0.00        64.2    0.00    0.00   0.00             43             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009   3.85   6.29        78.8   10.86    1.37   3.56           2780           439\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010   2.32  13.95        99.9   32.19   14.70   9.32           5212          2930\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013   4.71  34.89        89.9   43.53   21.51  39.08           5522          5750\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014  13.33  17.03       100.3   18.10    6.15   3.90           1155           325\n",
      "\n",
      "\n",
      "--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\n",
      "                                   page_id  cer_%  wer_%  coverage_%  subs_%  dels_%  ins_%  matched_chars  total_errors\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001   5.23   8.70        87.8    0.76    0.76   3.70           1054            55\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002   0.08   0.25       100.0    0.08    0.00   0.00           2475             2\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003  76.32  83.33         1.0    0.00    0.00  76.32             38            29\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004  16.48  20.07        99.8    5.50   15.12   5.10           5219          1342\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005  39.52  42.02        15.1    0.00   39.25   0.00            716           281\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006   1.04   5.83        78.2    5.05   10.69   9.55           3583           906\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00   0.00        64.2    0.00    0.00   0.00             43             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009   2.79   6.29        78.8    8.13    0.11   3.53           2780           327\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010   2.25  13.95        99.9   31.50   14.68   9.32           5212          2893\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012   0.00   0.00         0.0    0.00    0.00   0.00              0             0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013   4.48  34.89        89.9   83.01   13.35  52.75           5522          8234\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014  12.57  17.03       100.3    5.11    5.54   3.90           1155           168\n",
      "\n",
      "\n",
      "--- LETTERS ONLY (no whitespace/punctuation) ---\n",
      "                                   page_id  cer_%  coverage_%  subs_%  dels_%  ins_%\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001   5.01        87.8    0.76    0.00   2.94\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002   0.10       100.0    0.08    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003  75.00         1.0    0.00    0.00  63.16\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004  15.96        99.8    0.69   11.78   0.04\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005  39.22        15.1    0.00   31.01   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006   0.75        78.2    0.67    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007   0.00        64.2    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008   0.00         0.0    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009   3.14        78.8   10.32    0.00   0.11\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010   1.47        99.9    1.86    0.77   1.02\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011   0.00         0.0    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012   0.00         0.0    0.00    0.00   0.00\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013   2.37        89.9    2.66    0.22   0.04\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014   8.93       100.3    3.46    4.42   0.00\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WORST PERFORMING PAGES (Standard Normalization)\n",
      "================================================================================\n",
      "\n",
      "1. La_Plume_bpt6k1185893k_1_10_1889__page-003\n",
      "   CER: 76.32%  |  WER: 83.33%\n",
      "   Coverage: 1.0% of gold text\n",
      "   Errors: 0 subs, 0 dels, 29 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 76.32%):\n",
      "         Gold: \"LA PLUME\n",
      "Revue LittÃ©raire & Artistique\"\n",
      "         Pred: \"LA PLUME\n",
      "Revue LittÃ©raire & Artistique\n",
      "NUMÃ‰RO 10\n",
      "1er SEPTEMBRE 1889\"\n",
      "\n",
      "2. La_Plume_bpt6k1185893k_1_10_1889__page-005\n",
      "   CER: 39.52%  |  WER: 42.02%\n",
      "   Coverage: 15.1% of gold text\n",
      "   Errors: 0 subs, 281 dels, 0 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (verse, CER: 39.52%):\n",
      "         Gold: \"LES BONNES SOUVENANCES\n",
      "\n",
      "Parmi les marronniers, parmi les\n",
      "Lilas blancs, les lilas violets,\n",
      "La villa d...\"\n",
      "         Pred: \"Parmi les marronniers, parmi les\n",
      "Lilas blancs, les lilas violets,\n",
      "La villa de houblon s'enguirlande,...\"\n",
      "\n",
      "3. La_Plume_bpt6k1185893k_1_10_1889__page-004\n",
      "   CER: 16.48%  |  WER: 20.07%\n",
      "   Coverage: 99.8% of gold text\n",
      "   Errors: 287 subs, 789 dels, 266 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (prose, CER: 21.90%):\n",
      "         Gold: \"d'art : de grÃ¢ce, n'imprimons plus que des chefs-d'Å“uvre !...\n",
      "\n",
      "(Le mien, mon chef-d'Å“uvre, publiÃ© de...\"\n",
      "         Pred: \"d'art : de grÃ¢ce, n'imprimons plus que des chefs-d'Å“uvre !... (Le mien, mon chef-d'Å“uvre, publiÃ© dep...\"\n",
      "      Item 2 (verse, CER: 2.92%):\n",
      "         Gold: \"LES INGÃ‰NUS\n",
      "\n",
      "Les hauts talons luttaient avec les longues jupes.\n",
      "En sorte que, selon le terrain et le...\"\n",
      "         Pred: \"LES INGÃ‰NUS Les hauts talons luttaient avec les longues jupes. En sorte que, selon le terrain et le ...\"\n",
      "\n",
      "4. La_Plume_bpt6k1185893k_1_10_1889__page-014\n",
      "   CER: 12.57%  |  WER: 17.03%\n",
      "   Coverage: 100.3% of gold text\n",
      "   Errors: 59 subs, 64 dels, 45 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (ad, CER: 22.15%):\n",
      "         Gold: \"En vente aux bureaux de LA PLUME\n",
      "\n",
      "Å’UVRES DE LÃ‰ON DESCHAMPS\n",
      "\n",
      "A LA GUEULE DU MONSTRE, poÃ©sies, un vol....\"\n",
      "         Pred: \"A LA GUEULE DU MONSTRE, poÃ©sies, un vol. in-18, vÃ©lin teintÃ©. (A. Dupret, Ã©dit.) ......................\"\n",
      "      Item 2 (paratext, CER: 7.18%):\n",
      "         Gold: \"LA PLUME, revue littÃ©raire et artistique\n",
      "\n",
      "BULLETIN Dâ€™ABONNEMENT\n",
      "\n",
      "Je soussignÃ©\n",
      "demeurant Ã \n",
      "dÃ©clare so...\"\n",
      "         Pred: \"LA PLUME, revue littÃ©raire et artistique\n",
      "BULLETIN Dâ€™ABONNEMENT\n",
      "Je soussignÃ©\n",
      "Demeurant Ã \n",
      "DÃ©clare\n",
      "Sous...\"\n",
      "\n",
      "5. La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "   CER: 5.23%  |  WER: 8.70%\n",
      "   Coverage: 87.8% of gold text\n",
      "   Errors: 8 subs, 8 dels, 39 ins\n",
      "\n",
      "   Worst items on this page:\n",
      "      Item 1 (paratext, CER: 68.42%):\n",
      "         Gold: \"RÃ‰DACTION ET ADMINISTRATION\n",
      "36, Boulevard Arago, 36\n",
      "PARIS\"\n",
      "         Pred: \"RÃ‰DACTION ET ADMINISTRATION\n",
      "36, Boulevard Arago, 36\n",
      "PARIS\n",
      "Directeur de la Revue : LÃ©on DESCHAMPS\"\n",
      "      Item 2 (paratext, CER: 1.79%):\n",
      "         Gold: \"SOMMAIRE :\n",
      "\n",
      "Texte :\n",
      "LÃ©on DESCHAMPS : LÃ©on Vanier. â€” Paul VERLAINE : Les IngÃ©nus. â€” StÃ©phane MALLARMÃ‰...\"\n",
      "         Pred: \"SOMMAIRE\n",
      "Texte :\n",
      "LÃ©on DESCHAMPS : LÃ©on Vanier. â€” Paul VERLAINE : Les IngÃ©nus. â€” StÃ©phane MALLARMÃ‰ : ...\"\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ERROR TYPE DISTRIBUTION (Standard Normalization)\n",
      "================================================================================\n",
      "\n",
      "Total errors across all pages: 14,237\n",
      "   Substitutions: 6,989 (49.1%)\n",
      "   Deletions:     3,030 (21.3%)\n",
      "   Insertions:    4,218 (29.6%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Average CER (standard): 11.48%\n",
      "- Pages with CER > 20%: 2\n",
      "- Pages with CER < 5%: 9\n",
      "- Most common error type: Substitutions\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page-by-Page Text Diagnostics\n",
    "Detailed error analysis for each page with three normalization levels.\n",
    "Shows error type distribution, worst performing pages, and actual text examples.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def get_levenshtein_operations(reference: str, hypothesis: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get detailed Levenshtein operations breakdown.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with counts of substitutions, deletions, insertions\n",
    "    \"\"\"\n",
    "    if not reference and not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': 0, 'total': 0}\n",
    "    \n",
    "    if not reference:\n",
    "        return {'substitutions': 0, 'deletions': 0, 'insertions': len(hypothesis), 'total': len(hypothesis)}\n",
    "    \n",
    "    if not hypothesis:\n",
    "        return {'substitutions': 0, 'deletions': len(reference), 'insertions': 0, 'total': len(reference)}\n",
    "    \n",
    "    # Use SequenceMatcher to get operations\n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    substitutions = 0\n",
    "    deletions = 0\n",
    "    insertions = 0\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Both strings differ - count as substitutions\n",
    "            substitutions += max(i2 - i1, j2 - j1)\n",
    "        elif tag == 'delete':\n",
    "            # Only in reference\n",
    "            deletions += (i2 - i1)\n",
    "        elif tag == 'insert':\n",
    "            # Only in hypothesis\n",
    "            insertions += (j2 - j1)\n",
    "    \n",
    "    return {\n",
    "        'substitutions': substitutions,\n",
    "        'deletions': deletions,\n",
    "        'insertions': insertions,\n",
    "        'total': substitutions + deletions + insertions\n",
    "    }\n",
    "\n",
    "\n",
    "def diagnose_page_text_quality(page: Dict, normalization: str = 'standard') -> Dict:\n",
    "    \"\"\"\n",
    "    Detailed text quality diagnosis for a single page.\n",
    "    \n",
    "    Args:\n",
    "        page: Page data from all_pages\n",
    "        normalization: 'strict', 'standard', or 'letters_only'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detailed metrics and error breakdowns\n",
    "    \"\"\"\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Get matched pairs\n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "        return {\n",
    "            'page_id': page_id,\n",
    "            'cer': 0.0,\n",
    "            'wer': 0.0,\n",
    "            'matched_chars': 0,\n",
    "            'total_gold_chars': total_gold_chars,\n",
    "            'match_coverage': 0.0,\n",
    "            'substitutions': 0,\n",
    "            'deletions': 0,\n",
    "            'insertions': 0,\n",
    "            'total_errors': 0,\n",
    "            'items_analyzed': []\n",
    "        }\n",
    "    \n",
    "    # Concatenate matched text\n",
    "    gold_text = ' '.join(gold_item.get('item_text_raw', '') for gold_item, _, _ in matched_pairs)\n",
    "    pred_text = ' '.join(pred_item.get('item_text_raw', '') for _, pred_item, _ in matched_pairs)\n",
    "    \n",
    "    # Calculate CER/WER\n",
    "    cer = character_error_rate(gold_text, pred_text, normalization)\n",
    "    wer = word_error_rate(gold_text, pred_text, normalization)\n",
    "    \n",
    "    # Get error breakdown using normalized text\n",
    "    if normalization == 'strict':\n",
    "        gold_norm = normalize_text_strict(gold_text)\n",
    "        pred_norm = normalize_text_strict(pred_text)\n",
    "    elif normalization == 'standard':\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "    else:  # letters_only\n",
    "        gold_norm = normalize_text_letters_only(gold_text)\n",
    "        pred_norm = normalize_text_letters_only(pred_text)\n",
    "    \n",
    "    ops = get_levenshtein_operations(gold_norm, pred_norm)\n",
    "    \n",
    "    # Analyze individual items\n",
    "    items_analyzed = []\n",
    "    for gold_item, pred_item, similarity in matched_pairs:\n",
    "        gold_item_text = gold_item.get('item_text_raw', '')\n",
    "        pred_item_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        item_cer = character_error_rate(gold_item_text, pred_item_text, normalization)\n",
    "        \n",
    "        items_analyzed.append({\n",
    "            'gold_class': gold_item.get('item_class'),\n",
    "            'cer': item_cer,\n",
    "            'gold_preview': gold_item_text[:100],\n",
    "            'pred_preview': pred_item_text[:100],\n",
    "            'gold_length': len(gold_item_text),\n",
    "            'pred_length': len(pred_item_text)\n",
    "        })\n",
    "    \n",
    "    total_gold_chars = sum(len(item.get('item_text_raw', '')) for item in gold_items)\n",
    "    \n",
    "    return {\n",
    "        'page_id': page_id,\n",
    "        'cer': cer,\n",
    "        'wer': wer,\n",
    "        'matched_chars': len(gold_text),\n",
    "        'total_gold_chars': total_gold_chars,\n",
    "        'match_coverage': len(gold_text) / total_gold_chars * 100 if total_gold_chars > 0 else 0,\n",
    "        'substitutions': ops['substitutions'],\n",
    "        'deletions': ops['deletions'],\n",
    "        'insertions': ops['insertions'],\n",
    "        'total_errors': ops['total'],\n",
    "        'items_analyzed': items_analyzed\n",
    "    }\n",
    "\n",
    "\n",
    "# Diagnose all pages for all three normalizations\n",
    "print(\"Running detailed page-by-page diagnostics...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_diagnostics_strict = []\n",
    "page_diagnostics_standard = []\n",
    "page_diagnostics_letters = []\n",
    "\n",
    "for page in all_pages:\n",
    "    diag_strict = diagnose_page_text_quality(page, 'strict')\n",
    "    page_diagnostics_strict.append(diag_strict)\n",
    "    \n",
    "    diag_standard = diagnose_page_text_quality(page, 'standard')\n",
    "    page_diagnostics_standard.append(diag_standard)\n",
    "    \n",
    "    diag_letters = diagnose_page_text_quality(page, 'letters_only')\n",
    "    page_diagnostics_letters.append(diag_letters)\n",
    "\n",
    "# Create summary DataFrames\n",
    "def create_summary_df(diagnostics, normalization_name):\n",
    "    \"\"\"Create summary DataFrame from diagnostics.\"\"\"\n",
    "    data = []\n",
    "    for d in diagnostics:\n",
    "        if d['matched_chars'] > 0:\n",
    "            sub_pct = d['substitutions'] / d['matched_chars'] * 100\n",
    "            del_pct = d['deletions'] / d['matched_chars'] * 100\n",
    "            ins_pct = d['insertions'] / d['matched_chars'] * 100\n",
    "        else:\n",
    "            sub_pct = del_pct = ins_pct = 0\n",
    "        \n",
    "        data.append({\n",
    "            'page_id': d['page_id'],\n",
    "            'cer_%': round(d['cer'] * 100, 2),\n",
    "            'wer_%': round(d['wer'] * 100, 2),\n",
    "            'coverage_%': round(d['match_coverage'], 1),\n",
    "            'subs_%': round(sub_pct, 2),\n",
    "            'dels_%': round(del_pct, 2),\n",
    "            'ins_%': round(ins_pct, 2),\n",
    "            'matched_chars': d['matched_chars'],\n",
    "            'total_errors': d['total_errors']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_strict = create_summary_df(page_diagnostics_strict, 'Strict')\n",
    "df_standard = create_summary_df(page_diagnostics_standard, 'Standard')\n",
    "df_letters = create_summary_df(page_diagnostics_letters, 'Letters Only')\n",
    "\n",
    "# Print summary tables\n",
    "print(\"=\"*80)\n",
    "print(\"PAGE-BY-PAGE TEXT QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- STRICT NORMALIZATION (preserves all whitespace) ---\")\n",
    "print(df_strict.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- STANDARD NORMALIZATION (normalized whitespace - RECOMMENDED) ---\")\n",
    "print(df_standard.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n--- LETTERS ONLY (no whitespace/punctuation) ---\")\n",
    "print(df_letters[['page_id', 'cer_%', 'coverage_%', 'subs_%', 'dels_%', 'ins_%']].to_string(index=False))\n",
    "\n",
    "# Identify worst pages (using standard normalization)\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"WORST PERFORMING PAGES (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "worst_pages = sorted(page_diagnostics_standard, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "for i, page_diag in enumerate(worst_pages, 1):\n",
    "    print(f\"\\n{i}. {page_diag['page_id']}\")\n",
    "    print(f\"   CER: {page_diag['cer']:.2%}  |  WER: {page_diag['wer']:.2%}\")\n",
    "    print(f\"   Coverage: {page_diag['match_coverage']:.1f}% of gold text\")\n",
    "    print(f\"   Errors: {page_diag['substitutions']} subs, {page_diag['deletions']} dels, {page_diag['insertions']} ins\")\n",
    "    \n",
    "    # Show worst items from this page\n",
    "    if page_diag['items_analyzed']:\n",
    "        worst_items = sorted(page_diag['items_analyzed'], key=lambda x: x['cer'], reverse=True)[:2]\n",
    "        print(f\"\\n   Worst items on this page:\")\n",
    "        for j, item in enumerate(worst_items, 1):\n",
    "            print(f\"      Item {j} ({item['gold_class']}, CER: {item['cer']:.2%}):\")\n",
    "            print(f\"         Gold: \\\"{item['gold_preview']}{'...' if item['gold_length'] > 100 else ''}\\\"\")\n",
    "            print(f\"         Pred: \\\"{item['pred_preview']}{'...' if item['pred_length'] > 100 else ''}\\\"\")\n",
    "\n",
    "# Error distribution analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ERROR TYPE DISTRIBUTION (Standard Normalization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_errors = sum(d['total_errors'] for d in page_diagnostics_standard)\n",
    "total_subs = sum(d['substitutions'] for d in page_diagnostics_standard)\n",
    "total_dels = sum(d['deletions'] for d in page_diagnostics_standard)\n",
    "total_ins = sum(d['insertions'] for d in page_diagnostics_standard)\n",
    "\n",
    "print(f\"\\nTotal errors across all pages: {total_errors:,}\")\n",
    "print(f\"   Substitutions: {total_subs:,} ({total_subs/total_errors*100:.1f}%)\")\n",
    "print(f\"   Deletions:     {total_dels:,} ({total_dels/total_errors*100:.1f}%)\")\n",
    "print(f\"   Insertions:    {total_ins:,} ({total_ins/total_errors*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Average CER (standard): {df_standard['cer_%'].mean():.2f}%\")\n",
    "print(f\"- Pages with CER > 20%: {len(df_standard[df_standard['cer_%'] > 20])}\")\n",
    "print(f\"- Pages with CER < 5%: {len(df_standard[df_standard['cer_%'] < 5])}\")\n",
    "print(f\"- Most common error type: \" + \n",
    "      (\"Substitutions\" if total_subs > max(total_dels, total_ins) else \n",
    "       \"Deletions\" if total_dels > total_ins else \"Insertions\"))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b21705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing character-level confusions across all pages...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CHARACTER CONFUSION MATRIX\n",
      "================================================================================\n",
      "\n",
      "Total character substitutions: 130\n",
      "Unique confusion pairs: 115\n",
      "\n",
      "Top 30 Most Common Character Substitutions:\n",
      "Gold â†’ Pred                    Count     \n",
      "--------------------------------------------------------------------------------\n",
      "\" ; E\" â†’ \"; Ã‰\"                 8         \n",
      "'a' â†’ 'Ã '                      3         \n",
      "'E' â†’ 'Ã‰'                      2         \n",
      "'Ã®' â†’ 'i'                      2         \n",
      "'e' â†’ 'Ã¨'                      2         \n",
      "\"t) \" â†’ \"l)\"                   2         \n",
      "\".)\" â†’ \")\"                     2         \n",
      "'d' â†’ 'D'                      2         \n",
      "\"ance\" â†’ \"ir\"                  1         \n",
      "'l' â†’ 'L'                      1         \n",
      "'C' â†’ 'G'                      1         \n",
      "\"de\" â†’ \"DE\"                    1         \n",
      "'Ã©' â†’ 'e'                      1         \n",
      "\" dÃ©\" â†’ \"dÃ©- \"                 1         \n",
      "'e' â†’ 'Ã©'                      1         \n",
      "\"nec\" â†’ \"ce\"                   1         \n",
      "'i' â†’ 'I'                      1         \n",
      "\"Ã¨res luttes lit\" â†’ \"eres luttes lit- \" 1         \n",
      "\"y\" â†’ \"vous\"                   1         \n",
      "\"que\" â†’ \"ces- \"                1         \n",
      "\"t ce rÃ©gal comblait nos jeunes yeux de fous. \" â†’ \"R\" 1         \n",
      "\"e soir tombait, un soir Ã©quivoque d'automne : Les belles, se pendant rÃªveuses Ã  nos bras, Dirent alors des mots si spÃ©cieux, tout bas, Que notre Ã¢me depuis ce temps tremble et s'Ã©tonne. Paul Verlaine\" â†’ \"AINE\" 1         \n",
      "'t' â†’ 'l'                      1         \n",
      "\"Cent affic\" â†’ \"Si discord parmi l'exaltation de l'\" 1         \n",
      "\"s s'assimilant l'or incompris des\" â†’ \"ure, un cri faussa ce nom connu pour dÃ©ployer la conti- nitÃ© de cimes tard Ã©vanouies, Fontainebleau, que\" 1         \n",
      "\"ours, trahison de la lettre, ont fui, comm\" â†’ \"e pensai, la glace du compartiment vio- lentÃ©e, du poing aussi Ã©treindr\" 1         \n",
      "\"tous confins de la \" â†’ \"la gorge l'interrupteur : Tais-toi ! ne di\" 1         \n",
      "\"ille, mes ye\" â†’ \"ulgue pas du fait d'un aboi indiffÃ©rent l'ombre ici insinuÃ©e dans mon esprit, a\" 1         \n",
      "\"au ras de l'horizon par un dÃ©part sur le rail traÃ®nÃ©s a\" â†’ \"portiÃ¨res de wagons battant sous un \" 1         \n",
      "\"ant de se recueillir dans l'abstruse fiertÃ© que donne une approche de forÃªt en son temps d'apothÃ©ose. Si discord parmi l'exaltation de l'heure, un cri faussa ce nom connu pour dÃ©ployer la continuitÃ© de cimes tard Ã©vanouies, Fontainebleau, que je pensai, la \" â†’ \"ent inspirÃ© et Ã©\" 1         \n",
      "\n",
      "\n",
      "================================================================================\n",
      "SYSTEMATIC ERROR PATTERNS\n",
      "================================================================================\n",
      "\n",
      "Total confusions: 130\n",
      "Categorized: 85 (65.4%)\n",
      "Uncategorized: 45 (34.6%)\n",
      "\n",
      "Pattern Breakdown:\n",
      "   Space Issues                  64 ( 49.2%)\n",
      "   Case Errors                    9 (  6.9%)\n",
      "   Punctuation Errors             6 (  4.6%)\n",
      "   Accent Removal                 3 (  2.3%)\n",
      "   Accent Confusion               2 (  1.5%)\n",
      "   Ligature Issues                1 (  0.8%)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ACCENT & DIACRITIC ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Accented character confusions: 5\n",
      "\n",
      "Most common accented character errors:\n",
      "   'Ã®' â†’ 'i': 2 times\n",
      "   'Ã©' â†’ 'e': 1 times\n",
      "   'Ã¨' â†’ 'Ã©': 1 times\n",
      "   'Ã ' â†’ 'Ã€': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LIGATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Ligature-related confusions: 1\n",
      "\n",
      "Ligature substitutions:\n",
      "   'Å“' â†’ 'oe': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CASE SENSITIVITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Case-only differences: 10\n",
      "\n",
      "Most common case errors:\n",
      "   'd' â†’ 'D': 2 times\n",
      "   'l' â†’ 'L': 1 times\n",
      "   'i' â†’ 'I': 1 times\n",
      "   'P' â†’ 'p': 1 times\n",
      "   'M' â†’ 'm': 1 times\n",
      "   'D' â†’ 'd': 1 times\n",
      "   'F' â†’ 'f': 1 times\n",
      "   's' â†’ 'S': 1 times\n",
      "   'Ã ' â†’ 'Ã€': 1 times\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Based on the error analysis:\n",
      "\n",
      "âš  CASE SENSITIVITY ISSUES (6.9%)\n",
      "   - Model confusing upper/lowercase\n",
      "   - May indicate line/title detection problems\n",
      "\n",
      "âœ“ ERROR DIVERSITY IS HIGH (unique ratio: 0.88)\n",
      "   - Errors are diverse, not systematic\n",
      "   - Suggests random OCR noise rather than systematic bias\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cross-Page Error Analysis\n",
    "Character-level confusion matrix and systematic error pattern detection.\n",
    "Analyzes all pages together to identify recurring OCR issues.\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def get_character_confusions(reference: str, hypothesis: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract character-level substitutions from aligned strings.\n",
    "    \n",
    "    Returns:\n",
    "        List of (gold_char, pred_char) tuples for substitutions\n",
    "    \"\"\"\n",
    "    confusions = []\n",
    "    \n",
    "    sm = SequenceMatcher(None, reference, hypothesis)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            # Character substitution\n",
    "            gold_substr = reference[i1:i2]\n",
    "            pred_substr = hypothesis[j1:j2]\n",
    "            \n",
    "            # For single character replacements\n",
    "            if len(gold_substr) == 1 and len(pred_substr) == 1:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "            # For multi-character replacements (like Å“ -> oe)\n",
    "            elif len(gold_substr) > 0 and len(pred_substr) > 0:\n",
    "                confusions.append((gold_substr, pred_substr))\n",
    "    \n",
    "    return confusions\n",
    "\n",
    "\n",
    "def analyze_character_patterns(confusions: list) -> dict:\n",
    "    \"\"\"\n",
    "    Detect systematic patterns in character confusions.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with pattern names and counts\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'accent_removal': 0,\n",
    "        'accent_confusion': 0,\n",
    "        'ligature_issues': 0,\n",
    "        'case_errors': 0,\n",
    "        'punctuation_errors': 0,\n",
    "        'similar_shape': 0,\n",
    "        'space_issues': 0\n",
    "    }\n",
    "    \n",
    "    accent_chars = 'Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´ÃµÃ¶Ã¹ÃºÃ»Ã¼Ã½Ã¿Ã±Ã§Ã€ÃÃ‚ÃƒÃ„Ã…ÃˆÃ‰ÃŠÃ‹ÃŒÃÃŽÃÃ’Ã“Ã”Ã•Ã–Ã™ÃšÃ›ÃœÃÅ¸Ã‘Ã‡'\n",
    "    ligatures = 'Å“Ã¦Å’Ã†'\n",
    "    \n",
    "    for gold, pred in confusions:\n",
    "        # Accent removal (Ã© -> e, Ã  -> a)\n",
    "        if len(gold) == 1 and len(pred) == 1:\n",
    "            gold_base = unicodedata.normalize('NFD', gold)[0]\n",
    "            pred_normalized = unicodedata.normalize('NFD', pred)[0]\n",
    "            if gold in accent_chars and gold_base == pred:\n",
    "                patterns['accent_removal'] += 1\n",
    "            elif gold in accent_chars and pred in accent_chars and gold != pred:\n",
    "                patterns['accent_confusion'] += 1\n",
    "            elif gold.lower() == pred.lower():\n",
    "                patterns['case_errors'] += 1\n",
    "        \n",
    "        # Ligature issues (Å“ -> oe, Ã¦ -> ae)\n",
    "        if gold in ligatures and pred not in ligatures:\n",
    "            patterns['ligature_issues'] += 1\n",
    "        \n",
    "        # Similar shape confusions (common OCR errors)\n",
    "        similar_pairs = [\n",
    "            ('l', 'i'), ('i', 'l'), ('rn', 'm'), ('m', 'rn'),\n",
    "            ('cl', 'd'), ('d', 'cl'), ('o', '0'), ('0', 'o'),\n",
    "            ('1', 'l'), ('l', '1'), ('s', '5'), ('5', 's')\n",
    "        ]\n",
    "        if (gold, pred) in similar_pairs:\n",
    "            patterns['similar_shape'] += 1\n",
    "        \n",
    "        # Punctuation confusion\n",
    "        if gold in '.,;:!?\\'\"' or pred in '.,;:!?\\'\"':\n",
    "            patterns['punctuation_errors'] += 1\n",
    "        \n",
    "        # Space-related issues\n",
    "        if ' ' in gold or ' ' in pred:\n",
    "            patterns['space_issues'] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Collect all character confusions across all pages\n",
    "print(\"Analyzing character-level confusions across all pages...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_confusions = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        pred_text = pred_item.get('item_text_raw', '')\n",
    "        \n",
    "        # Use standard normalization for fair comparison\n",
    "        gold_norm = normalize_text_standard(gold_text)\n",
    "        pred_norm = normalize_text_standard(pred_text)\n",
    "        \n",
    "        confusions = get_character_confusions(gold_norm, pred_norm)\n",
    "        all_confusions.extend(confusions)\n",
    "\n",
    "# Count confusion frequencies\n",
    "confusion_counter = Counter(all_confusions)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHARACTER CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal character substitutions: {len(all_confusions):,}\")\n",
    "print(f\"Unique confusion pairs: {len(confusion_counter):,}\")\n",
    "\n",
    "# Top 30 most common confusions\n",
    "print(\"\\nTop 30 Most Common Character Substitutions:\")\n",
    "print(f\"{'Gold â†’ Pred':<30} {'Count':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for (gold, pred), count in confusion_counter.most_common(30):\n",
    "    # Escape special characters for display\n",
    "    gold_display = repr(gold)[1:-1] if gold in '\\n\\t\\r' else gold\n",
    "    pred_display = repr(pred)[1:-1] if pred in '\\n\\t\\r' else pred\n",
    "    \n",
    "    # Create display string\n",
    "    if len(gold) == 1 and len(pred) == 1:\n",
    "        display = f\"'{gold_display}' â†’ '{pred_display}'\"\n",
    "    else:\n",
    "        display = f'\"{gold_display}\" â†’ \"{pred_display}\"'\n",
    "    \n",
    "    print(f\"{display:<30} {count:<10}\")\n",
    "\n",
    "# Pattern analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC ERROR PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "patterns = analyze_character_patterns(all_confusions)\n",
    "total_categorized = sum(patterns.values())\n",
    "\n",
    "print(f\"\\nTotal confusions: {len(all_confusions):,}\")\n",
    "print(f\"Categorized: {total_categorized:,} ({total_categorized/len(all_confusions)*100:.1f}%)\")\n",
    "print(f\"Uncategorized: {len(all_confusions) - total_categorized:,} \" +\n",
    "      f\"({(len(all_confusions) - total_categorized)/len(all_confusions)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPattern Breakdown:\")\n",
    "for pattern, count in sorted(patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        pct = count / len(all_confusions) * 100\n",
    "        pattern_name = pattern.replace('_', ' ').title()\n",
    "        print(f\"   {pattern_name:<25} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Specific accent analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ACCENT & DIACRITIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accent_confusions = [(g, p) for g, p in all_confusions \n",
    "                     if len(g) == 1 and len(p) == 1 \n",
    "                     and any(c in 'Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´ÃµÃ¶Ã¹ÃºÃ»Ã¼Ã½Ã¿Ã±Ã§Ã€ÃÃ‚ÃƒÃ„Ã…ÃˆÃ‰ÃŠÃ‹ÃŒÃÃŽÃÃ’Ã“Ã”Ã•Ã–Ã™ÃšÃ›ÃœÃÅ¸Ã‘Ã‡' for c in g)]\n",
    "\n",
    "if accent_confusions:\n",
    "    accent_counter = Counter(accent_confusions)\n",
    "    print(f\"\\nAccented character confusions: {len(accent_confusions):,}\")\n",
    "    print(\"\\nMost common accented character errors:\")\n",
    "    for (gold, pred), count in accent_counter.most_common(15):\n",
    "        print(f\"   '{gold}' â†’ '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo accented character confusions detected.\")\n",
    "\n",
    "# Ligature analysis  \n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"LIGATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ligature_confusions = [(g, p) for g, p in all_confusions if g in 'Å“Ã¦Å’Ã†' or p in 'Å“Ã¦Å’Ã†']\n",
    "\n",
    "if ligature_confusions:\n",
    "    ligature_counter = Counter(ligature_confusions)\n",
    "    print(f\"\\nLigature-related confusions: {len(ligature_confusions):,}\")\n",
    "    print(\"\\nLigature substitutions:\")\n",
    "    for (gold, pred), count in ligature_counter.most_common(10):\n",
    "        print(f\"   '{gold}' â†’ '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo ligature confusions detected.\")\n",
    "\n",
    "# Case sensitivity analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CASE SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "case_confusions = [(g, p) for g, p in all_confusions \n",
    "                   if len(g) == 1 and len(p) == 1 and g.lower() == p.lower() and g != p]\n",
    "\n",
    "if case_confusions:\n",
    "    case_counter = Counter(case_confusions)\n",
    "    print(f\"\\nCase-only differences: {len(case_confusions):,}\")\n",
    "    print(\"\\nMost common case errors:\")\n",
    "    for (gold, pred), count in case_counter.most_common(10):\n",
    "        print(f\"   '{gold}' â†’ '{pred}': {count} times\")\n",
    "else:\n",
    "    print(\"\\nNo case-only confusions detected.\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBased on the error analysis:\")\n",
    "\n",
    "# Check for high accent issues\n",
    "accent_pct = patterns['accent_removal'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if accent_pct > 5:\n",
    "    print(f\"\\nâš  HIGH ACCENT REMOVAL RATE ({accent_pct:.1f}%)\")\n",
    "    print(\"   - Consider post-processing to restore accents using dictionary lookup\")\n",
    "    print(\"   - May need model fine-tuning on accented French text\")\n",
    "\n",
    "# Check for ligature issues\n",
    "ligature_pct = patterns['ligature_issues'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if ligature_pct > 2:\n",
    "    print(f\"\\nâš  LIGATURE HANDLING ISSUES ({ligature_pct:.1f}%)\")\n",
    "    print(\"   - Ligatures (Å“, Ã¦) being split or confused\")\n",
    "    print(\"   - Common in historical French texts\")\n",
    "\n",
    "# Check for case errors\n",
    "case_pct = patterns['case_errors'] / len(all_confusions) * 100 if all_confusions else 0\n",
    "if case_pct > 3:\n",
    "    print(f\"\\nâš  CASE SENSITIVITY ISSUES ({case_pct:.1f}%)\")\n",
    "    print(\"   - Model confusing upper/lowercase\")\n",
    "    print(\"   - May indicate line/title detection problems\")\n",
    "\n",
    "# General observation\n",
    "if len(all_confusions) > 0:\n",
    "    unique_ratio = len(confusion_counter) / len(all_confusions)\n",
    "    if unique_ratio > 0.5:\n",
    "        print(f\"\\nâœ“ ERROR DIVERSITY IS HIGH (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Errors are diverse, not systematic\")\n",
    "        print(\"   - Suggests random OCR noise rather than systematic bias\")\n",
    "    else:\n",
    "        print(f\"\\nâš  ERROR CONCENTRATION DETECTED (unique ratio: {unique_ratio:.2f})\")\n",
    "        print(\"   - Same errors repeat frequently\")\n",
    "        print(\"   - Suggests systematic model bias that could be corrected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating classification accuracy...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OVERALL CLASSIFICATION METRICS\n",
      "================================================================================\n",
      "\n",
      "Total matched items evaluated: 37\n",
      "Correctly classified: 25 (67.57%)\n",
      "Misclassified: 12 (32.43%)\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "--------------------------------------------------------------------------------\n",
      "                 prose     verse        ad  paratext   unknown\n",
      "--------------------------------------------------------------------------------\n",
      "       prose         7         1         0         0         0\n",
      "       verse         0         7         0         0         0\n",
      "          ad         1         0         0         0         0\n",
      "    paratext        10         0         0        11         0\n",
      "     unknown         0         0         0         0         0\n",
      "\n",
      "\n",
      "PER-CLASS METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "Class        Precision    Recall       F1-Score     Support     \n",
      "--------------------------------------------------------------------------------\n",
      "prose        38.89%       87.50%       0.538        8           \n",
      "verse        87.50%       100.00%      0.933        7           \n",
      "ad           0.00%        0.00%        0.000        1           \n",
      "paratext     100.00%      52.38%       0.688        21          \n",
      "unknown      0.00%        0.00%        0.000        0           \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg    45.28%       47.98%       0.432        37          \n",
      "Weighted Avg 81.72%       67.57%       0.683        37          \n",
      "\n",
      "\n",
      "MOST COMMON MISCLASSIFICATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "Gold â†’ Predicted               Count      % of Gold Class\n",
      "--------------------------------------------------------------------------------\n",
      "paratext â†’ prose                10         47.6%\n",
      "prose â†’ verse                1          12.5%\n",
      "ad â†’ prose                1          100.0%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTRIBUTIONS ANALYSIS (Prose + Verse)\n",
      "================================================================================\n",
      "\n",
      "Total contribution items: 15\n",
      "Correctly classified: 14 (93.33%)\n",
      "Misclassified: 1 (6.67%)\n",
      "\n",
      "Contributions Confusion Matrix:\n",
      "                 prose     verse\n",
      "----------------------------------------\n",
      "       prose         7         1\n",
      "       verse         0         7\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PER-PAGE CLASSIFICATION BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "                                   page_id  total_items  correct  accuracy_%  misclassified\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001            4        4       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002            2        1        50.0              1\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003            1        1       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004            3        3       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005            1        1       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006            3        3       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007            1        1       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008            0        0         0.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009            4        3        75.0              1\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010            4        4       100.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011            0        0         0.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012            0        0         0.0              0\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013           10        1        10.0              9\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014            4        3        75.0              1\n",
      "\n",
      "\n",
      "DETAILED PER-PAGE CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (4/4)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     4/4 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 50.00% (1/2)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/2 correct\n",
      "\n",
      "Misclassifications (1):\n",
      "   paratext â†’ prose: 1 time\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (3/3)\n",
      "\n",
      "Class distribution:\n",
      "   prose        2/2 correct\n",
      "   verse        1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   verse        1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (3/3)\n",
      "\n",
      "Class distribution:\n",
      "   prose        2/2 correct\n",
      "   verse        1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (1/1)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/1 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 75.00% (3/4)\n",
      "\n",
      "Class distribution:\n",
      "   prose        1/2 correct\n",
      "   verse        2/2 correct\n",
      "\n",
      "Misclassifications (1):\n",
      "   prose â†’ verse: 1 time\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 100.00% (4/4)\n",
      "\n",
      "Class distribution:\n",
      "   prose        2/2 correct\n",
      "   verse        2/2 correct\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 10.00% (1/10)\n",
      "\n",
      "Class distribution:\n",
      "   paratext     1/10 correct\n",
      "\n",
      "Misclassifications (9):\n",
      "   paratext â†’ prose: 9 times\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 75.00% (3/4)\n",
      "\n",
      "Class distribution:\n",
      "   ad           0/1 correct\n",
      "   paratext     3/3 correct\n",
      "\n",
      "Misclassifications (1):\n",
      "   ad â†’ prose: 1 time\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Overall classification accuracy: 67.57%\n",
      "- Best performing class: verse\n",
      "- Most challenging class: ad\n",
      "- Most common confusion: paratext â†’ prose (10 times)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classification Accuracy Evaluation\n",
    "Evaluate how well the model classifies items into the five categories:\n",
    "prose, verse, ad, paratext, unknown\n",
    "\n",
    "Structure:\n",
    "1. Overall classification metrics across all pages\n",
    "2. Per-page classification breakdown\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_classification(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                           matches: List[Tuple[int, int, float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate classification accuracy on matched pairs.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Dict with classification metrics\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return {\n",
    "            'gold_classes': [],\n",
    "            'pred_classes': [],\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'accuracy': 0.0\n",
    "        }\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    gold_classes = []\n",
    "    pred_classes = []\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_classes.append(gold_item['item_class'])\n",
    "        pred_classes.append(pred_item['item_class'])\n",
    "    \n",
    "    correct = sum(1 for g, p in zip(gold_classes, pred_classes) if g == p)\n",
    "    total = len(gold_classes)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'gold_classes': gold_classes,\n",
    "        'pred_classes': pred_classes,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Collect classification data from all pages\n",
    "print(\"Evaluating classification accuracy...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "all_gold_classes = []\n",
    "all_pred_classes = []\n",
    "page_classification_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    result = evaluate_classification(gold_items, pred_items, matches)\n",
    "    result['page_id'] = page_id\n",
    "    page_classification_results.append(result)\n",
    "    \n",
    "    all_gold_classes.extend(result['gold_classes'])\n",
    "    all_pred_classes.extend(result['pred_classes'])\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_matched = len(all_gold_classes)\n",
    "total_correct = sum(1 for g, p in zip(all_gold_classes, all_pred_classes) if g == p)\n",
    "overall_accuracy = total_correct / total_matched if total_matched > 0 else 0.0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL CLASSIFICATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal matched items evaluated: {total_matched}\")\n",
    "print(f\"Correctly classified: {total_correct} ({overall_accuracy:.2%})\")\n",
    "print(f\"Misclassified: {total_matched - total_correct} ({(1-overall_accuracy):.2%})\")\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['prose', 'verse', 'ad', 'paratext', 'unknown']\n",
    "\n",
    "# Confusion matrix\n",
    "if total_matched > 0:\n",
    "    cm = confusion_matrix(all_gold_classes, all_pred_classes, labels=class_labels)\n",
    "    \n",
    "    print(\"\\n\\nCONFUSION MATRIX\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'':>12}\", end='')\n",
    "    for label in class_labels:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, label in enumerate(class_labels):\n",
    "        print(f\"{label:>12}\", end='')\n",
    "        for j in range(len(class_labels)):\n",
    "            print(f\"{cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\n\\nPER-CLASS METRICS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, label in enumerate(class_labels):\n",
    "        # Calculate metrics for this class\n",
    "        tp = cm[i][i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        support = cm[i, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"{label:<12} {precision:<12.2%} {recall:<12.2%} {f1:<12.3f} {support:<12}\")\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    supports = []\n",
    "    \n",
    "    for i in range(len(class_labels)):\n",
    "        tp = cm[i][i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        support = cm[i, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        supports.append(support)\n",
    "    \n",
    "    macro_precision = np.mean(precisions)\n",
    "    macro_recall = np.mean(recalls)\n",
    "    macro_f1 = np.mean(f1s)\n",
    "    \n",
    "    total_support = sum(supports)\n",
    "    weighted_precision = sum(p * s for p, s in zip(precisions, supports)) / total_support\n",
    "    weighted_recall = sum(r * s for r, s in zip(recalls, supports)) / total_support\n",
    "    weighted_f1 = sum(f * s for f, s in zip(f1s, supports)) / total_support\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Macro Avg':<12} {macro_precision:<12.2%} {macro_recall:<12.2%} {macro_f1:<12.3f} {total_support:<12}\")\n",
    "    print(f\"{'Weighted Avg':<12} {weighted_precision:<12.2%} {weighted_recall:<12.2%} {weighted_f1:<12.3f} {total_support:<12}\")\n",
    "    \n",
    "    # Most common misclassifications\n",
    "    print(\"\\n\\nMOST COMMON MISCLASSIFICATIONS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    misclass_counts = []\n",
    "    for i, gold_label in enumerate(class_labels):\n",
    "        for j, pred_label in enumerate(class_labels):\n",
    "            if i != j and cm[i][j] > 0:\n",
    "                misclass_counts.append((gold_label, pred_label, cm[i][j]))\n",
    "    \n",
    "    misclass_counts.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    if misclass_counts:\n",
    "        print(f\"{'Gold â†’ Predicted':<30} {'Count':<10} {'% of Gold Class'}\")\n",
    "        print(\"-\"*80)\n",
    "        for gold_label, pred_label, count in misclass_counts[:10]:\n",
    "            gold_total = cm[class_labels.index(gold_label), :].sum()\n",
    "            pct = count / gold_total * 100 if gold_total > 0 else 0\n",
    "            print(f\"{gold_label} â†’ {pred_label:<20} {count:<10} {pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"No misclassifications detected!\")\n",
    "\n",
    "# Contributions-specific analysis\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"CONTRIBUTIONS ANALYSIS (Prose + Verse)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "contrib_gold = [g for g in all_gold_classes if g in ['prose', 'verse']]\n",
    "contrib_pred = [p for g, p in zip(all_gold_classes, all_pred_classes) if g in ['prose', 'verse']]\n",
    "\n",
    "if contrib_gold:\n",
    "    contrib_correct = sum(1 for g, p in zip(contrib_gold, contrib_pred) if g == p)\n",
    "    contrib_accuracy = contrib_correct / len(contrib_gold)\n",
    "    \n",
    "    print(f\"\\nTotal contribution items: {len(contrib_gold)}\")\n",
    "    print(f\"Correctly classified: {contrib_correct} ({contrib_accuracy:.2%})\")\n",
    "    print(f\"Misclassified: {len(contrib_gold) - contrib_correct} ({(1-contrib_accuracy):.2%})\")\n",
    "    \n",
    "    # Contribution confusion\n",
    "    contrib_labels = ['prose', 'verse']\n",
    "    contrib_cm = confusion_matrix(contrib_gold, contrib_pred, labels=contrib_labels)\n",
    "    \n",
    "    print(\"\\nContributions Confusion Matrix:\")\n",
    "    print(f\"{'':>12}\", end='')\n",
    "    for label in contrib_labels:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\"*40)\n",
    "    for i, label in enumerate(contrib_labels):\n",
    "        print(f\"{label:>12}\", end='')\n",
    "        for j in range(len(contrib_labels)):\n",
    "            print(f\"{contrib_cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "\n",
    "# Per-page classification breakdown\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PER-PAGE CLASSIFICATION BREAKDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "page_class_df_data = []\n",
    "for result in page_classification_results:\n",
    "    page_class_df_data.append({\n",
    "        'page_id': result['page_id'],\n",
    "        'total_items': result['total'],\n",
    "        'correct': result['correct'],\n",
    "        'accuracy_%': round(result['accuracy'] * 100, 1) if result['total'] > 0 else 0.0,\n",
    "        'misclassified': result['total'] - result['correct']\n",
    "    })\n",
    "\n",
    "page_class_df = pd.DataFrame(page_class_df_data)\n",
    "print(\"\\n\" + page_class_df.to_string(index=False))\n",
    "\n",
    "# Detailed per-page analysis\n",
    "print(\"\\n\\nDETAILED PER-PAGE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in page_classification_results:\n",
    "    if result['total'] == 0:\n",
    "        continue\n",
    "    \n",
    "    page_id = result['page_id']\n",
    "    gold_classes = result['gold_classes']\n",
    "    pred_classes = result['pred_classes']\n",
    "    \n",
    "    print(f\"\\n{page_id}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Accuracy: {result['accuracy']:.2%} ({result['correct']}/{result['total']})\")\n",
    "    \n",
    "    # Class distribution\n",
    "    from collections import Counter\n",
    "    gold_dist = Counter(gold_classes)\n",
    "    \n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for cls in ['prose', 'verse', 'ad', 'paratext', 'unknown']:\n",
    "        if cls in gold_dist:\n",
    "            gold_count = gold_dist[cls]\n",
    "            pred_count = sum(1 for g, p in zip(gold_classes, pred_classes) \n",
    "                           if g == cls and p == cls)\n",
    "            print(f\"   {cls:<12} {pred_count}/{gold_count} correct\")\n",
    "    \n",
    "    # Misclassifications for this page\n",
    "    misclass_page = [(g, p) for g, p in zip(gold_classes, pred_classes) if g != p]\n",
    "    if misclass_page:\n",
    "        print(f\"\\nMisclassifications ({len(misclass_page)}):\")\n",
    "        misclass_counter = Counter(misclass_page)\n",
    "        for (gold, pred), count in misclass_counter.most_common():\n",
    "            print(f\"   {gold} â†’ {pred}: {count} time{'s' if count > 1 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Overall classification accuracy: {overall_accuracy:.2%}\")\n",
    "print(f\"- Best performing class: {class_labels[np.argmax([recalls[i] for i in range(len(class_labels))])]}\")\n",
    "print(f\"- Most challenging class: {class_labels[np.argmin([recalls[i] for i in range(len(class_labels))])]}\")\n",
    "if misclass_counts:\n",
    "    print(f\"- Most common confusion: {misclass_counts[0][0]} â†’ {misclass_counts[0][1]} ({misclass_counts[0][2]} times)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c433e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metadata extraction (titles and authors)...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OVERALL METADATA EXTRACTION METRICS\n",
      "================================================================================\n",
      "\n",
      "--- TITLE EXTRACTION ---\n",
      "Gold items with titles:       13\n",
      "Predicted items with titles:  12\n",
      "Exact matches:                11\n",
      "Partial matches (â‰¥80% sim):   11\n",
      "\n",
      "Precision: 91.67%\n",
      "Recall:    84.62%\n",
      "F1 Score:  0.880\n",
      "\n",
      "--- AUTHOR EXTRACTION ---\n",
      "Gold items with authors:      12\n",
      "Predicted items with authors: 10\n",
      "Exact matches:                10\n",
      "Partial matches (â‰¥80% sim):   10\n",
      "\n",
      "Precision: 100.00%\n",
      "Recall:    83.33%\n",
      "F1 Score:  0.909\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PER-PAGE METADATA EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "                                   page_id  title_gold  title_pred  title_F1  author_gold  author_pred  author_F1\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001           1           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002           1           1     1.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003           0           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004           2           2     1.000            2            1      0.667\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005           1           1     1.000            1            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006           2           2     1.000            3            3      1.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007           0           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008           0           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009           3           3     0.667            3            3      1.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010           3           3     1.000            3            3      1.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011           0           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012           0           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013           0           0     0.000            0            0      0.000\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014           0           0     0.000            0            0      0.000\n",
      "\n",
      "\n",
      "DETAILED PER-PAGE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  0/1 extracted (P: 0.00%, R: 0.00%, F1: 0.000)\n",
      "Authors: No gold authors on this page\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  1/1 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: No gold authors on this page\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 1/2 extracted (P: 100.00%, R: 50.00%, F1: 0.667)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  1/1 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 0/1 extracted (P: 0.00%, R: 0.00%, F1: 0.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/2 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 3/3 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  2/3 extracted (P: 66.67%, R: 66.67%, F1: 0.667)\n",
      "Authors: 3/3 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010\n",
      "--------------------------------------------------------------------------------\n",
      "Titles:  3/3 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "Authors: 3/3 extracted (P: 100.00%, R: 100.00%, F1: 1.000)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Title extraction F1: 0.880\n",
      "- Author extraction F1: 0.909\n",
      "- Title exact match rate: 11/13 (84.6%)\n",
      "- Author exact match rate: 10/12 (83.3%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metadata Extraction Evaluation\n",
    "Evaluate title and author extraction accuracy on matched items.\n",
    "\n",
    "Metrics:\n",
    "- Exact match: Field matches exactly\n",
    "- Partial match: String similarity above threshold\n",
    "- Presence: Field is present (not None) in both gold and pred\n",
    "- Precision: Of predicted fields, how many are correct?\n",
    "- Recall: Of gold fields, how many were extracted?\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def normalize_metadata_string(s: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Normalize metadata string for comparison.\n",
    "    - Lowercase\n",
    "    - Remove extra whitespace\n",
    "    - Remove punctuation at start/end\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip('.,;:!?')\n",
    "    return s\n",
    "\n",
    "\n",
    "def metadata_similarity(gold: Optional[str], pred: Optional[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity between two metadata strings.\n",
    "    Returns 1.0 for exact match, 0.0 for no match, partial scores for similarity.\n",
    "    \"\"\"\n",
    "    gold_norm = normalize_metadata_string(gold)\n",
    "    pred_norm = normalize_metadata_string(pred)\n",
    "    \n",
    "    if not gold_norm and not pred_norm:\n",
    "        return 1.0  # Both null\n",
    "    if not gold_norm or not pred_norm:\n",
    "        return 0.0  # One null, one not\n",
    "    \n",
    "    if gold_norm == pred_norm:\n",
    "        return 1.0  # Exact match\n",
    "    \n",
    "    # Use SequenceMatcher for partial similarity\n",
    "    return SequenceMatcher(None, gold_norm, pred_norm).ratio()\n",
    "\n",
    "\n",
    "def evaluate_metadata_field(gold_items: List[Dict], pred_items: List[Dict],\n",
    "                            matches: List[Tuple[int, int, float]],\n",
    "                            field_name: str,\n",
    "                            similarity_threshold: float = 0.8) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a specific metadata field (title or author).\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold items\n",
    "        pred_items: List of pred items\n",
    "        matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        field_name: 'item_title' or 'item_author'\n",
    "        similarity_threshold: Minimum similarity for partial match\n",
    "    \n",
    "    Returns:\n",
    "        Dict with metrics\n",
    "    \"\"\"\n",
    "    if not matches:\n",
    "        return {\n",
    "            'gold_present': 0,\n",
    "            'pred_present': 0,\n",
    "            'exact_matches': 0,\n",
    "            'partial_matches': 0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'examples': []\n",
    "        }\n",
    "    \n",
    "    matched_pairs = get_matched_pairs(matches, gold_items, pred_items)\n",
    "    \n",
    "    gold_present = 0  # Gold has non-null value\n",
    "    pred_present = 0  # Pred has non-null value\n",
    "    exact_matches = 0\n",
    "    partial_matches = 0\n",
    "    examples = []\n",
    "    \n",
    "    for gold_item, pred_item, _ in matched_pairs:\n",
    "        gold_value = gold_item.get(field_name)\n",
    "        pred_value = pred_item.get(field_name)\n",
    "        \n",
    "        gold_has_value = gold_value is not None and gold_value.strip() != ''\n",
    "        pred_has_value = pred_value is not None and pred_value.strip() != ''\n",
    "        \n",
    "        if gold_has_value:\n",
    "            gold_present += 1\n",
    "        \n",
    "        if pred_has_value:\n",
    "            pred_present += 1\n",
    "        \n",
    "        if gold_has_value and pred_has_value:\n",
    "            similarity = metadata_similarity(gold_value, pred_value)\n",
    "            \n",
    "            if similarity == 1.0:\n",
    "                exact_matches += 1\n",
    "                partial_matches += 1\n",
    "            elif similarity >= similarity_threshold:\n",
    "                partial_matches += 1\n",
    "                # Store example for partial matches\n",
    "                if len(examples) < 5:\n",
    "                    examples.append({\n",
    "                        'gold': gold_value,\n",
    "                        'pred': pred_value,\n",
    "                        'similarity': similarity,\n",
    "                        'item_class': gold_item.get('item_class')\n",
    "                    })\n",
    "    \n",
    "    # Calculate metrics based on partial matches (more lenient)\n",
    "    # Precision: of predicted values, how many match gold?\n",
    "    precision = partial_matches / pred_present if pred_present > 0 else 0.0\n",
    "    \n",
    "    # Recall: of gold values, how many were extracted?\n",
    "    recall = partial_matches / gold_present if gold_present > 0 else 0.0\n",
    "    \n",
    "    # F1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'gold_present': gold_present,\n",
    "        'pred_present': pred_present,\n",
    "        'exact_matches': exact_matches,\n",
    "        'partial_matches': partial_matches,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'examples': examples\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate metadata across all pages\n",
    "print(\"Evaluating metadata extraction (titles and authors)...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "page_metadata_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    page_id = page['page_id']\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    \n",
    "    # Evaluate titles\n",
    "    title_metrics = evaluate_metadata_field(gold_items, pred_items, matches, 'item_title')\n",
    "    \n",
    "    # Evaluate authors\n",
    "    author_metrics = evaluate_metadata_field(gold_items, pred_items, matches, 'item_author')\n",
    "    \n",
    "    page_metadata_results.append({\n",
    "        'page_id': page_id,\n",
    "        'title': title_metrics,\n",
    "        'author': author_metrics\n",
    "    })\n",
    "\n",
    "# Aggregate overall metrics\n",
    "overall_title = {\n",
    "    'gold_present': sum(r['title']['gold_present'] for r in page_metadata_results),\n",
    "    'pred_present': sum(r['title']['pred_present'] for r in page_metadata_results),\n",
    "    'exact_matches': sum(r['title']['exact_matches'] for r in page_metadata_results),\n",
    "    'partial_matches': sum(r['title']['partial_matches'] for r in page_metadata_results)\n",
    "}\n",
    "\n",
    "overall_author = {\n",
    "    'gold_present': sum(r['author']['gold_present'] for r in page_metadata_results),\n",
    "    'pred_present': sum(r['author']['pred_present'] for r in page_metadata_results),\n",
    "    'exact_matches': sum(r['author']['exact_matches'] for r in page_metadata_results),\n",
    "    'partial_matches': sum(r['author']['partial_matches'] for r in page_metadata_results)\n",
    "}\n",
    "\n",
    "# Calculate overall precision, recall, F1\n",
    "overall_title['precision'] = (overall_title['partial_matches'] / overall_title['pred_present'] \n",
    "                              if overall_title['pred_present'] > 0 else 0.0)\n",
    "overall_title['recall'] = (overall_title['partial_matches'] / overall_title['gold_present'] \n",
    "                          if overall_title['gold_present'] > 0 else 0.0)\n",
    "overall_title['f1'] = (2 * overall_title['precision'] * overall_title['recall'] / \n",
    "                       (overall_title['precision'] + overall_title['recall']) \n",
    "                       if (overall_title['precision'] + overall_title['recall']) > 0 else 0.0)\n",
    "\n",
    "overall_author['precision'] = (overall_author['partial_matches'] / overall_author['pred_present'] \n",
    "                               if overall_author['pred_present'] > 0 else 0.0)\n",
    "overall_author['recall'] = (overall_author['partial_matches'] / overall_author['gold_present'] \n",
    "                           if overall_author['gold_present'] > 0 else 0.0)\n",
    "overall_author['f1'] = (2 * overall_author['precision'] * overall_author['recall'] / \n",
    "                        (overall_author['precision'] + overall_author['recall']) \n",
    "                        if (overall_author['precision'] + overall_author['recall']) > 0 else 0.0)\n",
    "\n",
    "# Print overall results\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL METADATA EXTRACTION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- TITLE EXTRACTION ---\")\n",
    "print(f\"Gold items with titles:       {overall_title['gold_present']}\")\n",
    "print(f\"Predicted items with titles:  {overall_title['pred_present']}\")\n",
    "print(f\"Exact matches:                {overall_title['exact_matches']}\")\n",
    "print(f\"Partial matches (â‰¥80% sim):   {overall_title['partial_matches']}\")\n",
    "print(f\"\\nPrecision: {overall_title['precision']:.2%}\")\n",
    "print(f\"Recall:    {overall_title['recall']:.2%}\")\n",
    "print(f\"F1 Score:  {overall_title['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n--- AUTHOR EXTRACTION ---\")\n",
    "print(f\"Gold items with authors:      {overall_author['gold_present']}\")\n",
    "print(f\"Predicted items with authors: {overall_author['pred_present']}\")\n",
    "print(f\"Exact matches:                {overall_author['exact_matches']}\")\n",
    "print(f\"Partial matches (â‰¥80% sim):   {overall_author['partial_matches']}\")\n",
    "print(f\"\\nPrecision: {overall_author['precision']:.2%}\")\n",
    "print(f\"Recall:    {overall_author['recall']:.2%}\")\n",
    "print(f\"F1 Score:  {overall_author['f1']:.3f}\")\n",
    "\n",
    "# Collect examples from all pages\n",
    "all_title_examples = []\n",
    "all_author_examples = []\n",
    "\n",
    "for result in page_metadata_results:\n",
    "    all_title_examples.extend(result['title']['examples'])\n",
    "    all_author_examples.extend(result['author']['examples'])\n",
    "\n",
    "# Show examples of partial matches (not exact)\n",
    "if all_title_examples:\n",
    "    print(\"\\n\\nEXAMPLES OF PARTIAL TITLE MATCHES\")\n",
    "    print(\"-\"*80)\n",
    "    for i, ex in enumerate(all_title_examples[:5], 1):\n",
    "        print(f\"\\n{i}. {ex['item_class'].upper()} (Similarity: {ex['similarity']:.2%})\")\n",
    "        print(f\"   Gold: \\\"{ex['gold']}\\\"\")\n",
    "        print(f\"   Pred: \\\"{ex['pred']}\\\"\")\n",
    "\n",
    "if all_author_examples:\n",
    "    print(\"\\n\\nEXAMPLES OF PARTIAL AUTHOR MATCHES\")\n",
    "    print(\"-\"*80)\n",
    "    for i, ex in enumerate(all_author_examples[:5], 1):\n",
    "        print(f\"\\n{i}. {ex['item_class'].upper()} (Similarity: {ex['similarity']:.2%})\")\n",
    "        print(f\"   Gold: \\\"{ex['gold']}\\\"\")\n",
    "        print(f\"   Pred: \\\"{ex['pred']}\\\"\")\n",
    "\n",
    "# Per-page breakdown\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PER-PAGE METADATA EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "page_meta_df_data = []\n",
    "for result in page_metadata_results:\n",
    "    page_meta_df_data.append({\n",
    "        'page_id': result['page_id'],\n",
    "        'title_gold': result['title']['gold_present'],\n",
    "        'title_pred': result['title']['pred_present'],\n",
    "        'title_F1': round(result['title']['f1'], 3),\n",
    "        'author_gold': result['author']['gold_present'],\n",
    "        'author_pred': result['author']['pred_present'],\n",
    "        'author_F1': round(result['author']['f1'], 3)\n",
    "    })\n",
    "\n",
    "page_meta_df = pd.DataFrame(page_meta_df_data)\n",
    "print(\"\\n\" + page_meta_df.to_string(index=False))\n",
    "\n",
    "# Detailed per-page analysis\n",
    "print(\"\\n\\nDETAILED PER-PAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in page_metadata_results:\n",
    "    page_id = result['page_id']\n",
    "    title_metrics = result['title']\n",
    "    author_metrics = result['author']\n",
    "    \n",
    "    if title_metrics['gold_present'] == 0 and author_metrics['gold_present'] == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{page_id}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if title_metrics['gold_present'] > 0:\n",
    "        print(f\"Titles:  {title_metrics['partial_matches']}/{title_metrics['gold_present']} extracted \" +\n",
    "              f\"(P: {title_metrics['precision']:.2%}, R: {title_metrics['recall']:.2%}, \" +\n",
    "              f\"F1: {title_metrics['f1']:.3f})\")\n",
    "    else:\n",
    "        print(f\"Titles:  No gold titles on this page\")\n",
    "    \n",
    "    if author_metrics['gold_present'] > 0:\n",
    "        print(f\"Authors: {author_metrics['partial_matches']}/{author_metrics['gold_present']} extracted \" +\n",
    "              f\"(P: {author_metrics['precision']:.2%}, R: {author_metrics['recall']:.2%}, \" +\n",
    "              f\"F1: {author_metrics['f1']:.3f})\")\n",
    "    else:\n",
    "        print(f\"Authors: No gold authors on this page\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"- Title extraction F1: {overall_title['f1']:.3f}\")\n",
    "print(f\"- Author extraction F1: {overall_author['f1']:.3f}\")\n",
    "print(f\"- Title exact match rate: {overall_title['exact_matches']}/{overall_title['gold_present']} \" +\n",
    "      f\"({overall_title['exact_matches']/overall_title['gold_present']*100:.1f}%)\" \n",
    "      if overall_title['gold_present'] > 0 else \"- Title exact match rate: N/A\")\n",
    "print(f\"- Author exact match rate: {overall_author['exact_matches']}/{overall_author['gold_present']} \" +\n",
    "      f\"({overall_author['exact_matches']/overall_author['gold_present']*100:.1f}%)\"\n",
    "      if overall_author['gold_present'] > 0 else \"- Author exact match rate: N/A\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "838969fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating continuation tracking (all items)...\n",
      "\n",
      "======================================================================\n",
      "CONTINUATION TRACKING - GLOBAL SUMMARY (All Items)\n",
      "======================================================================\n",
      "\n",
      "Dataset coverage:\n",
      "  Matched items:        37\n",
      "  Unmatched gold items: 33\n",
      "  Unmatched pred items: 15\n",
      "\n",
      "is_continuation field:\n",
      "  Gold positives (True):     7\n",
      "  Pred positives (True):     3  (mismatch: -4)\n",
      "  True Positives (TP):       1\n",
      "  False Positives (FP):      2\n",
      "  False Negatives (FN):      6\n",
      "  True Negatives (TN):       33\n",
      "  Precision:                 33.33%\n",
      "  Recall:                    14.29%\n",
      "  F1 Score:                  0.200\n",
      "\n",
      "continues_on_next_page field:\n",
      "  Gold positives (True):     7\n",
      "  Pred positives (True):     7  (mismatch: +0)\n",
      "  True Positives (TP):       3\n",
      "  False Positives (FP):      4\n",
      "  False Negatives (FN):      4\n",
      "  True Negatives (TN):       34\n",
      "  Precision:                 42.86%\n",
      "  Recall:                    42.86%\n",
      "  F1 Score:                  0.429\n",
      "\n",
      "======================================================================\n",
      "PER-PAGE BREAKDOWN\n",
      "======================================================================\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-001.json\n",
      "  Matched: 4  |  Unmatched gold: 4  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002.json\n",
      "  Matched: 2  |  Unmatched gold: 0  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003.json\n",
      "  Matched: 1  |  Unmatched gold: 2  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 1  (mismatch: +1)\n",
      "    TP: 0  FP: 1  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 2  (mismatch: +1)\n",
      "    TP: 0  FP: 2  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-004.json\n",
      "  Matched: 3  |  Unmatched gold: 2  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005.json\n",
      "  Matched: 1  |  Unmatched gold: 4  |  Unmatched pred: 3\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-006.json\n",
      "  Matched: 3  |  Unmatched gold: 3  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-007.json\n",
      "  Matched: 1  |  Unmatched gold: 1  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-008.json\n",
      "  Matched: 0  |  Unmatched gold: 0  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-009.json\n",
      "  Matched: 4  |  Unmatched gold: 3  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-010.json\n",
      "  Matched: 4  |  Unmatched gold: 2  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 1  FP: 0  FN: 0\n",
      "    P: 100.00%  R: 100.00%  F1: 1.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011.json\n",
      "  Matched: 0  |  Unmatched gold: 3  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 1  Pred: 1  (mismatch: +0)\n",
      "    TP: 0  FP: 1  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012.json\n",
      "  Matched: 0  |  Unmatched gold: 8  |  Unmatched pred: 0\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 1  Pred: 0  (mismatch: -1)\n",
      "    TP: 0  FP: 0  FN: 1\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013.json\n",
      "  Matched: 10  |  Unmatched gold: 1  |  Unmatched pred: 1\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-014.json\n",
      "  Matched: 4  |  Unmatched gold: 0  |  Unmatched pred: 2\n",
      "\n",
      "  is_continuation:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n",
      "  continues_on_next_page:\n",
      "    Gold: 0  Pred: 0  (mismatch: +0)\n",
      "    TP: 0  FP: 0  FN: 0\n",
      "    P: 0.00%  R: 0.00%  F1: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Continuation Tracking Evaluation\n",
    "\n",
    "Evaluates the accuracy of continuation fields (is_continuation, continues_on_next_page)\n",
    "across ALL items in the dataset, including unmatched items.\n",
    "\n",
    "Fields are treated as binary:\n",
    "- True = continuation exists\n",
    "- False/None = no continuation (treated identically)\n",
    "\n",
    "Evaluation logic:\n",
    "- Matched items: Compare gold vs pred continuation fields directly\n",
    "- Unmatched gold items with continuation=True: False Negatives (model missed them)\n",
    "- Unmatched pred items with continuation=True: False Positives (model hallucinated them)\n",
    "\n",
    "Metrics: Precision, Recall, F1 for each field\n",
    "Reports: Global aggregates first, then per-page breakdown\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_continuation_all_items(\n",
    "    gold_items: List[Dict],\n",
    "    pred_items: List[Dict],\n",
    "    matches: List[Tuple[int, int, float]],\n",
    "    unmatched_gold: Set[int],\n",
    "    unmatched_pred: Set[int]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy across ALL items.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: Gold standard items\n",
    "        pred_items: Predicted items\n",
    "        matches: List of (gold_idx, pred_idx, similarity) tuples\n",
    "        unmatched_gold: Set of unmatched gold indices\n",
    "        unmatched_pred: Set of unmatched pred indices\n",
    "        \n",
    "    Returns:\n",
    "        Dict with metrics for is_continuation and continues_on_next_page\n",
    "    \"\"\"\n",
    "    # Initialize counters for both fields\n",
    "    is_cont_tp = is_cont_fp = is_cont_fn = is_cont_tn = 0\n",
    "    continues_tp = continues_fp = continues_fn = continues_tn = 0\n",
    "    \n",
    "    # 1. Evaluate matched items\n",
    "    for gold_idx, pred_idx, _ in matches:\n",
    "        gold_item = gold_items[gold_idx]\n",
    "        pred_item = pred_items[pred_idx]\n",
    "        \n",
    "        # Evaluate is_continuation\n",
    "        gold_is_cont = gold_item.get('is_continuation') is True\n",
    "        pred_is_cont = pred_item.get('is_continuation') is True\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_tp += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_fp += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_fn += 1\n",
    "        else:\n",
    "            is_cont_tn += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page') is True\n",
    "        pred_continues = pred_item.get('continues_on_next_page') is True\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_tp += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_fp += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_fn += 1\n",
    "        else:\n",
    "            continues_tn += 1\n",
    "    \n",
    "    # 2. Evaluate unmatched gold items (missed continuations = False Negatives)\n",
    "    for gold_idx in unmatched_gold:\n",
    "        gold_item = gold_items[gold_idx]\n",
    "        \n",
    "        # If gold has continuation=True but item wasn't matched, that's a FN\n",
    "        if gold_item.get('is_continuation') is True:\n",
    "            is_cont_fn += 1\n",
    "        \n",
    "        if gold_item.get('continues_on_next_page') is True:\n",
    "            continues_fn += 1\n",
    "    \n",
    "    # 3. Evaluate unmatched pred items (hallucinated continuations = False Positives)\n",
    "    for pred_idx in unmatched_pred:\n",
    "        pred_item = pred_items[pred_idx]\n",
    "        \n",
    "        # If pred has continuation=True but item wasn't matched, that's a FP\n",
    "        if pred_item.get('is_continuation') is True:\n",
    "            is_cont_fp += 1\n",
    "        \n",
    "        if pred_item.get('continues_on_next_page') is True:\n",
    "            continues_fp += 1\n",
    "    \n",
    "    # Calculate metrics for is_continuation\n",
    "    is_cont_p = is_cont_tp / (is_cont_tp + is_cont_fp) if (is_cont_tp + is_cont_fp) > 0 else 0.0\n",
    "    is_cont_r = is_cont_tp / (is_cont_tp + is_cont_fn) if (is_cont_tp + is_cont_fn) > 0 else 0.0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0.0\n",
    "    \n",
    "    # Calculate metrics for continues_on_next_page\n",
    "    continues_p = continues_tp / (continues_tp + continues_fp) if (continues_tp + continues_fp) > 0 else 0.0\n",
    "    continues_r = continues_tp / (continues_tp + continues_fn) if (continues_tp + continues_fn) > 0 else 0.0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            'tp': is_cont_tp,\n",
    "            'fp': is_cont_fp,\n",
    "            'fn': is_cont_fn,\n",
    "            'tn': is_cont_tn,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            'tp': continues_tp,\n",
    "            'fp': continues_fp,\n",
    "            'fn': continues_fn,\n",
    "            'tn': continues_tn,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        },\n",
    "        'n_matched': len(matches),\n",
    "        'n_unmatched_gold': len(unmatched_gold),\n",
    "        'n_unmatched_pred': len(unmatched_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate continuation tracking on all pages\n",
    "print(\"Evaluating continuation tracking (all items)...\")\n",
    "print()\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for page in all_pages:\n",
    "    gold_items = page['gold_items']\n",
    "    pred_items = page['pred_items']\n",
    "    matches = page['matches']\n",
    "    unmatched_gold = page['unmatched_gold']\n",
    "    unmatched_pred = page['unmatched_pred']\n",
    "    page_name = page['page_name']\n",
    "    \n",
    "    result = evaluate_continuation_all_items(\n",
    "        gold_items, pred_items, matches, \n",
    "        unmatched_gold, unmatched_pred\n",
    "    )\n",
    "    result['page'] = page_name\n",
    "    \n",
    "    continuation_results.append(result)\n",
    "\n",
    "# Aggregate global metrics\n",
    "total_is_cont = {\n",
    "    'tp': sum(r['is_continuation']['tp'] for r in continuation_results),\n",
    "    'fp': sum(r['is_continuation']['fp'] for r in continuation_results),\n",
    "    'fn': sum(r['is_continuation']['fn'] for r in continuation_results),\n",
    "    'tn': sum(r['is_continuation']['tn'] for r in continuation_results)\n",
    "}\n",
    "\n",
    "total_continues = {\n",
    "    'tp': sum(r['continues_on_next_page']['tp'] for r in continuation_results),\n",
    "    'fp': sum(r['continues_on_next_page']['fp'] for r in continuation_results),\n",
    "    'fn': sum(r['continues_on_next_page']['fn'] for r in continuation_results),\n",
    "    'tn': sum(r['continues_on_next_page']['tn'] for r in continuation_results)\n",
    "}\n",
    "\n",
    "# Calculate global metrics\n",
    "is_cont_p = total_is_cont['tp'] / (total_is_cont['tp'] + total_is_cont['fp']) if (total_is_cont['tp'] + total_is_cont['fp']) > 0 else 0.0\n",
    "is_cont_r = total_is_cont['tp'] / (total_is_cont['tp'] + total_is_cont['fn']) if (total_is_cont['tp'] + total_is_cont['fn']) > 0 else 0.0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0.0\n",
    "\n",
    "continues_p = total_continues['tp'] / (total_continues['tp'] + total_continues['fp']) if (total_continues['tp'] + total_continues['fp']) > 0 else 0.0\n",
    "continues_r = total_continues['tp'] / (total_continues['tp'] + total_continues['fn']) if (total_continues['tp'] + total_continues['fn']) > 0 else 0.0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0.0\n",
    "\n",
    "total_matched = sum(r['n_matched'] for r in continuation_results)\n",
    "total_unmatched_gold = sum(r['n_unmatched_gold'] for r in continuation_results)\n",
    "total_unmatched_pred = sum(r['n_unmatched_pred'] for r in continuation_results)\n",
    "\n",
    "# Count how many items have True values in gold\n",
    "gold_is_cont_count = total_is_cont['tp'] + total_is_cont['fn']\n",
    "gold_continues_count = total_continues['tp'] + total_continues['fn']\n",
    "\n",
    "# Count how many True values the model predicted\n",
    "pred_is_cont_count = total_is_cont['tp'] + total_is_cont['fp']\n",
    "pred_continues_count = total_continues['tp'] + total_continues['fp']\n",
    "\n",
    "# Calculate quantity mismatch\n",
    "is_cont_mismatch = pred_is_cont_count - gold_is_cont_count\n",
    "continues_mismatch = pred_continues_count - gold_continues_count\n",
    "\n",
    "# Print global summary\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"CONTINUATION TRACKING - GLOBAL SUMMARY (All Items)\")\n",
    "print(f\"{'='*70}\")\n",
    "print()\n",
    "print(f\"Dataset coverage:\")\n",
    "print(f\"  Matched items:        {total_matched}\")\n",
    "print(f\"  Unmatched gold items: {total_unmatched_gold}\")\n",
    "print(f\"  Unmatched pred items: {total_unmatched_pred}\")\n",
    "print()\n",
    "print(f\"is_continuation field:\")\n",
    "print(f\"  Gold positives (True):     {gold_is_cont_count}\")\n",
    "print(f\"  Pred positives (True):     {pred_is_cont_count}  (mismatch: {is_cont_mismatch:+d})\")\n",
    "print(f\"  True Positives (TP):       {total_is_cont['tp']}\")\n",
    "print(f\"  False Positives (FP):      {total_is_cont['fp']}\")\n",
    "print(f\"  False Negatives (FN):      {total_is_cont['fn']}\")\n",
    "print(f\"  True Negatives (TN):       {total_is_cont['tn']}\")\n",
    "print(f\"  Precision:                 {is_cont_p:.2%}\")\n",
    "print(f\"  Recall:                    {is_cont_r:.2%}\")\n",
    "print(f\"  F1 Score:                  {is_cont_f1:.3f}\")\n",
    "print()\n",
    "print(f\"continues_on_next_page field:\")\n",
    "print(f\"  Gold positives (True):     {gold_continues_count}\")\n",
    "print(f\"  Pred positives (True):     {pred_continues_count}  (mismatch: {continues_mismatch:+d})\")\n",
    "print(f\"  True Positives (TP):       {total_continues['tp']}\")\n",
    "print(f\"  False Positives (FP):      {total_continues['fp']}\")\n",
    "print(f\"  False Negatives (FN):      {total_continues['fn']}\")\n",
    "print(f\"  True Negatives (TN):       {total_continues['tn']}\")\n",
    "print(f\"  Precision:                 {continues_p:.2%}\")\n",
    "print(f\"  Recall:                    {continues_r:.2%}\")\n",
    "print(f\"  F1 Score:                  {continues_f1:.3f}\")\n",
    "print()\n",
    "\n",
    "# Per-page breakdown\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"PER-PAGE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "print()\n",
    "\n",
    "for result in continuation_results:\n",
    "    page = result['page']\n",
    "    n_matched = result['n_matched']\n",
    "    n_unmatch_gold = result['n_unmatched_gold']\n",
    "    n_unmatch_pred = result['n_unmatched_pred']\n",
    "    \n",
    "    is_cont = result['is_continuation']\n",
    "    continues = result['continues_on_next_page']\n",
    "    \n",
    "    # Count gold positives for this page\n",
    "    page_is_cont_gold = is_cont['tp'] + is_cont['fn']\n",
    "    page_continues_gold = continues['tp'] + continues['fn']\n",
    "    \n",
    "    # Count pred positives for this page\n",
    "    page_is_cont_pred = is_cont['tp'] + is_cont['fp']\n",
    "    page_continues_pred = continues['tp'] + continues['fp']\n",
    "    \n",
    "    # Calculate mismatch\n",
    "    is_cont_mismatch = page_is_cont_pred - page_is_cont_gold\n",
    "    continues_mismatch = page_continues_pred - page_continues_gold\n",
    "    \n",
    "    print(f\"{page}\")\n",
    "    print(f\"  Matched: {n_matched}  |  Unmatched gold: {n_unmatch_gold}  |  Unmatched pred: {n_unmatch_pred}\")\n",
    "    print()\n",
    "    print(f\"  is_continuation:\")\n",
    "    print(f\"    Gold: {page_is_cont_gold}  Pred: {page_is_cont_pred}  (mismatch: {is_cont_mismatch:+d})\")\n",
    "    print(f\"    TP: {is_cont['tp']}  FP: {is_cont['fp']}  FN: {is_cont['fn']}\")\n",
    "    print(f\"    P: {is_cont['precision']:.2%}  R: {is_cont['recall']:.2%}  F1: {is_cont['f1']:.3f}\")\n",
    "    print()\n",
    "    print(f\"  continues_on_next_page:\")\n",
    "    print(f\"    Gold: {page_continues_gold}  Pred: {page_continues_pred}  (mismatch: {continues_mismatch:+d})\")\n",
    "    print(f\"    TP: {continues['tp']}  FP: {continues['fp']}  FN: {continues['fn']}\")\n",
    "    print(f\"    P: {continues['precision']:.2%}  R: {continues['recall']:.2%}  F1: {continues['f1']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1 OCR EVALUATION - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "AGGREGATE METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "            Dimension                  Metric  Value                                 Details\n",
      "  Structure Detection         Item Match Rate  52.9%                     37/70 items matched\n",
      "  Structure Detection Contribution Match Rate  60.0%             15/25 contributions matched\n",
      "   Text Quality (OCR)     CER (Standard, All) 14.83%               Order-agnostic evaluation\n",
      "   Text Quality (OCR) CER (Standard, Contrib) 12.42%           Structure-aware, matched only\n",
      "   Text Quality (OCR)                Coverage  48.8% Contribution chars successfully matched\n",
      "       Classification        Overall Accuracy  67.6%                             25/37 items\n",
      "       Classification   Contribution Accuracy  93.3%                 14/15 prose/verse items\n",
      "  Metadata Extraction                Title F1  0.960                         P: 100%, R: 92%\n",
      "  Metadata Extraction               Author F1  0.909                         P: 100%, R: 83%\n",
      "Continuation Tracking      is_continuation F1  0.200                          P: 33%, R: 14%\n",
      "Continuation Tracking    continues_on_next F1  0.429                          P: 43%, R: 43%\n",
      "\n",
      "================================================================================\n",
      "PROBLEM PAGES\n",
      "================================================================================\n",
      "\n",
      "Identified 6 pages with significant issues:\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-012.json:\n",
      "  â€¢ Low match rate (0%)\n",
      "  â€¢ Zero predictions\n",
      "  â€¢ Low contribution matching (0%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-003.json:\n",
      "  â€¢ Low match rate (33%)\n",
      "  â€¢ Low contribution matching (0%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-005.json:\n",
      "  â€¢ Low match rate (20%)\n",
      "  â€¢ Low contribution matching (33%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-011.json:\n",
      "  â€¢ Low match rate (0%)\n",
      "  â€¢ Low contribution matching (0%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-002.json:\n",
      "  â€¢ High classification errors (50%)\n",
      "\n",
      "La_Plume_bpt6k1185893k_1_10_1889__page-013.json:\n",
      "  â€¢ High classification errors (90%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final Summary Report\n",
    "\n",
    "Synthesizes all evaluation findings into a summary that aggregates metrics across all evaluation dimensions and identifies problematic pages\n",
    "\"\"\"\n",
    "\n",
    "# Collect aggregate metrics from all previous evaluations\n",
    "# These values should be computed from the previous cells\n",
    "\n",
    "# Helper function to create summary table\n",
    "def create_summary_table():\n",
    "    \"\"\"\n",
    "    Create aggregate metrics table summarizing all evaluation dimensions.\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    # 1. STRUCTURE METRICS (from Cell 3)\n",
    "    total_gold_items = sum(len(page['gold_items']) for page in all_pages)\n",
    "    total_pred_items = sum(len(page['pred_items']) for page in all_pages)\n",
    "    total_matches = sum(len(page['matches']) for page in all_pages)\n",
    "    \n",
    "    # Count contributions\n",
    "    total_gold_contrib = sum(\n",
    "        len([item for item in page['gold_items'] \n",
    "             if item['item_class'] in ['prose', 'verse']])\n",
    "        for page in all_pages\n",
    "    )\n",
    "    total_pred_contrib = sum(\n",
    "        len([item for item in page['pred_items'] \n",
    "             if item['item_class'] in ['prose', 'verse']])\n",
    "        for page in all_pages\n",
    "    )\n",
    "    contrib_matches = sum(\n",
    "        len(filter_matches_by_class(page['matches'], page['gold_items'], ['prose', 'verse']))\n",
    "        for page in all_pages\n",
    "    )\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Structure Detection',\n",
    "        'Metric': 'Item Match Rate',\n",
    "        'Value': f\"{(total_matches/total_gold_items)*100:.1f}%\",\n",
    "        'Details': f\"{total_matches}/{total_gold_items} items matched\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Structure Detection',\n",
    "        'Metric': 'Contribution Match Rate',\n",
    "        'Value': f\"{(contrib_matches/total_gold_contrib)*100:.1f}%\",\n",
    "        'Details': f\"{contrib_matches}/{total_gold_contrib} contributions matched\"\n",
    "    })\n",
    "    \n",
    "    # 2. TEXT QUALITY METRICS (from Cell 5 - reference computed values)\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'CER (Standard, All)',\n",
    "        'Value': f\"{avg_oa_all['cer_standard']:.2%}\",\n",
    "        'Details': 'Order-agnostic evaluation'\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'CER (Standard, Contrib)',\n",
    "        'Value': f\"{avg_sa_contrib['cer_standard']:.2%}\",\n",
    "        'Details': 'Structure-aware, matched only'\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Dimension': 'Text Quality (OCR)',\n",
    "        'Metric': 'Coverage',\n",
    "        'Value': f\"{total_sa_contrib_matched/total_sa_contrib_gold*100:.1f}%\",\n",
    "        'Details': 'Contribution chars successfully matched'\n",
    "    })\n",
    "    \n",
    "    # 3. CLASSIFICATION METRICS (from Cell 8 - recalculate)\n",
    "    total_matched = 0\n",
    "    total_correct = 0\n",
    "    contrib_matched = 0\n",
    "    contrib_correct = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        for g_idx, p_idx, _ in matches:\n",
    "            gold_class = gold_items[g_idx]['item_class']\n",
    "            pred_class = pred_items[p_idx]['item_class']\n",
    "            \n",
    "            total_matched += 1\n",
    "            if gold_class == pred_class:\n",
    "                total_correct += 1\n",
    "            \n",
    "            if gold_class in ['prose', 'verse']:\n",
    "                contrib_matched += 1\n",
    "                if gold_class == pred_class:\n",
    "                    contrib_correct += 1\n",
    "    \n",
    "    overall_acc = (total_correct / total_matched * 100) if total_matched > 0 else 0\n",
    "    contrib_acc = (contrib_correct / contrib_matched * 100) if contrib_matched > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Classification',\n",
    "        'Metric': 'Overall Accuracy',\n",
    "        'Value': f\"{overall_acc:.1f}%\",\n",
    "        'Details': f\"{total_correct}/{total_matched} items\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Classification',\n",
    "        'Metric': 'Contribution Accuracy',\n",
    "        'Value': f\"{contrib_acc:.1f}%\",\n",
    "        'Details': f\"{contrib_correct}/{contrib_matched} prose/verse items\"\n",
    "    })\n",
    "    \n",
    "    # 4. METADATA METRICS (from Cell 9 - recalculate)\n",
    "    \n",
    "    METADATA_SIMILARITY_THRESHOLD = 0.8  # Same as Cell 9\n",
    "    \n",
    "    title_gold = 0\n",
    "    title_pred = 0\n",
    "    title_correct = 0\n",
    "    author_gold = 0\n",
    "    author_pred = 0\n",
    "    author_correct = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        for g_idx, p_idx, _ in matches:\n",
    "            gold_item = gold_items[g_idx]\n",
    "            pred_item = pred_items[p_idx]\n",
    "            \n",
    "            # Title evaluation (exact or â‰¥80% similar using text_similarity)\n",
    "            if gold_item.get('item_title'):\n",
    "                title_gold += 1\n",
    "                if pred_item.get('item_title'):\n",
    "                    title_pred += 1\n",
    "                    # Check exact match or high similarity\n",
    "                    if (gold_item['item_title'].strip() == pred_item['item_title'].strip() or\n",
    "                        text_similarity(gold_item['item_title'], pred_item['item_title']) >= METADATA_SIMILARITY_THRESHOLD):\n",
    "                        title_correct += 1\n",
    "            \n",
    "            # Author evaluation (exact or â‰¥80% similar using text_similarity)\n",
    "            if gold_item.get('item_author'):\n",
    "                author_gold += 1\n",
    "                if pred_item.get('item_author'):\n",
    "                    author_pred += 1\n",
    "                    # Check exact match or high similarity\n",
    "                    if (gold_item['item_author'].strip() == pred_item['item_author'].strip() or\n",
    "                        text_similarity(gold_item['item_author'], pred_item['item_author']) >= METADATA_SIMILARITY_THRESHOLD):\n",
    "                        author_correct += 1\n",
    "    \n",
    "    title_p = (title_correct / title_pred * 100) if title_pred > 0 else 0\n",
    "    title_r = (title_correct / title_gold * 100) if title_gold > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    author_p = (author_correct / author_pred * 100) if author_pred > 0 else 0\n",
    "    author_r = (author_correct / author_gold * 100) if author_gold > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Metadata Extraction',\n",
    "        'Metric': 'Title F1',\n",
    "        'Value': f\"{title_f1/100:.3f}\",\n",
    "        'Details': f\"P: {title_p:.0f}%, R: {title_r:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Metadata Extraction',\n",
    "        'Metric': 'Author F1',\n",
    "        'Value': f\"{author_f1/100:.3f}\",\n",
    "        'Details': f\"P: {author_p:.0f}%, R: {author_r:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    # 5. CONTINUATION TRACKING (from Cell 10 - recalculate with ALL items)\n",
    "    is_cont_tp = is_cont_fp = is_cont_fn = 0\n",
    "    continues_tp = continues_fp = continues_fn = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        unmatched_gold = page['unmatched_gold']\n",
    "        unmatched_pred = page['unmatched_pred']\n",
    "        \n",
    "        # 1. Evaluate matched items\n",
    "        for g_idx, p_idx, _ in matches:\n",
    "            gold_item = gold_items[g_idx]\n",
    "            pred_item = pred_items[p_idx]\n",
    "            \n",
    "            # is_continuation\n",
    "            gold_is_cont = gold_item.get('is_continuation') is True\n",
    "            pred_is_cont = pred_item.get('is_continuation') is True\n",
    "            \n",
    "            if gold_is_cont and pred_is_cont:\n",
    "                is_cont_tp += 1\n",
    "            elif not gold_is_cont and pred_is_cont:\n",
    "                is_cont_fp += 1\n",
    "            elif gold_is_cont and not pred_is_cont:\n",
    "                is_cont_fn += 1\n",
    "            \n",
    "            # continues_on_next_page\n",
    "            gold_continues = gold_item.get('continues_on_next_page') is True\n",
    "            pred_continues = pred_item.get('continues_on_next_page') is True\n",
    "            \n",
    "            if gold_continues and pred_continues:\n",
    "                continues_tp += 1\n",
    "            elif not gold_continues and pred_continues:\n",
    "                continues_fp += 1\n",
    "            elif gold_continues and not pred_continues:\n",
    "                continues_fn += 1\n",
    "        \n",
    "        # 2. Evaluate unmatched gold items (missed continuations = FN)\n",
    "        for gold_idx in unmatched_gold:\n",
    "            gold_item = gold_items[gold_idx]\n",
    "            if gold_item.get('is_continuation') is True:\n",
    "                is_cont_fn += 1\n",
    "            if gold_item.get('continues_on_next_page') is True:\n",
    "                continues_fn += 1\n",
    "        \n",
    "        # 3. Evaluate unmatched pred items (hallucinated continuations = FP)\n",
    "        for pred_idx in unmatched_pred:\n",
    "            pred_item = pred_items[pred_idx]\n",
    "            if pred_item.get('is_continuation') is True:\n",
    "                is_cont_fp += 1\n",
    "            if pred_item.get('continues_on_next_page') is True:\n",
    "                continues_fp += 1\n",
    "    \n",
    "    is_cont_p = is_cont_tp / (is_cont_tp + is_cont_fp) if (is_cont_tp + is_cont_fp) > 0 else 0\n",
    "    is_cont_r = is_cont_tp / (is_cont_tp + is_cont_fn) if (is_cont_tp + is_cont_fn) > 0 else 0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "    \n",
    "    continues_p = continues_tp / (continues_tp + continues_fp) if (continues_tp + continues_fp) > 0 else 0\n",
    "    continues_r = continues_tp / (continues_tp + continues_fn) if (continues_tp + continues_fn) > 0 else 0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Continuation Tracking',\n",
    "        'Metric': 'is_continuation F1',\n",
    "        'Value': f\"{is_cont_f1:.3f}\",\n",
    "        'Details': f\"P: {is_cont_p*100:.0f}%, R: {is_cont_r*100:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': 'Continuation Tracking',\n",
    "        'Metric': 'continues_on_next F1',\n",
    "        'Value': f\"{continues_f1:.3f}\",\n",
    "        'Details': f\"P: {continues_p*100:.0f}%, R: {continues_r*100:.0f}%\"\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "def identify_problem_pages():\n",
    "    \"\"\"\n",
    "    Identify pages with significant issues across multiple dimensions.\n",
    "    Returns list of (page_name, issues) tuples.\n",
    "    \"\"\"\n",
    "    problem_pages = {}\n",
    "    \n",
    "    for page in all_pages:\n",
    "        page_name = page['page_name']\n",
    "        issues = []\n",
    "        \n",
    "        gold_items = page['gold_items']\n",
    "        pred_items = page['pred_items']\n",
    "        matches = page['matches']\n",
    "        \n",
    "        # Skip empty pages\n",
    "        if len(gold_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Issue 1: Low match rate\n",
    "        match_rate = len(matches) / len(gold_items) if len(gold_items) > 0 else 0\n",
    "        if match_rate < 0.5:\n",
    "            issues.append(f\"Low match rate ({match_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Issue 2: Zero predictions\n",
    "        if len(pred_items) == 0:\n",
    "            issues.append(\"Zero predictions\")\n",
    "        \n",
    "        # Issue 3: Low contribution match rate\n",
    "        gold_contrib = [item for item in gold_items if item['item_class'] in ['prose', 'verse']]\n",
    "        if gold_contrib:\n",
    "            contrib_matches = filter_matches_by_class(matches, gold_items, ['prose', 'verse'])\n",
    "            contrib_match_rate = len(contrib_matches) / len(gold_contrib) if len(gold_contrib) > 0 else 0\n",
    "            if contrib_match_rate < 0.5:\n",
    "                issues.append(f\"Low contribution matching ({contrib_match_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Issue 4: High classification errors\n",
    "        if matches:\n",
    "            misclassified = 0\n",
    "            for g_idx, p_idx, _ in matches:\n",
    "                if gold_items[g_idx]['item_class'] != pred_items[p_idx]['item_class']:\n",
    "                    misclassified += 1\n",
    "            error_rate = misclassified / len(matches)\n",
    "            if error_rate > 0.3:\n",
    "                issues.append(f\"High classification errors ({error_rate*100:.0f}%)\")\n",
    "        \n",
    "        if issues:\n",
    "            problem_pages[page_name] = issues\n",
    "    \n",
    "    return problem_pages\n",
    "\n",
    "\n",
    "# Generate summary table\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1 OCR EVALUATION - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "summary_df = create_summary_table()\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(\"-\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Problem pages\n",
    "print(\"=\" * 80)\n",
    "print(\"PROBLEM PAGES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "problem_pages = identify_problem_pages()\n",
    "if problem_pages:\n",
    "    print(f\"Identified {len(problem_pages)} pages with significant issues:\")\n",
    "    print()\n",
    "    \n",
    "    # Sort by number of issues\n",
    "    sorted_problems = sorted(problem_pages.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for page_name, issues in sorted_problems:\n",
    "        print(f\"{page_name}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  â€¢ {issue}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No pages identified with critical issues.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
