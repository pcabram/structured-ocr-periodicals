{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdabe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "✓ Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "✓ Predictions: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/interim_pages/Second_try_revised\n",
      "\n",
      "Dataset sizes:\n",
      "   Gold standard files: 1\n",
      "   Prediction files: 14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import Levenshtein\n",
    "\n",
    "# Path setup\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import schemas\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "\n",
    "# Paths\n",
    "GOLD_DIR = PROJECT_ROOT / \"data\" / \"gold_standard\" / \"cleaned\"\n",
    "PRED_DIR = PROJECT_ROOT / \"data\" / \"interim_pages\" / \"Second_try_revised\"\n",
    "\n",
    "print(f\"✓ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"✓ Gold standard: {GOLD_DIR}\")\n",
    "print(f\"✓ Predictions: {PRED_DIR}\")\n",
    "\n",
    "# Count files\n",
    "gold_files = list(GOLD_DIR.glob(\"*.json\"))\n",
    "pred_files = list(PRED_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"   Gold standard files: {len(gold_files)}\")\n",
    "print(f\"   Prediction files: {len(pred_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d6d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 matching page pairs\n"
     ]
    }
   ],
   "source": [
    "def load_page_pairs() -> List[Tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Match gold standard files with prediction files by filename.\n",
    "    Returns list of (gold_path, pred_path) tuples.\n",
    "    \"\"\"\n",
    "    gold_files = {f.name: f for f in GOLD_DIR.glob(\"*.json\")}\n",
    "    pred_files = {f.name: f for f in PRED_DIR.glob(\"*.json\")}\n",
    "    \n",
    "    # Find common files\n",
    "    common_names = set(gold_files.keys()) & set(pred_files.keys())\n",
    "    \n",
    "    pairs = [(gold_files[name], pred_files[name]) for name in sorted(common_names)]\n",
    "    \n",
    "    print(f\"Found {len(pairs)} matching page pairs\")\n",
    "    if len(pairs) < len(gold_files):\n",
    "        missing = set(gold_files.keys()) - set(pred_files.keys())\n",
    "        print(f\"⚠️  {len(missing)} gold standard pages without predictions:\")\n",
    "        for name in sorted(missing)[:5]:\n",
    "            print(f\"   • {name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "page_pairs = load_page_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text quality...\n",
      "\n",
      "✓ La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "   All items - CER: 1.18%, WER: 2.96%\n",
      "   Contributions - CER: 0.00%, WER: 0.00%\n",
      "\n",
      "============================================================\n",
      "TEXT QUALITY SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Average CER: 1.18%\n",
      "   Average WER: 2.96%\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Average CER: 0.00%\n",
      "   Average WER: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def character_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate Character Error Rate using Levenshtein distance.\"\"\"\n",
    "    if not reference:\n",
    "        return 1.0 if hypothesis else 0.0\n",
    "    distance = Levenshtein.distance(reference, hypothesis)\n",
    "    return distance / len(reference)\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate Word Error Rate using Levenshtein distance on words.\"\"\"\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    if not ref_words:\n",
    "        return 1.0 if hyp_words else 0.0\n",
    "    distance = Levenshtein.distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "def evaluate_text_quality(gold_path: Path, pred_path: Path, \n",
    "                         item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare text quality between gold and prediction.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        item_classes: If provided, only evaluate these item classes\n",
    "    \n",
    "    Returns:\n",
    "        Dict with CER, WER, and per-item metrics\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    \n",
    "    # Extract text blocks\n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    # Flatten to text\n",
    "    gold_text = \"\\n\\n\".join(item.get('item_text_raw', '') for item in gold_items)\n",
    "    pred_text = \"\\n\\n\".join(item.get('item_text_raw', '') for item in pred_items)\n",
    "    \n",
    "    cer = character_error_rate(gold_text, pred_text)\n",
    "    wer = word_error_rate(gold_text, pred_text)\n",
    "    \n",
    "    return {\n",
    "        'cer': cer,\n",
    "        'wer': wer,\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split()),\n",
    "        'gold_items': len(gold_items),\n",
    "        'pred_items': len(pred_items)\n",
    "    }\n",
    "\n",
    "# Evaluate text quality for all pages\n",
    "print(\"Evaluating text quality...\\n\")\n",
    "\n",
    "all_results = []\n",
    "contributions_results = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_text_quality(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    all_results.append(result_all)\n",
    "    \n",
    "    # Contributions only (prose + verse)\n",
    "    result_contrib = evaluate_text_quality(gold_path, pred_path, \n",
    "                                          item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    contributions_results.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   All items - CER: {result_all['cer']:.2%}, WER: {result_all['wer']:.2%}\")\n",
    "    print(f\"   Contributions - CER: {result_contrib['cer']:.2%}, WER: {result_contrib['wer']:.2%}\\n\")\n",
    "\n",
    "# Compute averages\n",
    "avg_cer_all = sum(r['cer'] for r in all_results) / len(all_results) if all_results else 0\n",
    "avg_wer_all = sum(r['wer'] for r in all_results) / len(all_results) if all_results else 0\n",
    "avg_cer_contrib = sum(r['cer'] for r in contributions_results) / len(contributions_results) if contributions_results else 0\n",
    "avg_wer_contrib = sum(r['wer'] for r in contributions_results) / len(contributions_results) if contributions_results else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"TEXT QUALITY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Average CER: {avg_cer_all:.2%}\")\n",
    "print(f\"   Average WER: {avg_wer_all:.2%}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Average CER: {avg_cer_contrib:.2%}\")\n",
    "print(f\"   Average WER: {avg_wer_contrib:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e807f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item boundary detection...\n",
      "\n",
      "✓ La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "   All items - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "   Contributions - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "\n",
      "============================================================\n",
      "ITEM BOUNDARY DETECTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Precision: 100.00%\n",
      "   Recall: 100.00%\n",
      "   F1: 1.000\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Precision: 0.00%\n",
      "   Recall: 0.00%\n",
      "   F1: 0.000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_item_boundaries(gold_path: Path, pred_path: Path, \n",
    "                            item_classes: Optional[List[str]] = None,\n",
    "                            tolerance: int = 20) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate whether item boundaries are correctly detected.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        item_classes: If provided, only evaluate these item classes\n",
    "        tolerance: Character tolerance window for boundary matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    \n",
    "    def get_boundaries(items, filter_classes=None):\n",
    "        \"\"\"Get character positions where items start.\"\"\"\n",
    "        boundaries = []\n",
    "        pos = 0\n",
    "        for item in items:\n",
    "            if filter_classes is None or item.get('item_class') in filter_classes:\n",
    "                boundaries.append(pos)\n",
    "            # Add text length + separator\n",
    "            pos += len(item.get('item_text_raw', '')) + 2\n",
    "        return set(boundaries)\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    gold_bounds = get_boundaries(gold_items, item_classes)\n",
    "    pred_bounds = get_boundaries(pred_items, item_classes)\n",
    "    \n",
    "    # Match boundaries within tolerance\n",
    "    tp = 0\n",
    "    for pred_b in pred_bounds:\n",
    "        if any(abs(pred_b - gold_b) <= tolerance for gold_b in gold_bounds):\n",
    "            tp += 1\n",
    "    \n",
    "    fp = len(pred_bounds) - tp\n",
    "    fn = len(gold_bounds) - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'gold_boundaries': len(gold_bounds),\n",
    "        'pred_boundaries': len(pred_bounds)\n",
    "    }\n",
    "\n",
    "# Evaluate item boundaries\n",
    "print(\"Evaluating item boundary detection...\\n\")\n",
    "\n",
    "boundary_results_all = []\n",
    "boundary_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_item_boundaries(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    boundary_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_item_boundaries(gold_path, pred_path,\n",
    "                                             item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    boundary_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   All items - P: {result_all['precision']:.2%}, \"\n",
    "          f\"R: {result_all['recall']:.2%}, F1: {result_all['f1']:.3f}\")\n",
    "    print(f\"   Contributions - P: {result_contrib['precision']:.2%}, \"\n",
    "          f\"R: {result_contrib['recall']:.2%}, F1: {result_contrib['f1']:.3f}\\n\")\n",
    "\n",
    "# Compute micro-averages (sum all TP/FP/FN, then compute metrics)\n",
    "total_tp_all = sum(r['tp'] for r in boundary_results_all)\n",
    "total_fp_all = sum(r['fp'] for r in boundary_results_all)\n",
    "total_fn_all = sum(r['fn'] for r in boundary_results_all)\n",
    "\n",
    "precision_all = total_tp_all / (total_tp_all + total_fp_all) if (total_tp_all + total_fp_all) > 0 else 0\n",
    "recall_all = total_tp_all / (total_tp_all + total_fn_all) if (total_tp_all + total_fn_all) > 0 else 0\n",
    "f1_all = 2 * precision_all * recall_all / (precision_all + recall_all) if (precision_all + recall_all) > 0 else 0\n",
    "\n",
    "total_tp_contrib = sum(r['tp'] for r in boundary_results_contrib)\n",
    "total_fp_contrib = sum(r['fp'] for r in boundary_results_contrib)\n",
    "total_fn_contrib = sum(r['fn'] for r in boundary_results_contrib)\n",
    "\n",
    "precision_contrib = total_tp_contrib / (total_tp_contrib + total_fp_contrib) if (total_tp_contrib + total_fp_contrib) > 0 else 0\n",
    "recall_contrib = total_tp_contrib / (total_tp_contrib + total_fn_contrib) if (total_tp_contrib + total_fn_contrib) > 0 else 0\n",
    "f1_contrib = 2 * precision_contrib * recall_contrib / (precision_contrib + recall_contrib) if (precision_contrib + recall_contrib) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ITEM BOUNDARY DETECTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Precision: {precision_all:.2%}\")\n",
    "print(f\"   Recall: {recall_all:.2%}\")\n",
    "print(f\"   F1: {f1_all:.3f}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Precision: {precision_contrib:.2%}\")\n",
    "print(f\"   Recall: {recall_contrib:.2%}\")\n",
    "print(f\"   F1: {f1_contrib:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609abbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating item classification...\n",
      "\n",
      "✓ La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "   All items - Accuracy: 100.00% (7/7)\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION ACCURACY SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Overall Accuracy: 100.00% (7/7)\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Overall Accuracy: 0.00% (0/0)\n",
      "\n",
      "Confusion Matrix (All Items):\n",
      "Gold / Pred    paratext    \n",
      "paratext       7           \n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification(gold_path: Path, pred_path: Path,\n",
    "                           item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate item_class classification accuracy.\n",
    "    \n",
    "    Assumes items are in same order (or uses simple alignment).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with accuracy, per-class metrics, confusion matrix\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    # Simple alignment: assume same number and order\n",
    "    if len(gold_items) != len(pred_items):\n",
    "        print(f\"   ⚠️  Item count mismatch: gold={len(gold_items)}, pred={len(pred_items)}\")\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    correct = 0\n",
    "    confusion = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_class = gold_items[i].get('item_class', 'unknown')\n",
    "        pred_class = pred_items[i].get('item_class', 'unknown')\n",
    "        \n",
    "        confusion[gold_class][pred_class] += 1\n",
    "        if gold_class == pred_class:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / min_len if min_len > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'total': min_len,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion': dict(confusion),\n",
    "        'gold_count': len(gold_items),\n",
    "        'pred_count': len(pred_items)\n",
    "    }\n",
    "\n",
    "# Evaluate classification\n",
    "print(\"Evaluating item classification...\\n\")\n",
    "\n",
    "classification_results_all = []\n",
    "classification_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_classification(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    classification_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_classification(gold_path, pred_path,\n",
    "                                            item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    classification_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   All items - Accuracy: {result_all['accuracy']:.2%} \"\n",
    "          f\"({result_all['correct']}/{result_all['total']})\")\n",
    "    if result_contrib['total'] > 0:\n",
    "        print(f\"   Contributions - Accuracy: {result_contrib['accuracy']:.2%} \"\n",
    "              f\"({result_contrib['correct']}/{result_contrib['total']})\")\n",
    "    print()\n",
    "\n",
    "# Compute overall accuracy\n",
    "total_correct_all = sum(r['correct'] for r in classification_results_all)\n",
    "total_items_all = sum(r['total'] for r in classification_results_all)\n",
    "overall_accuracy_all = total_correct_all / total_items_all if total_items_all > 0 else 0\n",
    "\n",
    "total_correct_contrib = sum(r['correct'] for r in classification_results_contrib)\n",
    "total_items_contrib = sum(r['total'] for r in classification_results_contrib)\n",
    "overall_accuracy_contrib = total_correct_contrib / total_items_contrib if total_items_contrib > 0 else 0\n",
    "\n",
    "# Aggregate confusion matrix\n",
    "all_confusion = defaultdict(lambda: defaultdict(int))\n",
    "for result in classification_results_all:\n",
    "    for gold_class, pred_dict in result['confusion'].items():\n",
    "        for pred_class, count in pred_dict.items():\n",
    "            all_confusion[gold_class][pred_class] += count\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CLASSIFICATION ACCURACY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_all:.2%} ({total_correct_all}/{total_items_all})\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy_contrib:.2%} ({total_correct_contrib}/{total_items_contrib})\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (All Items):\")\n",
    "print(f\"{'Gold / Pred':<15}\", end=\"\")\n",
    "all_classes = sorted(set(list(all_confusion.keys()) + \n",
    "                        [pred for preds in all_confusion.values() for pred in preds.keys()]))\n",
    "for pred_class in all_classes:\n",
    "    print(f\"{pred_class:<12}\", end=\"\")\n",
    "print()\n",
    "for gold_class in all_classes:\n",
    "    print(f\"{gold_class:<15}\", end=\"\")\n",
    "    for pred_class in all_classes:\n",
    "        count = all_confusion[gold_class][pred_class]\n",
    "        print(f\"{count:<12}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c433e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metadata extraction...\n",
      "\n",
      "✓ La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "   Title F1: 1.000, Author F1: 0.000\n",
      "\n",
      "============================================================\n",
      "METADATA EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "All Items:\n",
      "   Title - P: 100.00%, R: 100.00%, F1: 1.000\n",
      "           Exact matches: 1/1\n",
      "   Author - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "            Exact matches: 0/0\n",
      "\n",
      "Contributions Only (prose + verse):\n",
      "   Title - P: 0.00%, R: 0.00%, F1: 0.000\n",
      "   Author - P: 0.00%, R: 0.00%, F1: 0.000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_metadata(gold_path: Path, pred_path: Path,\n",
    "                     item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate title and author extraction accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with title/author presence detection and exact match metrics\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    title_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    author_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'exact_match': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Title evaluation\n",
    "        gold_title = gold_item.get('item_title')\n",
    "        pred_title = pred_item.get('item_title')\n",
    "        \n",
    "        if gold_title and pred_title:\n",
    "            title_metrics['tp'] += 1\n",
    "            if gold_title == pred_title:\n",
    "                title_metrics['exact_match'] += 1\n",
    "        elif not gold_title and pred_title:\n",
    "            title_metrics['fp'] += 1\n",
    "        elif gold_title and not pred_title:\n",
    "            title_metrics['fn'] += 1\n",
    "        \n",
    "        # Author evaluation\n",
    "        gold_author = gold_item.get('item_author')\n",
    "        pred_author = pred_item.get('item_author')\n",
    "        \n",
    "        if gold_author and pred_author:\n",
    "            author_metrics['tp'] += 1\n",
    "            if gold_author == pred_author:\n",
    "                author_metrics['exact_match'] += 1\n",
    "        elif not gold_author and pred_author:\n",
    "            author_metrics['fp'] += 1\n",
    "        elif gold_author and not pred_author:\n",
    "            author_metrics['fn'] += 1\n",
    "    \n",
    "    # Compute F1 for title\n",
    "    title_p = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fp']) if (title_metrics['tp'] + title_metrics['fp']) > 0 else 0\n",
    "    title_r = title_metrics['tp'] / (title_metrics['tp'] + title_metrics['fn']) if (title_metrics['tp'] + title_metrics['fn']) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    # Compute F1 for author\n",
    "    author_p = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fp']) if (author_metrics['tp'] + author_metrics['fp']) > 0 else 0\n",
    "    author_r = author_metrics['tp'] / (author_metrics['tp'] + author_metrics['fn']) if (author_metrics['tp'] + author_metrics['fn']) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {\n",
    "            **title_metrics,\n",
    "            'precision': title_p,\n",
    "            'recall': title_r,\n",
    "            'f1': title_f1\n",
    "        },\n",
    "        'author': {\n",
    "            **author_metrics,\n",
    "            'precision': author_p,\n",
    "            'recall': author_r,\n",
    "            'f1': author_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate metadata extraction\n",
    "print(\"Evaluating metadata extraction...\\n\")\n",
    "\n",
    "metadata_results_all = []\n",
    "metadata_results_contrib = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    # All items\n",
    "    result_all = evaluate_metadata(gold_path, pred_path)\n",
    "    result_all['page'] = gold_path.name\n",
    "    metadata_results_all.append(result_all)\n",
    "    \n",
    "    # Contributions only\n",
    "    result_contrib = evaluate_metadata(gold_path, pred_path,\n",
    "                                      item_classes=['prose', 'verse'])\n",
    "    result_contrib['page'] = gold_path.name\n",
    "    metadata_results_contrib.append(result_contrib)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   Title F1: {result_all['title']['f1']:.3f}, \"\n",
    "          f\"Author F1: {result_all['author']['f1']:.3f}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "def aggregate_metadata_metrics(results):\n",
    "    total_title_tp = sum(r['title']['tp'] for r in results)\n",
    "    total_title_fp = sum(r['title']['fp'] for r in results)\n",
    "    total_title_fn = sum(r['title']['fn'] for r in results)\n",
    "    total_title_exact = sum(r['title']['exact_match'] for r in results)\n",
    "    \n",
    "    title_p = total_title_tp / (total_title_tp + total_title_fp) if (total_title_tp + total_title_fp) > 0 else 0\n",
    "    title_r = total_title_tp / (total_title_tp + total_title_fn) if (total_title_tp + total_title_fn) > 0 else 0\n",
    "    title_f1 = 2 * title_p * title_r / (title_p + title_r) if (title_p + title_r) > 0 else 0\n",
    "    \n",
    "    total_author_tp = sum(r['author']['tp'] for r in results)\n",
    "    total_author_fp = sum(r['author']['fp'] for r in results)\n",
    "    total_author_fn = sum(r['author']['fn'] for r in results)\n",
    "    total_author_exact = sum(r['author']['exact_match'] for r in results)\n",
    "    \n",
    "    author_p = total_author_tp / (total_author_tp + total_author_fp) if (total_author_tp + total_author_fp) > 0 else 0\n",
    "    author_r = total_author_tp / (total_author_tp + total_author_fn) if (total_author_tp + total_author_fn) > 0 else 0\n",
    "    author_f1 = 2 * author_p * author_r / (author_p + author_r) if (author_p + author_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'title': {'precision': title_p, 'recall': title_r, 'f1': title_f1, 'exact_match': total_title_exact, 'tp': total_title_tp},\n",
    "        'author': {'precision': author_p, 'recall': author_r, 'f1': author_f1, 'exact_match': total_author_exact, 'tp': total_author_tp}\n",
    "    }\n",
    "\n",
    "agg_all = aggregate_metadata_metrics(metadata_results_all)\n",
    "agg_contrib = aggregate_metadata_metrics(metadata_results_contrib)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METADATA EXTRACTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll Items:\")\n",
    "print(f\"   Title - P: {agg_all['title']['precision']:.2%}, R: {agg_all['title']['recall']:.2%}, F1: {agg_all['title']['f1']:.3f}\")\n",
    "print(f\"           Exact matches: {agg_all['title']['exact_match']}/{agg_all['title']['tp']}\")\n",
    "print(f\"   Author - P: {agg_all['author']['precision']:.2%}, R: {agg_all['author']['recall']:.2%}, F1: {agg_all['author']['f1']:.3f}\")\n",
    "print(f\"            Exact matches: {agg_all['author']['exact_match']}/{agg_all['author']['tp']}\")\n",
    "print(f\"\\nContributions Only (prose + verse):\")\n",
    "print(f\"   Title - P: {agg_contrib['title']['precision']:.2%}, R: {agg_contrib['title']['recall']:.2%}, F1: {agg_contrib['title']['f1']:.3f}\")\n",
    "print(f\"   Author - P: {agg_contrib['author']['precision']:.2%}, R: {agg_contrib['author']['recall']:.2%}, F1: {agg_contrib['author']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838969fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating continuation tracking...\n",
      "\n",
      "✓ La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "   is_continuation - F1: 0.000\n",
      "   continues_on_next - F1: 0.000\n",
      "\n",
      "============================================================\n",
      "CONTINUATION TRACKING SUMMARY (Contributions Only)\n",
      "============================================================\n",
      "\n",
      "is_continuation:\n",
      "   Precision: 0.00%\n",
      "   Recall: 0.00%\n",
      "   F1: 0.000\n",
      "\n",
      "continues_on_next_page:\n",
      "   Precision: 0.00%\n",
      "   Recall: 0.00%\n",
      "   F1: 0.000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_continuation_tracking(gold_path: Path, pred_path: Path,\n",
    "                                  item_classes: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate continuation field accuracy (is_continuation, continues_on_next_page).\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, F1 for each continuation field\n",
    "    \"\"\"\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Filter by item class if specified\n",
    "    if item_classes:\n",
    "        gold_items = [item for item in gold_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "        pred_items = [item for item in pred_items \n",
    "                     if item.get('item_class') in item_classes]\n",
    "    \n",
    "    min_len = min(len(gold_items), len(pred_items))\n",
    "    \n",
    "    # Metrics for is_continuation\n",
    "    is_cont_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    # Metrics for continues_on_next_page\n",
    "    continues_metrics = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0}\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        gold_item = gold_items[i]\n",
    "        pred_item = pred_items[i]\n",
    "        \n",
    "        # Evaluate is_continuation (treat absent as False)\n",
    "        gold_is_cont = gold_item.get('is_continuation', False)\n",
    "        pred_is_cont = pred_item.get('is_continuation', False)\n",
    "        \n",
    "        if gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['tp'] += 1\n",
    "        elif not gold_is_cont and pred_is_cont:\n",
    "            is_cont_metrics['fp'] += 1\n",
    "        elif gold_is_cont and not pred_is_cont:\n",
    "            is_cont_metrics['fn'] += 1\n",
    "        else:\n",
    "            is_cont_metrics['tn'] += 1\n",
    "        \n",
    "        # Evaluate continues_on_next_page\n",
    "        gold_continues = gold_item.get('continues_on_next_page', False)\n",
    "        pred_continues = pred_item.get('continues_on_next_page', False)\n",
    "        \n",
    "        if gold_continues and pred_continues:\n",
    "            continues_metrics['tp'] += 1\n",
    "        elif not gold_continues and pred_continues:\n",
    "            continues_metrics['fp'] += 1\n",
    "        elif gold_continues and not pred_continues:\n",
    "            continues_metrics['fn'] += 1\n",
    "        else:\n",
    "            continues_metrics['tn'] += 1\n",
    "    \n",
    "    # Compute metrics for is_continuation\n",
    "    is_cont_p = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fp']) if (is_cont_metrics['tp'] + is_cont_metrics['fp']) > 0 else 0\n",
    "    is_cont_r = is_cont_metrics['tp'] / (is_cont_metrics['tp'] + is_cont_metrics['fn']) if (is_cont_metrics['tp'] + is_cont_metrics['fn']) > 0 else 0\n",
    "    is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "    \n",
    "    # Compute metrics for continues_on_next_page\n",
    "    continues_p = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fp']) if (continues_metrics['tp'] + continues_metrics['fp']) > 0 else 0\n",
    "    continues_r = continues_metrics['tp'] / (continues_metrics['tp'] + continues_metrics['fn']) if (continues_metrics['tp'] + continues_metrics['fn']) > 0 else 0\n",
    "    continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'is_continuation': {\n",
    "            **is_cont_metrics,\n",
    "            'precision': is_cont_p,\n",
    "            'recall': is_cont_r,\n",
    "            'f1': is_cont_f1\n",
    "        },\n",
    "        'continues_on_next_page': {\n",
    "            **continues_metrics,\n",
    "            'precision': continues_p,\n",
    "            'recall': continues_r,\n",
    "            'f1': continues_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate continuation tracking\n",
    "print(\"Evaluating continuation tracking...\\n\")\n",
    "\n",
    "continuation_results = []\n",
    "\n",
    "for gold_path, pred_path in page_pairs:\n",
    "    result = evaluate_continuation_tracking(gold_path, pred_path,\n",
    "                                           item_classes=['prose', 'verse'])\n",
    "    result['page'] = gold_path.name\n",
    "    continuation_results.append(result)\n",
    "    \n",
    "    print(f\"✓ {gold_path.name}\")\n",
    "    print(f\"   is_continuation - F1: {result['is_continuation']['f1']:.3f}\")\n",
    "    print(f\"   continues_on_next - F1: {result['continues_on_next_page']['f1']:.3f}\\n\")\n",
    "\n",
    "# Aggregate continuation metrics\n",
    "total_is_cont_tp = sum(r['is_continuation']['tp'] for r in continuation_results)\n",
    "total_is_cont_fp = sum(r['is_continuation']['fp'] for r in continuation_results)\n",
    "total_is_cont_fn = sum(r['is_continuation']['fn'] for r in continuation_results)\n",
    "\n",
    "is_cont_p = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fp) if (total_is_cont_tp + total_is_cont_fp) > 0 else 0\n",
    "is_cont_r = total_is_cont_tp / (total_is_cont_tp + total_is_cont_fn) if (total_is_cont_tp + total_is_cont_fn) > 0 else 0\n",
    "is_cont_f1 = 2 * is_cont_p * is_cont_r / (is_cont_p + is_cont_r) if (is_cont_p + is_cont_r) > 0 else 0\n",
    "\n",
    "total_continues_tp = sum(r['continues_on_next_page']['tp'] for r in continuation_results)\n",
    "total_continues_fp = sum(r['continues_on_next_page']['fp'] for r in continuation_results)\n",
    "total_continues_fn = sum(r['continues_on_next_page']['fn'] for r in continuation_results)\n",
    "\n",
    "continues_p = total_continues_tp / (total_continues_tp + total_continues_fp) if (total_continues_tp + total_continues_fp) > 0 else 0\n",
    "continues_r = total_continues_tp / (total_continues_tp + total_continues_fn) if (total_continues_tp + total_continues_fn) > 0 else 0\n",
    "continues_f1 = 2 * continues_p * continues_r / (continues_p + continues_r) if (continues_p + continues_r) > 0 else 0\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CONTINUATION TRACKING SUMMARY (Contributions Only)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nis_continuation:\")\n",
    "print(f\"   Precision: {is_cont_p:.2%}\")\n",
    "print(f\"   Recall: {is_cont_r:.2%}\")\n",
    "print(f\"   F1: {is_cont_f1:.3f}\")\n",
    "print(f\"\\ncontinues_on_next_page:\")\n",
    "print(f\"   Precision: {continues_p:.2%}\")\n",
    "print(f\"   Recall: {continues_r:.2%}\")\n",
    "print(f\"   F1: {continues_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7cb9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EVALUATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Second_try_revised vs Gold Standard\n",
      "Evaluated on 1 pages\n",
      "\n",
      "Metric                         All Items            Contributions       \n",
      "----------------------------------------------------------------------\n",
      "TEXT QUALITY                  \n",
      "  Character Error Rate                      1.18%              0.00%\n",
      "  Word Error Rate                           2.96%              0.00%\n",
      "\n",
      "STRUCTURE QUALITY             \n",
      "  Boundary Detection F1                     1.000              0.000\n",
      "  Classification Accuracy                 100.00%              0.00%\n",
      "\n",
      "METADATA EXTRACTION           \n",
      "  Title F1                                  1.000              0.000\n",
      "  Author F1                                 0.000              0.000\n",
      "\n",
      "CONTINUATION TRACKING          N/A                  Contributions       \n",
      "  is_continuation F1                                             0.000\n",
      "  continues_on_next F1                                           0.000\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSecond_try_revised vs Gold Standard\")\n",
    "print(f\"Evaluated on {len(page_pairs)} pages\\n\")\n",
    "\n",
    "print(f\"{'Metric':<30} {'All Items':<20} {'Contributions':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Text Quality\n",
    "print(f\"{'TEXT QUALITY':<30}\")\n",
    "print(f\"{'  Character Error Rate':<30} {avg_cer_all:>18.2%} {avg_cer_contrib:>18.2%}\")\n",
    "print(f\"{'  Word Error Rate':<30} {avg_wer_all:>18.2%} {avg_wer_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Structure Quality\n",
    "print(f\"{'STRUCTURE QUALITY':<30}\")\n",
    "print(f\"{'  Boundary Detection F1':<30} {f1_all:>18.3f} {f1_contrib:>18.3f}\")\n",
    "print(f\"{'  Classification Accuracy':<30} {overall_accuracy_all:>18.2%} {overall_accuracy_contrib:>18.2%}\")\n",
    "print()\n",
    "\n",
    "# Metadata Quality\n",
    "print(f\"{'METADATA EXTRACTION':<30}\")\n",
    "print(f\"{'  Title F1':<30} {agg_all['title']['f1']:>18.3f} {agg_contrib['title']['f1']:>18.3f}\")\n",
    "print(f\"{'  Author F1':<30} {agg_all['author']['f1']:>18.3f} {agg_contrib['author']['f1']:>18.3f}\")\n",
    "print()\n",
    "\n",
    "# Continuation Tracking\n",
    "print(f\"{'CONTINUATION TRACKING':<30} {'N/A':<20} {'Contributions':<20}\")\n",
    "print(f\"{'  is_continuation F1':<30} {'':<20} {is_cont_f1:>18.3f}\")\n",
    "print(f\"{'  continues_on_next F1':<30} {'':<20} {continues_f1:>18.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e01889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Worst 5 Pages by Character Error Rate:\n",
      "1. La_Plume___revue_littéraire_[...]_bpt6k1185893k__page-001.json\n",
      "   CER: 1.18%, WER: 2.96%\n",
      "   Gold: 7 items, 1191 chars\n",
      "   Pred: 7 items, 1181 chars\n",
      "\n",
      "\n",
      "Pages with Item Count Mismatches:\n",
      "  No mismatches found!\n",
      "\n",
      "Most Common Classification Errors:\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find worst performing pages by CER\n",
    "worst_pages_cer = sorted(all_results, key=lambda x: x['cer'], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nWorst 5 Pages by Character Error Rate:\")\n",
    "for i, result in enumerate(worst_pages_cer, 1):\n",
    "    print(f\"{i}. {result['page']}\")\n",
    "    print(f\"   CER: {result['cer']:.2%}, WER: {result['wer']:.2%}\")\n",
    "    print(f\"   Gold: {result['gold_items']} items, {result['gold_chars']} chars\")\n",
    "    print(f\"   Pred: {result['pred_items']} items, {result['pred_chars']} chars\")\n",
    "    print()\n",
    "\n",
    "# Find pages with item count mismatches\n",
    "print(\"\\nPages with Item Count Mismatches:\")\n",
    "mismatches = [r for r in classification_results_all if r['gold_count'] != r['pred_count']]\n",
    "if mismatches:\n",
    "    for result in mismatches:\n",
    "        diff = result['pred_count'] - result['gold_count']\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"  • {result['page']}: Gold={result['gold_count']}, Pred={result['pred_count']} ({sign}{diff})\")\n",
    "else:\n",
    "    print(\"  No mismatches found!\")\n",
    "\n",
    "# Classification errors\n",
    "print(\"\\nMost Common Classification Errors:\")\n",
    "errors = []\n",
    "for gold_class, pred_dict in all_confusion.items():\n",
    "    for pred_class, count in pred_dict.items():\n",
    "        if gold_class != pred_class and count > 0:\n",
    "            errors.append((count, gold_class, pred_class))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, gold_class, pred_class in errors[:10]:\n",
    "    print(f\"  • {gold_class} → {pred_class}: {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
