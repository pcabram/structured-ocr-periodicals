{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6e0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 BnF Comparative Evaluation\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "\n",
      "Directories:\n",
      "  Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "  Predictions:   /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n",
      "  BnF OCR:       /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/bnf_ocr\n",
      "  Schema:        Stage1PageModel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 BnF Comparative Evaluation\n",
    "\n",
    "Compares Mistral OCR structured predictions against BnF's unstructured OCR text.\n",
    "\n",
    "Input:  Gold standard from data/gold_standard/cleaned/{magazine_name}/\n",
    "        Predictions from data/predictions/{magazine_name}/\n",
    "        BnF OCR from data/bnf_ocr/{magazine_name}/\n",
    "Output: Comparative metrics and analysis\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "import unicodedata\n",
    "\n",
    "# Project imports\n",
    "from utils.paths import PROJECT_ROOT, PREDICTIONS, GOLD_CLEAN, BNF_OCR\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only\n",
    ")\n",
    "from utils.ocr_metrics import (\n",
    "    character_error_rate,\n",
    "    word_error_rate,\n",
    "    evaluate_text_quality\n",
    ")\n",
    "\n",
    "# Paths\n",
    "GOLD_ROOT = GOLD_CLEAN\n",
    "PRED_ROOT = PREDICTIONS\n",
    "BNF_ROOT = BNF_OCR\n",
    "\n",
    "print(\"Stage 1 BnF Comparative Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Gold standard: {GOLD_ROOT}\")\n",
    "print(f\"  Predictions:   {PRED_ROOT}\")\n",
    "print(f\"  BnF OCR:       {BNF_ROOT}\")\n",
    "print(f\"  Schema:        {Stage1PageModel.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf78755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Finding Magazine Triplets\n",
      "============================================================\n",
      "\n",
      "Found 1 magazine(s) for comparison:\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893:\n",
      "  Gold files: 1\n",
      "  Pred files: 34\n",
      "  BnF files:  1\n",
      "  Matching:   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find Magazine Triplets for Comparison\n",
    "\"\"\"\n",
    "\n",
    "def find_magazine_triplets() -> List[Tuple[str, Path, Path, Path, int]]:\n",
    "    \"\"\"\n",
    "    Find magazines that have gold standard, predictions, AND BnF OCR files.\n",
    "    \n",
    "    Returns:\n",
    "        List of (magazine_name, gold_dir, pred_dir, bnf_dir, num_matching_files) tuples\n",
    "    \"\"\"\n",
    "    gold_magazines = {d.name: d for d in GOLD_ROOT.iterdir() if d.is_dir()}\n",
    "    pred_magazines = {d.name: d for d in PRED_ROOT.iterdir() if d.is_dir()}\n",
    "    bnf_magazines = {d.name: d for d in BNF_ROOT.iterdir() if d.is_dir()}\n",
    "    \n",
    "    common_magazines = set(gold_magazines.keys()) & set(pred_magazines.keys()) & set(bnf_magazines.keys())\n",
    "    \n",
    "    triplets = []\n",
    "    for mag_name in sorted(common_magazines):\n",
    "        gold_dir = gold_magazines[mag_name]\n",
    "        pred_dir = pred_magazines[mag_name]\n",
    "        bnf_dir = bnf_magazines[mag_name]\n",
    "        \n",
    "        # Find matching page files by stem\n",
    "        gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "        bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "        \n",
    "        matching_stems = set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys())\n",
    "        \n",
    "        if matching_stems:\n",
    "            triplets.append((mag_name, gold_dir, pred_dir, bnf_dir, len(matching_stems)))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "# Find triplets\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Finding Magazine Triplets\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "magazine_triplets = find_magazine_triplets()\n",
    "\n",
    "if not magazine_triplets:\n",
    "    print(\"No matching magazine triplets found.\")\n",
    "    print(\"\\nCheck that:\")\n",
    "    print(\"  1. Gold standard files exist in gold_standard/cleaned/\")\n",
    "    print(\"  2. Prediction files exist in predictions/\")\n",
    "    print(\"  3. BnF OCR files exist in bnf_ocr/\")\n",
    "    print(\"  4. Magazine names and page filenames match across all three\")\n",
    "else:\n",
    "    print(f\"Found {len(magazine_triplets)} magazine(s) for comparison:\\n\")\n",
    "    for mag_name, gold_dir, pred_dir, bnf_dir, num_files in magazine_triplets:\n",
    "        print(f\"{mag_name}:\")\n",
    "        print(f\"  Gold files: {len(list(gold_dir.glob('*.json')))}\")\n",
    "        print(f\"  Pred files: {len(list(pred_dir.glob('*.json')))}\")\n",
    "        print(f\"  BnF files:  {len(list(bnf_dir.glob('*.txt')))}\")\n",
    "        print(f\"  Matching:   {num_files}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62fb27e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Text Extraction Test\n",
      "============================================================\n",
      "\n",
      "Test page: La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "  Gold length: 4,474 chars, 717 words\n",
      "  Pred length: 4,470 chars, 718 words\n",
      "  BnF length:  4,495 chars, 753 words\n",
      "\n",
      "  Pred/Gold ratio: 1.00\n",
      "  BnF/Gold ratio:  1.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Extraction Functions\n",
    "\"\"\"\n",
    "\n",
    "def load_bnf_text(txt_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Load BnF OCR text file.\n",
    "    \n",
    "    Args:\n",
    "        txt_path: Path to BnF .txt file\n",
    "        \n",
    "    Returns:\n",
    "        Raw text content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return txt_path.read_text(encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 if UTF-8 fails\n",
    "        return txt_path.read_text(encoding='latin-1')\n",
    "\n",
    "\n",
    "def extract_text_from_json(json_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract all text from a JSON file (gold or prediction).\n",
    "    \n",
    "    Concatenates all item_text_raw fields in order.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        text_parts = []\n",
    "        \n",
    "        for item in data.get('items', []):\n",
    "            if item.get('item_text_raw'):\n",
    "                text_parts.append(item['item_text_raw'])\n",
    "        \n",
    "        # Join with space to preserve word boundaries\n",
    "        return ' '.join(text_parts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {json_path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Test extraction on first available page\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Text Extraction Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_triplets:\n",
    "    mag_name, gold_dir, pred_dir, bnf_dir, _ = magazine_triplets[0]\n",
    "    \n",
    "    # Get first matching page\n",
    "    gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "    pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "    bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "    \n",
    "    matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "    \n",
    "    if matching_stems:\n",
    "        test_stem = matching_stems[0]\n",
    "        \n",
    "        gold_text = extract_text_from_json(gold_files[test_stem])\n",
    "        pred_text = extract_text_from_json(pred_files[test_stem])\n",
    "        bnf_text = load_bnf_text(bnf_files[test_stem])\n",
    "        \n",
    "        print(f\"Test page: {test_stem}\")\n",
    "        print(f\"  Gold length: {len(gold_text):,} chars, {len(gold_text.split()):,} words\")\n",
    "        print(f\"  Pred length: {len(pred_text):,} chars, {len(pred_text.split()):,} words\")\n",
    "        print(f\"  BnF length:  {len(bnf_text):,} chars, {len(bnf_text.split()):,} words\")\n",
    "        print(f\"\\n  Pred/Gold ratio: {len(pred_text)/len(gold_text):.2f}\")\n",
    "        print(f\"  BnF/Gold ratio:  {len(bnf_text)/len(gold_text):.2f}\")\n",
    "else:\n",
    "    print(\"No magazine triplets available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a82e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Normalization Test\n",
      "============================================================\n",
      "\n",
      "Original text:\n",
      "  'L'Affiche Illustrée — 1893  \n",
      "\n",
      "  Si loin qu'on remonte...'\n",
      "\n",
      "STRICT:\n",
      "  'L'Affiche Illustrée — 1893  \n",
      "\n",
      "  Si loin qu'on remonte...'\n",
      "\n",
      "STANDARD:\n",
      "  'L'Affiche Illustrée — 1893 Si loin qu'on remonte...'\n",
      "\n",
      "LETTERS ONLY:\n",
      "  'LAfficheIllustrée1893Siloinquonremonte'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Normalization Functions\n",
    "\n",
    "Three normalization levels matching 01c:\n",
    "- Strict: Only Unicode NFC normalization (preserves everything)\n",
    "- Standard: Normalize whitespace to single spaces (RECOMMENDED)\n",
    "- Letters Only: Remove all whitespace and punctuation (pure character recognition)\n",
    "\"\"\"\n",
    "\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only,\n",
    "    token_sort_text\n",
    ")\n",
    "\n",
    "# Test normalization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Normalization Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "test_text = \"L'Affiche Illustrée — 1893  \\n\\n  Si loin qu'on remonte...\"\n",
    "\n",
    "print(f\"Original text:\")\n",
    "print(f\"  '{test_text}'\")\n",
    "print()\n",
    "\n",
    "for level_name, normalize_func in [\n",
    "    ('STRICT', normalize_text_strict),\n",
    "    ('STANDARD', normalize_text_standard),\n",
    "    ('LETTERS ONLY', normalize_text_letters_only)\n",
    "]:\n",
    "    normalized = normalize_func(test_text)\n",
    "    print(f\"{level_name}:\")\n",
    "    print(f\"  '{normalized}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a840b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Error Rate Calculation Test\n",
      "============================================================\n",
      "\n",
      "Test page: La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "\n",
      "STRICT:\n",
      "  Mistral CER: 0.004  |  WER: 0.013\n",
      "  BnF CER:     0.058  |  WER: 0.172\n",
      "\n",
      "STANDARD:\n",
      "  Mistral CER: 0.004  |  WER: 0.013\n",
      "  BnF CER:     0.039  |  WER: 0.172\n",
      "\n",
      "LETTERS ONLY:\n",
      "  Mistral CER: 0.003  |  WER: 0.000\n",
      "  BnF CER:     0.024  |  WER: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Error Rate Calculation Functions\n",
    "\n",
    "Calculate Character Error Rate (CER) and Word Error Rate (WER) using Levenshtein distance.\n",
    "Supports all three normalization levels.\n",
    "\"\"\"\n",
    "\n",
    "from utils.ocr_metrics import character_error_rate, word_error_rate\n",
    "\n",
    "\n",
    "# Test error rate calculations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Error Rate Calculation Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_triplets:\n",
    "    mag_name, gold_dir, pred_dir, bnf_dir, _ = magazine_triplets[0]\n",
    "    \n",
    "    gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "    pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "    bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "    \n",
    "    matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "    \n",
    "    if matching_stems:\n",
    "        test_stem = matching_stems[0]\n",
    "        \n",
    "        gold_text = extract_text_from_json(gold_files[test_stem])\n",
    "        pred_text = extract_text_from_json(pred_files[test_stem])\n",
    "        bnf_text = load_bnf_text(bnf_files[test_stem])\n",
    "        \n",
    "        print(f\"Test page: {test_stem}\\n\")\n",
    "        \n",
    "        for level in ['strict', 'standard', 'letters_only']:\n",
    "            print(f\"{level.upper().replace('_', ' ')}:\")\n",
    "            \n",
    "            # Mistral vs Gold\n",
    "            pred_cer = character_error_rate(gold_text, pred_text, level)\n",
    "            pred_wer = word_error_rate(gold_text, pred_text, level) if level != 'letters_only' else 0.0\n",
    "            \n",
    "            # BnF vs Gold\n",
    "            bnf_cer = character_error_rate(gold_text, bnf_text, level)\n",
    "            bnf_wer = word_error_rate(gold_text, bnf_text, level) if level != 'letters_only' else 0.0\n",
    "            \n",
    "            print(f\"  Mistral CER: {pred_cer:.3f}  |  WER: {pred_wer:.3f}\")\n",
    "            print(f\"  BnF CER:     {bnf_cer:.3f}  |  WER: {bnf_wer:.3f}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No magazine triplets available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d037750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Bag-of-Words Coverage Test\n",
      "============================================================\n",
      "\n",
      "Test page: La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "\n",
      "STRICT:\n",
      "  Mistral - P: 0.984  R: 0.986  F1: 0.985\n",
      "            Shared: 432  Unique: 7  Missing: 6\n",
      "  BnF     - P: 0.778  R: 0.842  F1: 0.809\n",
      "            Shared: 369  Unique: 105  Missing: 69\n",
      "\n",
      "STANDARD:\n",
      "  Mistral - P: 0.984  R: 0.986  F1: 0.985\n",
      "            Shared: 432  Unique: 7  Missing: 6\n",
      "  BnF     - P: 0.778  R: 0.842  F1: 0.809\n",
      "            Shared: 369  Unique: 105  Missing: 69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bag-of-Words Coverage Metrics\n",
    "\n",
    "Order-agnostic word-level comparison:\n",
    "- Precision: % of predicted words that appear in reference\n",
    "- Recall: % of reference words that appear in predictions  \n",
    "- F1: Harmonic mean\n",
    "\n",
    "This tells us about content coverage regardless of order.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_word_coverage(reference: str, hypothesis: str, normalization: str = 'standard') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate word-level precision, recall, and F1 (bag-of-words).\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text (gold standard)\n",
    "        hypothesis: Hypothesis text (OCR output)\n",
    "        normalization: Normalization level to apply\n",
    "        \n",
    "    Returns:\n",
    "        Dict with precision, recall, f1, and word counts\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        # Use standard for word-level (need word boundaries)\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    words_ref = set(ref.split())\n",
    "    words_hyp = set(hyp.split())\n",
    "    \n",
    "    if len(words_hyp) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        # Precision: % of hypothesis words that appear in reference\n",
    "        precision = len(words_ref & words_hyp) / len(words_hyp)\n",
    "    \n",
    "    if len(words_ref) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        # Recall: % of reference words that appear in hypothesis\n",
    "        recall = len(words_ref & words_hyp) / len(words_ref)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'shared_words': len(words_ref & words_hyp),\n",
    "        'unique_to_hyp': len(words_hyp - words_ref),\n",
    "        'unique_to_ref': len(words_ref - words_hyp),\n",
    "        'total_ref_words': len(words_ref),\n",
    "        'total_hyp_words': len(words_hyp)\n",
    "    }\n",
    "\n",
    "\n",
    "# Test bag-of-words coverage\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Bag-of-Words Coverage Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_triplets:\n",
    "    mag_name, gold_dir, pred_dir, bnf_dir, _ = magazine_triplets[0]\n",
    "    \n",
    "    gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "    pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "    bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "    \n",
    "    matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "    \n",
    "    if matching_stems:\n",
    "        test_stem = matching_stems[0]\n",
    "        \n",
    "        gold_text = extract_text_from_json(gold_files[test_stem])\n",
    "        pred_text = extract_text_from_json(pred_files[test_stem])\n",
    "        bnf_text = load_bnf_text(bnf_files[test_stem])\n",
    "        \n",
    "        print(f\"Test page: {test_stem}\\n\")\n",
    "        \n",
    "        for level in ['strict', 'standard']:\n",
    "            print(f\"{level.upper()}:\")\n",
    "            \n",
    "            # Mistral vs Gold\n",
    "            pred_cov = calculate_word_coverage(gold_text, pred_text, level)\n",
    "            \n",
    "            # BnF vs Gold\n",
    "            bnf_cov = calculate_word_coverage(gold_text, bnf_text, level)\n",
    "            \n",
    "            print(f\"  Mistral - P: {pred_cov['precision']:.3f}  R: {pred_cov['recall']:.3f}  F1: {pred_cov['f1']:.3f}\")\n",
    "            print(f\"            Shared: {pred_cov['shared_words']}  Unique: {pred_cov['unique_to_hyp']}  Missing: {pred_cov['unique_to_ref']}\")\n",
    "            print(f\"  BnF     - P: {bnf_cov['precision']:.3f}  R: {bnf_cov['recall']:.3f}  F1: {bnf_cov['f1']:.3f}\")\n",
    "            print(f\"            Shared: {bnf_cov['shared_words']}  Unique: {bnf_cov['unique_to_hyp']}  Missing: {bnf_cov['unique_to_ref']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No magazine triplets available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a10898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating All Pages\n",
      "============================================================\n",
      "\n",
      "Processing La_Plume_bpt6k1212187t_15-11-1893...\n",
      "  ✓ Evaluated 1 pages\n",
      "\n",
      "✓ Total pages evaluated: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Per-Page Evaluation\n",
    "\n",
    "Process all matching pages and calculate comprehensive metrics:\n",
    "1. Direct sequence comparison (preserves order)\n",
    "2. Order-agnostic comparison (token sort)\n",
    "3. Bag-of-words coverage\n",
    "All at three normalization levels: strict, standard, letters_only\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_page(\n",
    "    gold_path: Path,\n",
    "    pred_path: Path,\n",
    "    bnf_path: Path\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single page triplet across all metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with page name and all metrics for both Mistral and BnF\n",
    "    \"\"\"\n",
    "    # Extract texts\n",
    "    gold_text = extract_text_from_json(gold_path)\n",
    "    pred_text = extract_text_from_json(pred_path)\n",
    "    bnf_text = load_bnf_text(bnf_path)\n",
    "    \n",
    "    result = {\n",
    "        'page_name': gold_path.stem,\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'bnf_chars': len(bnf_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split()),\n",
    "        'bnf_words': len(bnf_text.split()),\n",
    "        'mistral': {},\n",
    "        'bnf': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics at each normalization level\n",
    "    for level in ['strict', 'standard', 'letters_only']:\n",
    "        # 1. DIRECT SEQUENCE COMPARISON (preserves order)\n",
    "        pred_cer_direct = character_error_rate(gold_text, pred_text, level)\n",
    "        pred_wer_direct = word_error_rate(gold_text, pred_text, level) if level != 'letters_only' else 0.0\n",
    "        \n",
    "        bnf_cer_direct = character_error_rate(gold_text, bnf_text, level)\n",
    "        bnf_wer_direct = word_error_rate(gold_text, bnf_text, level) if level != 'letters_only' else 0.0\n",
    "        \n",
    "        # 2. ORDER-AGNOSTIC COMPARISON (token sort from 01c)\n",
    "        if level != 'letters_only':\n",
    "            # Normalize first, then sort\n",
    "            if level == 'strict':\n",
    "                gold_norm = normalize_text_strict(gold_text)\n",
    "                pred_norm = normalize_text_strict(pred_text)\n",
    "                bnf_norm = normalize_text_strict(bnf_text)\n",
    "            else:  # standard\n",
    "                gold_norm = normalize_text_standard(gold_text)\n",
    "                pred_norm = normalize_text_standard(pred_text)\n",
    "                bnf_norm = normalize_text_standard(bnf_text)\n",
    "            \n",
    "            gold_sorted = token_sort_text(gold_norm)\n",
    "            pred_sorted = token_sort_text(pred_norm)\n",
    "            bnf_sorted = token_sort_text(bnf_norm)\n",
    "            \n",
    "            # Calculate CER/WER on sorted text\n",
    "            pred_cer_sorted = character_error_rate(gold_sorted, pred_sorted, 'strict')  # Already normalized\n",
    "            pred_wer_sorted = word_error_rate(gold_sorted, pred_sorted, 'strict')\n",
    "            \n",
    "            bnf_cer_sorted = character_error_rate(gold_sorted, bnf_sorted, 'strict')\n",
    "            bnf_wer_sorted = word_error_rate(gold_sorted, bnf_sorted, 'strict')\n",
    "        else:\n",
    "            pred_cer_sorted = pred_wer_sorted = 0.0\n",
    "            bnf_cer_sorted = bnf_wer_sorted = 0.0\n",
    "        \n",
    "        # 3. BAG-OF-WORDS COVERAGE\n",
    "        if level != 'letters_only':\n",
    "            pred_coverage = calculate_word_coverage(gold_text, pred_text, level)\n",
    "            bnf_coverage = calculate_word_coverage(gold_text, bnf_text, level)\n",
    "        else:\n",
    "            pred_coverage = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "            bnf_coverage = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        \n",
    "        # Store Mistral metrics\n",
    "        result['mistral'][level] = {\n",
    "            'cer_direct': pred_cer_direct,\n",
    "            'wer_direct': pred_wer_direct,\n",
    "            'cer_sorted': pred_cer_sorted,\n",
    "            'wer_sorted': pred_wer_sorted,\n",
    "            'word_precision': pred_coverage['precision'],\n",
    "            'word_recall': pred_coverage['recall'],\n",
    "            'word_f1': pred_coverage['f1']\n",
    "        }\n",
    "        \n",
    "        # Store BnF metrics\n",
    "        result['bnf'][level] = {\n",
    "            'cer_direct': bnf_cer_direct,\n",
    "            'wer_direct': bnf_wer_direct,\n",
    "            'cer_sorted': bnf_cer_sorted,\n",
    "            'wer_sorted': bnf_wer_sorted,\n",
    "            'word_precision': bnf_coverage['precision'],\n",
    "            'word_recall': bnf_coverage['recall'],\n",
    "            'word_f1': bnf_coverage['f1']\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_all_pages() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate all matching page triplets across all magazines.\n",
    "    \n",
    "    Returns:\n",
    "        List of page evaluation results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for mag_name, gold_dir, pred_dir, bnf_dir, _ in magazine_triplets:\n",
    "        print(f\"Processing {mag_name}...\")\n",
    "        \n",
    "        # Find matching files\n",
    "        gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "        bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "        \n",
    "        matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "        \n",
    "        for stem in matching_stems:\n",
    "            result = evaluate_page(gold_files[stem], pred_files[stem], bnf_files[stem])\n",
    "            result['magazine'] = mag_name\n",
    "            all_results.append(result)\n",
    "        \n",
    "        print(f\"  ✓ Evaluated {len(matching_stems)} pages\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating All Pages\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "all_pages = evaluate_all_pages()\n",
    "\n",
    "print(f\"\\n✓ Total pages evaluated: {len(all_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391507b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Aggregate Statistics\n",
      "============================================================\n",
      "\n",
      "\n",
      "STRICT Normalization:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Direct Sequence Comparison (preserves order):\n",
      "  Character Error Rate (CER):\n",
      "    Mistral:  0.004 (±0.000)\n",
      "    BnF:      0.058 (±0.000)\n",
      "    → Mistral is 15.3x more accurate\n",
      "  Word Error Rate (WER):\n",
      "    Mistral:  0.013 (±0.000)\n",
      "    BnF:      0.172 (±0.000)\n",
      "    → Mistral is 13.7x more accurate\n",
      "\n",
      "Order-Agnostic Comparison (token sort):\n",
      "  Character Error Rate (CER):\n",
      "    Mistral:  0.017 (±0.000)\n",
      "    BnF:      0.218 (±0.000)\n",
      "  Word Error Rate (WER):\n",
      "    Mistral:  0.017 (±0.000)\n",
      "    BnF:      0.229 (±0.000)\n",
      "\n",
      "Bag-of-Words Coverage:\n",
      "  Word F1:\n",
      "    Mistral:  0.985\n",
      "    BnF:      0.809\n",
      "  Word Precision:\n",
      "    Mistral:  0.984\n",
      "    BnF:      0.778\n",
      "  Word Recall:\n",
      "    Mistral:  0.986\n",
      "    BnF:      0.842\n",
      "\n",
      "STANDARD Normalization:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Direct Sequence Comparison (preserves order):\n",
      "  Character Error Rate (CER):\n",
      "    Mistral:  0.004 (±0.000)\n",
      "    BnF:      0.039 (±0.000)\n",
      "    → Mistral is 10.9x more accurate\n",
      "  Word Error Rate (WER):\n",
      "    Mistral:  0.013 (±0.000)\n",
      "    BnF:      0.172 (±0.000)\n",
      "    → Mistral is 13.7x more accurate\n",
      "\n",
      "Order-Agnostic Comparison (token sort):\n",
      "  Character Error Rate (CER):\n",
      "    Mistral:  0.017 (±0.000)\n",
      "    BnF:      0.218 (±0.000)\n",
      "  Word Error Rate (WER):\n",
      "    Mistral:  0.017 (±0.000)\n",
      "    BnF:      0.229 (±0.000)\n",
      "\n",
      "Bag-of-Words Coverage:\n",
      "  Word F1:\n",
      "    Mistral:  0.985\n",
      "    BnF:      0.809\n",
      "  Word Precision:\n",
      "    Mistral:  0.984\n",
      "    BnF:      0.778\n",
      "  Word Recall:\n",
      "    Mistral:  0.986\n",
      "    BnF:      0.842\n",
      "\n",
      "LETTERS ONLY Normalization:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Direct Sequence Comparison (preserves order):\n",
      "  Character Error Rate (CER):\n",
      "    Mistral:  0.003 (±0.000)\n",
      "    BnF:      0.024 (±0.000)\n",
      "    → Mistral is 7.1x more accurate\n",
      "\n",
      "\n",
      "============================================================\n",
      "Length Statistics\n",
      "============================================================\n",
      "Average page length:\n",
      "  Gold Standard:       4474 chars, 717 words\n",
      "  Mistral Predictions: 4470 chars, 718 words\n",
      "  BnF OCR:             4495 chars, 753 words\n",
      "\n",
      "  Mistral/Gold ratio:  1.00\n",
      "  BnF/Gold ratio:      1.00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aggregate Statistics Across All Pages\n",
    "\n",
    "Calculate means, medians, and standard deviations for all metrics.\n",
    "Compare Mistral vs BnF performance.\n",
    "\"\"\"\n",
    "\n",
    "def compute_aggregate_stats(all_pages: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute aggregate statistics across all evaluated pages.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with statistics for each metric at each normalization level\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'mistral': {},\n",
    "        'bnf': {}\n",
    "    }\n",
    "    \n",
    "    for system in ['mistral', 'bnf']:\n",
    "        for level in ['strict', 'standard', 'letters_only']:\n",
    "            level_metrics = {\n",
    "                'cer_direct': [],\n",
    "                'wer_direct': [],\n",
    "                'cer_sorted': [],\n",
    "                'wer_sorted': [],\n",
    "                'word_precision': [],\n",
    "                'word_recall': [],\n",
    "                'word_f1': []\n",
    "            }\n",
    "            \n",
    "            for page in all_pages:\n",
    "                metrics = page[system][level]\n",
    "                for key in level_metrics.keys():\n",
    "                    value = metrics[key]\n",
    "                    # Filter out inf/nan values\n",
    "                    if value != float('inf') and not np.isnan(value):\n",
    "                        level_metrics[key].append(value)\n",
    "            \n",
    "            # Compute statistics\n",
    "            stats[system][level] = {}\n",
    "            for metric, values in level_metrics.items():\n",
    "                if values:\n",
    "                    stats[system][level][metric] = {\n",
    "                        'mean': np.mean(values),\n",
    "                        'median': np.median(values),\n",
    "                        'std': np.std(values),\n",
    "                        'min': np.min(values),\n",
    "                        'max': np.max(values),\n",
    "                        'n': len(values)\n",
    "                    }\n",
    "                else:\n",
    "                    stats[system][level][metric] = {\n",
    "                        'mean': 0, 'median': 0, 'std': 0,\n",
    "                        'min': 0, 'max': 0, 'n': 0\n",
    "                    }\n",
    "    \n",
    "    # Length statistics\n",
    "    stats['length'] = {\n",
    "        'gold_chars': np.mean([p['gold_chars'] for p in all_pages]),\n",
    "        'pred_chars': np.mean([p['pred_chars'] for p in all_pages]),\n",
    "        'bnf_chars': np.mean([p['bnf_chars'] for p in all_pages]),\n",
    "        'gold_words': np.mean([p['gold_words'] for p in all_pages]),\n",
    "        'pred_words': np.mean([p['pred_words'] for p in all_pages]),\n",
    "        'bnf_words': np.mean([p['bnf_words'] for p in all_pages]),\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Compute statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Aggregate Statistics\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "aggregate_stats = compute_aggregate_stats(all_pages)\n",
    "\n",
    "# Display statistics by normalization level\n",
    "for level in ['strict', 'standard', 'letters_only']:\n",
    "    print(f\"\\n{level.upper().replace('_', ' ')} Normalization:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mistral_stats = aggregate_stats['mistral'][level]\n",
    "    bnf_stats = aggregate_stats['bnf'][level]\n",
    "    \n",
    "    print(f\"\\nDirect Sequence Comparison (preserves order):\")\n",
    "    print(f\"  Character Error Rate (CER):\")\n",
    "    print(f\"    Mistral:  {mistral_stats['cer_direct']['mean']:.3f} (±{mistral_stats['cer_direct']['std']:.3f})\")\n",
    "    print(f\"    BnF:      {bnf_stats['cer_direct']['mean']:.3f} (±{bnf_stats['cer_direct']['std']:.3f})\")\n",
    "    print(f\"    → Mistral is {bnf_stats['cer_direct']['mean'] / mistral_stats['cer_direct']['mean']:.1f}x more accurate\" \n",
    "          if mistral_stats['cer_direct']['mean'] > 0 else \"\")\n",
    "    \n",
    "    if level != 'letters_only':\n",
    "        print(f\"  Word Error Rate (WER):\")\n",
    "        print(f\"    Mistral:  {mistral_stats['wer_direct']['mean']:.3f} (±{mistral_stats['wer_direct']['std']:.3f})\")\n",
    "        print(f\"    BnF:      {bnf_stats['wer_direct']['mean']:.3f} (±{bnf_stats['wer_direct']['std']:.3f})\")\n",
    "        print(f\"    → Mistral is {bnf_stats['wer_direct']['mean'] / mistral_stats['wer_direct']['mean']:.1f}x more accurate\"\n",
    "              if mistral_stats['wer_direct']['mean'] > 0 else \"\")\n",
    "    \n",
    "    if level != 'letters_only':\n",
    "        print(f\"\\nOrder-Agnostic Comparison (token sort):\")\n",
    "        print(f\"  Character Error Rate (CER):\")\n",
    "        print(f\"    Mistral:  {mistral_stats['cer_sorted']['mean']:.3f} (±{mistral_stats['cer_sorted']['std']:.3f})\")\n",
    "        print(f\"    BnF:      {bnf_stats['cer_sorted']['mean']:.3f} (±{bnf_stats['cer_sorted']['std']:.3f})\")\n",
    "        \n",
    "        print(f\"  Word Error Rate (WER):\")\n",
    "        print(f\"    Mistral:  {mistral_stats['wer_sorted']['mean']:.3f} (±{mistral_stats['wer_sorted']['std']:.3f})\")\n",
    "        print(f\"    BnF:      {bnf_stats['wer_sorted']['mean']:.3f} (±{bnf_stats['wer_sorted']['std']:.3f})\")\n",
    "        \n",
    "        print(f\"\\nBag-of-Words Coverage:\")\n",
    "        print(f\"  Word F1:\")\n",
    "        print(f\"    Mistral:  {mistral_stats['word_f1']['mean']:.3f}\")\n",
    "        print(f\"    BnF:      {bnf_stats['word_f1']['mean']:.3f}\")\n",
    "        print(f\"  Word Precision:\")\n",
    "        print(f\"    Mistral:  {mistral_stats['word_precision']['mean']:.3f}\")\n",
    "        print(f\"    BnF:      {bnf_stats['word_precision']['mean']:.3f}\")\n",
    "        print(f\"  Word Recall:\")\n",
    "        print(f\"    Mistral:  {mistral_stats['word_recall']['mean']:.3f}\")\n",
    "        print(f\"    BnF:      {bnf_stats['word_recall']['mean']:.3f}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"Length Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average page length:\")\n",
    "print(f\"  Gold Standard:       {aggregate_stats['length']['gold_chars']:.0f} chars, {aggregate_stats['length']['gold_words']:.0f} words\")\n",
    "print(f\"  Mistral Predictions: {aggregate_stats['length']['pred_chars']:.0f} chars, {aggregate_stats['length']['pred_words']:.0f} words\")\n",
    "print(f\"  BnF OCR:             {aggregate_stats['length']['bnf_chars']:.0f} chars, {aggregate_stats['length']['bnf_words']:.0f} words\")\n",
    "print(f\"\\n  Mistral/Gold ratio:  {aggregate_stats['length']['pred_chars']/aggregate_stats['length']['gold_chars']:.2f}\")\n",
    "print(f\"  BnF/Gold ratio:      {aggregate_stats['length']['bnf_chars']/aggregate_stats['length']['gold_chars']:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e782dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1 BnF COMPARATIVE EVALUATION - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Normalization        Approach       Metric Mistral   BnF Mistral Advantage\n",
      "       Strict Direct Sequence          CER   0.004 0.058      15.3x better\n",
      "       Strict Direct Sequence          WER   0.013 0.172      13.7x better\n",
      "       Strict  Order-Agnostic CER (sorted)   0.017 0.218      12.6x better\n",
      "       Strict  Order-Agnostic WER (sorted)   0.017 0.229      13.7x better\n",
      "       Strict    Bag-of-Words      Word F1   0.985 0.809     17.6pp higher\n",
      "     Standard Direct Sequence          CER   0.004 0.039      10.9x better\n",
      "     Standard Direct Sequence          WER   0.013 0.172      13.7x better\n",
      "     Standard  Order-Agnostic CER (sorted)   0.017 0.218      12.6x better\n",
      "     Standard  Order-Agnostic WER (sorted)   0.017 0.229      13.7x better\n",
      "     Standard    Bag-of-Words      Word F1   0.985 0.809     17.6pp higher\n",
      " Letters Only Direct Sequence          CER   0.003 0.024       7.1x better\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "1. OCR ACCURACY (Standard normalization):\n",
      "   - Mistral CER: 0.4%\n",
      "   - BnF CER:     3.9%\n",
      "\n",
      "2. WORD-LEVEL ACCURACY:\n",
      "   - Mistral WER: 1.3%\n",
      "   - BnF WER:     17.2%\n",
      "\n",
      "3. CONTENT COVERAGE:\n",
      "   - Mistral captures 98.5% of words correctly\n",
      "   - BnF captures 80.9% of words correctly\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Final Summary Table\n",
    "\n",
    "Synthesize all findings into a comprehensive comparison table.\n",
    "\"\"\"\n",
    "\n",
    "def create_summary_table() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive summary table comparing Mistral vs BnF.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all key metrics\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    stats = aggregate_stats\n",
    "    \n",
    "    for level in ['strict', 'standard', 'letters_only']:\n",
    "        mistral_stats = stats['mistral'][level]\n",
    "        bnf_stats = stats['bnf'][level]\n",
    "        \n",
    "        # Direct Sequence - CER\n",
    "        mistral_cer = mistral_stats['cer_direct']['mean']\n",
    "        bnf_cer = bnf_stats['cer_direct']['mean']\n",
    "        improvement = bnf_cer / mistral_cer if mistral_cer > 0 else 0\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Normalization': level.replace('_', ' ').title(),\n",
    "            'Approach': 'Direct Sequence',\n",
    "            'Metric': 'CER',\n",
    "            'Mistral': f\"{mistral_cer:.3f}\",\n",
    "            'BnF': f\"{bnf_cer:.3f}\",\n",
    "            'Mistral Advantage': f\"{improvement:.1f}x better\"\n",
    "        })\n",
    "        \n",
    "        # Direct Sequence - WER (not for letters_only)\n",
    "        if level != 'letters_only':\n",
    "            mistral_wer = mistral_stats['wer_direct']['mean']\n",
    "            bnf_wer = bnf_stats['wer_direct']['mean']\n",
    "            improvement = bnf_wer / mistral_wer if mistral_wer > 0 else 0\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Normalization': level.replace('_', ' ').title(),\n",
    "                'Approach': 'Direct Sequence',\n",
    "                'Metric': 'WER',\n",
    "                'Mistral': f\"{mistral_wer:.3f}\",\n",
    "                'BnF': f\"{bnf_wer:.3f}\",\n",
    "                'Mistral Advantage': f\"{improvement:.1f}x better\"\n",
    "            })\n",
    "        \n",
    "        # Order-Agnostic (token sort) - only for strict/standard\n",
    "        if level != 'letters_only':\n",
    "            mistral_cer_sorted = mistral_stats['cer_sorted']['mean']\n",
    "            bnf_cer_sorted = bnf_stats['cer_sorted']['mean']\n",
    "            improvement = bnf_cer_sorted / mistral_cer_sorted if mistral_cer_sorted > 0 else 0\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Normalization': level.replace('_', ' ').title(),\n",
    "                'Approach': 'Order-Agnostic',\n",
    "                'Metric': 'CER (sorted)',\n",
    "                'Mistral': f\"{mistral_cer_sorted:.3f}\",\n",
    "                'BnF': f\"{bnf_cer_sorted:.3f}\",\n",
    "                'Mistral Advantage': f\"{improvement:.1f}x better\"\n",
    "            })\n",
    "            \n",
    "            mistral_wer_sorted = mistral_stats['wer_sorted']['mean']\n",
    "            bnf_wer_sorted = bnf_stats['wer_sorted']['mean']\n",
    "            improvement = bnf_wer_sorted / mistral_wer_sorted if mistral_wer_sorted > 0 else 0\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Normalization': level.replace('_', ' ').title(),\n",
    "                'Approach': 'Order-Agnostic',\n",
    "                'Metric': 'WER (sorted)',\n",
    "                'Mistral': f\"{mistral_wer_sorted:.3f}\",\n",
    "                'BnF': f\"{bnf_wer_sorted:.3f}\",\n",
    "                'Mistral Advantage': f\"{improvement:.1f}x better\"\n",
    "            })\n",
    "            \n",
    "            # Bag-of-Words\n",
    "            mistral_f1 = mistral_stats['word_f1']['mean']\n",
    "            bnf_f1 = bnf_stats['word_f1']['mean']\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Normalization': level.replace('_', ' ').title(),\n",
    "                'Approach': 'Bag-of-Words',\n",
    "                'Metric': 'Word F1',\n",
    "                'Mistral': f\"{mistral_f1:.3f}\",\n",
    "                'BnF': f\"{bnf_f1:.3f}\",\n",
    "                'Mistral Advantage': f\"{(mistral_f1 - bnf_f1)*100:.1f}pp higher\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "# Generate and display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STAGE 1 BnF COMPARATIVE EVALUATION - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "summary_df = create_summary_table()\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mistral_cer_std = aggregate_stats['mistral']['standard']['cer_direct']['mean']\n",
    "bnf_cer_std = aggregate_stats['bnf']['standard']['cer_direct']['mean']\n",
    "mistral_wer_std = aggregate_stats['mistral']['standard']['wer_direct']['mean']\n",
    "bnf_wer_std = aggregate_stats['bnf']['standard']['wer_direct']['mean']\n",
    "mistral_f1 = aggregate_stats['mistral']['standard']['word_f1']['mean']\n",
    "bnf_f1 = aggregate_stats['bnf']['standard']['word_f1']['mean']\n",
    "\n",
    "print(f\"\"\"\n",
    "1. OCR ACCURACY (Standard normalization):\n",
    "   - Mistral CER: {mistral_cer_std:.1%}\n",
    "   - BnF CER:     {bnf_cer_std:.1%}\n",
    "\n",
    "2. WORD-LEVEL ACCURACY:\n",
    "   - Mistral WER: {mistral_wer_std:.1%}\n",
    "   - BnF WER:     {bnf_wer_std:.1%}\n",
    "\n",
    "3. CONTENT COVERAGE:\n",
    "   - Mistral captures {mistral_f1:.1%} of words correctly\n",
    "   - BnF captures {bnf_f1:.1%} of words correctly\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
