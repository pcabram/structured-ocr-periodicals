{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6e0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 BnF Comparative Evaluation\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "\n",
      "Directories:\n",
      "  Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "  Predictions:   /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n",
      "  BnF OCR:       /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/bnf_ocr\n",
      "  Schema:        Stage1PageModel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 BnF Comparative Evaluation\n",
    "\n",
    "Compares Mistral OCR structured predictions against BnF's unstructured OCR text\n",
    "using ORDER-AGNOSTIC bag-of-words metrics.\n",
    "\n",
    "Input:  Gold standard from data/gold_standard/cleaned/{magazine_name}/\n",
    "        Predictions from data/predictions/{magazine_name}/\n",
    "        BnF OCR from data/bnf_ocr/{magazine_name}/\n",
    "Output: Comparative metrics and analysis\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "import unicodedata\n",
    "\n",
    "# Project imports\n",
    "from utils.paths import PROJECT_ROOT, PREDICTIONS, GOLD_CLEAN, BNF_OCR\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only\n",
    ")\n",
    "\n",
    "# Paths\n",
    "GOLD_ROOT = GOLD_CLEAN\n",
    "PRED_ROOT = PREDICTIONS\n",
    "BNF_ROOT = BNF_OCR\n",
    "\n",
    "print(\"Stage 1 BnF Comparative Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Gold standard: {GOLD_ROOT}\")\n",
    "print(f\"  Predictions:   {PRED_ROOT}\")\n",
    "print(f\"  BnF OCR:       {BNF_ROOT}\")\n",
    "print(f\"  Schema:        {Stage1PageModel.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf78755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Finding Magazine Triplets\n",
      "============================================================\n",
      "\n",
      "Found 1 magazine(s) for comparison:\n",
      "\n",
      "La_Plume_bpt6k1212187t_15-11-1893:\n",
      "  Gold files: 6\n",
      "  Pred files: 34\n",
      "  BnF files:  6\n",
      "  Matching:   6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find Magazine Triplets for Comparison\n",
    "\"\"\"\n",
    "\n",
    "def find_magazine_triplets() -> List[Tuple[str, Path, Path, Path, int]]:\n",
    "    \"\"\"\n",
    "    Find magazines that have gold standard, predictions, AND BnF OCR files.\n",
    "    \n",
    "    Returns:\n",
    "        List of (magazine_name, gold_dir, pred_dir, bnf_dir, num_matching_files) tuples\n",
    "    \"\"\"\n",
    "    gold_magazines = {d.name: d for d in GOLD_ROOT.iterdir() if d.is_dir()}\n",
    "    pred_magazines = {d.name: d for d in PRED_ROOT.iterdir() if d.is_dir()}\n",
    "    bnf_magazines = {d.name: d for d in BNF_ROOT.iterdir() if d.is_dir()}\n",
    "    \n",
    "    common_magazines = set(gold_magazines.keys()) & set(pred_magazines.keys()) & set(bnf_magazines.keys())\n",
    "    \n",
    "    triplets = []\n",
    "    for mag_name in sorted(common_magazines):\n",
    "        gold_dir = gold_magazines[mag_name]\n",
    "        pred_dir = pred_magazines[mag_name]\n",
    "        bnf_dir = bnf_magazines[mag_name]\n",
    "        \n",
    "        # Find matching page files by stem\n",
    "        gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "        bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "        \n",
    "        matching_stems = set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys())\n",
    "        \n",
    "        if matching_stems:\n",
    "            triplets.append((mag_name, gold_dir, pred_dir, bnf_dir, len(matching_stems)))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "# Find triplets\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Finding Magazine Triplets\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "magazine_triplets = find_magazine_triplets()\n",
    "\n",
    "if not magazine_triplets:\n",
    "    print(\"No matching magazine triplets found.\")\n",
    "    print(\"\\nCheck that:\")\n",
    "    print(\"  1. Gold standard files exist in gold_standard/cleaned/\")\n",
    "    print(\"  2. Prediction files exist in predictions/\")\n",
    "    print(\"  3. BnF OCR files exist in bnf_ocr/\")\n",
    "    print(\"  4. Magazine names and page filenames match across all three\")\n",
    "else:\n",
    "    print(f\"Found {len(magazine_triplets)} magazine(s) for comparison:\\n\")\n",
    "    for mag_name, gold_dir, pred_dir, bnf_dir, num_files in magazine_triplets:\n",
    "        print(f\"{mag_name}:\")\n",
    "        print(f\"  Gold files: {len(list(gold_dir.glob('*.json')))}\")\n",
    "        print(f\"  Pred files: {len(list(pred_dir.glob('*.json')))}\")\n",
    "        print(f\"  BnF files:  {len(list(bnf_dir.glob('*.txt')))}\")\n",
    "        print(f\"  Matching:   {num_files}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62fb27e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Text Extraction Test\n",
      "============================================================\n",
      "\n",
      "Test page: La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "  Gold length: 4,475 chars, 717 words\n",
      "  Pred length: 4,470 chars, 718 words\n",
      "  BnF length:  4,495 chars, 753 words\n",
      "\n",
      "  Pred/Gold ratio: 1.00\n",
      "  BnF/Gold ratio:  1.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Extraction Functions\n",
    "\"\"\"\n",
    "\n",
    "def load_bnf_text(txt_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Load BnF OCR text file.\n",
    "    \n",
    "    Args:\n",
    "        txt_path: Path to BnF .txt file\n",
    "        \n",
    "    Returns:\n",
    "        Raw text content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return txt_path.read_text(encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 if UTF-8 fails\n",
    "        return txt_path.read_text(encoding='latin-1')\n",
    "\n",
    "\n",
    "def extract_text_from_json(json_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract all text from a JSON file (gold or prediction).\n",
    "    \n",
    "    Concatenates all item_text_raw fields in order.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        text_parts = []\n",
    "        \n",
    "        for item in data.get('items', []):\n",
    "            if item.get('item_text_raw'):\n",
    "                text_parts.append(item['item_text_raw'])\n",
    "        \n",
    "        # Join with space to preserve word boundaries\n",
    "        return ' '.join(text_parts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {json_path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Test extraction on first available page\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Text Extraction Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_triplets:\n",
    "    mag_name, gold_dir, pred_dir, bnf_dir, _ = magazine_triplets[0]\n",
    "    \n",
    "    # Get first matching page\n",
    "    gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "    pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "    bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "    \n",
    "    matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "    \n",
    "    if matching_stems:\n",
    "        test_stem = matching_stems[0]\n",
    "        \n",
    "        gold_text = extract_text_from_json(gold_files[test_stem])\n",
    "        pred_text = extract_text_from_json(pred_files[test_stem])\n",
    "        bnf_text = load_bnf_text(bnf_files[test_stem])\n",
    "        \n",
    "        print(f\"Test page: {test_stem}\")\n",
    "        print(f\"  Gold length: {len(gold_text):,} chars, {len(gold_text.split()):,} words\")\n",
    "        print(f\"  Pred length: {len(pred_text):,} chars, {len(pred_text.split()):,} words\")\n",
    "        print(f\"  BnF length:  {len(bnf_text):,} chars, {len(bnf_text.split()):,} words\")\n",
    "        print(f\"\\n  Pred/Gold ratio: {len(pred_text)/len(gold_text):.2f}\")\n",
    "        print(f\"  BnF/Gold ratio:  {len(bnf_text)/len(gold_text):.2f}\")\n",
    "else:\n",
    "    print(\"No magazine triplets available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a82e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Normalization Test (Random Sample from Corpus)\n",
      "============================================================\n",
      "\n",
      "Source: La_Plume_bpt6k1212187t_15-11-1893 / La_Plume_bpt6k1212187t_15-11-1893__page-006\n",
      "Random snippet (150 chars):\n",
      "\n",
      "ORIGINAL:\n",
      "  'ant, vigoureux, d'un caprice sans pareil. Jules Chéret, n'escamotant jamais la difficulté, en triomphant toujours, malgré les proportions nécessaires '\n",
      "\n",
      "STRICT:\n",
      "  'ant, vigoureux, d'un caprice sans pareil. Jules Chéret, n'escamotant jamais la difficulté, en triomphant toujours, malgré les proportions nécessaires '\n",
      "\n",
      "STANDARD:\n",
      "  'ant, vigoureux, d'un caprice sans pareil. Jules Chéret, n'escamotant jamais la difficulté, en triomphant toujours, malgré les proportions nécessaires'\n",
      "\n",
      "LETTERS ONLY:\n",
      "  'antvigoureuxduncapricesanspareilJulesChéretnescamotantjamaisladifficultéentriomphanttoujoursmalgrélesproportionsnécessaires'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Normalization Functions\n",
    "\n",
    "Three normalization levels matching 01c:\n",
    "- Strict: Only Unicode NFC normalization (preserves everything)\n",
    "- Standard: Normalize whitespace to single spaces\n",
    "- Letters Only: Remove all whitespace and punctuation (pure character recognition)\n",
    "\"\"\"\n",
    "\n",
    "# Test normalization on real data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Normalization Test (Random Sample from Corpus)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_triplets:\n",
    "    import random\n",
    "    \n",
    "    # Get a random page from available data\n",
    "    mag_name, gold_dir, pred_dir, bnf_dir, _ = random.choice(magazine_triplets)\n",
    "    gold_files = list(gold_dir.glob(\"*.json\"))\n",
    "    \n",
    "    if gold_files:\n",
    "        random_file = random.choice(gold_files)\n",
    "        test_text = extract_text_from_json(random_file)\n",
    "        \n",
    "        # Take a random 150-character snippet\n",
    "        if len(test_text) > 150:\n",
    "            start = random.randint(0, len(test_text) - 150)\n",
    "            test_text = test_text[start:start+150]\n",
    "        \n",
    "        print(f\"Source: {mag_name} / {random_file.stem}\")\n",
    "        print(f\"Random snippet (150 chars):\\n\")\n",
    "        print(f\"ORIGINAL:\")\n",
    "        print(f\"  '{test_text}'\")\n",
    "        print()\n",
    "        \n",
    "        for level_name, normalize_func in [\n",
    "            ('STRICT', normalize_text_strict),\n",
    "            ('STANDARD', normalize_text_standard),\n",
    "            ('LETTERS ONLY', normalize_text_letters_only)\n",
    "        ]:\n",
    "            normalized = normalize_func(test_text)\n",
    "            print(f\"{level_name}:\")\n",
    "            print(f\"  '{normalized}'\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No gold files found for testing\")\n",
    "else:\n",
    "    print(\"No magazine triplets available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d037750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Bag-of-Words Coverage Test\n",
      "============================================================\n",
      "\n",
      "Test page: La_Plume_bpt6k1212187t_15-11-1893__page-001\n",
      "\n",
      "STRICT:\n",
      "  Mistral - P: 0.982  R: 0.984  F1: 0.983\n",
      "            Shared: 431  Unique: 8  Missing: 7\n",
      "  BnF     - P: 0.781  R: 0.845  F1: 0.811\n",
      "            Shared: 370  Unique: 104  Missing: 68\n",
      "\n",
      "STANDARD:\n",
      "  Mistral - P: 0.982  R: 0.984  F1: 0.983\n",
      "            Shared: 431  Unique: 8  Missing: 7\n",
      "  BnF     - P: 0.781  R: 0.845  F1: 0.811\n",
      "            Shared: 370  Unique: 104  Missing: 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bag-of-Words Coverage Metrics\n",
    "\n",
    "Order-agnostic word-level comparison:\n",
    "- Precision: % of predicted words that appear in reference\n",
    "- Recall: % of reference words that appear in predictions  \n",
    "- F1: Harmonic mean\n",
    "\n",
    "This tells us about content coverage regardless of order.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_word_coverage(reference: str, hypothesis: str, normalization: str = 'standard') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate word-level precision, recall, and F1 (bag-of-words).\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text (gold standard)\n",
    "        hypothesis: Hypothesis text (OCR output)\n",
    "        normalization: Normalization level to apply\n",
    "        \n",
    "    Returns:\n",
    "        Dict with precision, recall, f1, and word counts\n",
    "    \"\"\"\n",
    "    # Apply normalization\n",
    "    if normalization == 'strict':\n",
    "        ref = normalize_text_strict(reference)\n",
    "        hyp = normalize_text_strict(hypothesis)\n",
    "    elif normalization == 'standard':\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    elif normalization == 'letters_only':\n",
    "        # Use standard for word-level (need word boundaries)\n",
    "        ref = normalize_text_standard(reference)\n",
    "        hyp = normalize_text_standard(hypothesis)\n",
    "    else:\n",
    "        ref = reference\n",
    "        hyp = hypothesis\n",
    "    \n",
    "    words_ref = set(ref.split())\n",
    "    words_hyp = set(hyp.split())\n",
    "    \n",
    "    if len(words_hyp) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        # Precision: % of hypothesis words that appear in reference\n",
    "        precision = len(words_ref & words_hyp) / len(words_hyp)\n",
    "    \n",
    "    if len(words_ref) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        # Recall: % of reference words that appear in hypothesis\n",
    "        recall = len(words_ref & words_hyp) / len(words_ref)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'shared_words': len(words_ref & words_hyp),\n",
    "        'unique_to_hyp': len(words_hyp - words_ref),\n",
    "        'unique_to_ref': len(words_ref - words_hyp),\n",
    "        'total_ref_words': len(words_ref),\n",
    "        'total_hyp_words': len(words_hyp)\n",
    "    }\n",
    "\n",
    "\n",
    "# Test bag-of-words coverage\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Bag-of-Words Coverage Test\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if magazine_triplets:\n",
    "    mag_name, gold_dir, pred_dir, bnf_dir, _ = magazine_triplets[0]\n",
    "    \n",
    "    gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "    pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "    bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "    \n",
    "    matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "    \n",
    "    if matching_stems:\n",
    "        test_stem = matching_stems[0]\n",
    "        \n",
    "        gold_text = extract_text_from_json(gold_files[test_stem])\n",
    "        pred_text = extract_text_from_json(pred_files[test_stem])\n",
    "        bnf_text = load_bnf_text(bnf_files[test_stem])\n",
    "        \n",
    "        print(f\"Test page: {test_stem}\\n\")\n",
    "        \n",
    "        for level in ['strict', 'standard']:\n",
    "            print(f\"{level.upper()}:\")\n",
    "            \n",
    "            # Mistral vs Gold\n",
    "            pred_cov = calculate_word_coverage(gold_text, pred_text, level)\n",
    "            \n",
    "            # BnF vs Gold\n",
    "            bnf_cov = calculate_word_coverage(gold_text, bnf_text, level)\n",
    "            \n",
    "            print(f\"  Mistral - P: {pred_cov['precision']:.3f}  R: {pred_cov['recall']:.3f}  F1: {pred_cov['f1']:.3f}\")\n",
    "            print(f\"            Shared: {pred_cov['shared_words']}  Unique: {pred_cov['unique_to_hyp']}  Missing: {pred_cov['unique_to_ref']}\")\n",
    "            print(f\"  BnF     - P: {bnf_cov['precision']:.3f}  R: {bnf_cov['recall']:.3f}  F1: {bnf_cov['f1']:.3f}\")\n",
    "            print(f\"            Shared: {bnf_cov['shared_words']}  Unique: {bnf_cov['unique_to_hyp']}  Missing: {bnf_cov['unique_to_ref']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No magazine triplets available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a10898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating All Pages\n",
      "============================================================\n",
      "\n",
      "Processing La_Plume_bpt6k1212187t_15-11-1893...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Evaluated 6 pages\n",
      "\n",
      "✓ Total pages evaluated: 6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Per-Page Evaluation\n",
    "\n",
    "Process all matching pages and calculate bag-of-words coverage metrics.\n",
    "All metrics are order-agnostic (extraction order doesn't affect scores).\n",
    "Calculated at two normalization levels: strict and standard.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_page(\n",
    "    gold_path: Path,\n",
    "    pred_path: Path,\n",
    "    bnf_path: Path\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single page triplet across all metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with page name and all metrics for both Mistral and BnF\n",
    "    \"\"\"\n",
    "    # Extract texts\n",
    "    gold_text = extract_text_from_json(gold_path)\n",
    "    pred_text = extract_text_from_json(pred_path)\n",
    "    bnf_text = load_bnf_text(bnf_path)\n",
    "    \n",
    "    result = {\n",
    "        'page_name': gold_path.stem,\n",
    "        'gold_chars': len(gold_text),\n",
    "        'pred_chars': len(pred_text),\n",
    "        'bnf_chars': len(bnf_text),\n",
    "        'gold_words': len(gold_text.split()),\n",
    "        'pred_words': len(pred_text.split()),\n",
    "        'bnf_words': len(bnf_text.split()),\n",
    "        'mistral': {},\n",
    "        'bnf': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics at each normalization level\n",
    "    for level in ['strict', 'standard']:\n",
    "        # BAG-OF-WORDS COVERAGE (order-agnostic)\n",
    "        pred_coverage = calculate_word_coverage(gold_text, pred_text, level)\n",
    "        bnf_coverage = calculate_word_coverage(gold_text, bnf_text, level)\n",
    "        # Store Mistral metrics\n",
    "        result['mistral'][level] = {\n",
    "            'word_precision': pred_coverage['precision'],\n",
    "            'word_recall': pred_coverage['recall'],\n",
    "            'word_f1': pred_coverage['f1'],\n",
    "            'shared_words': pred_coverage['shared_words'],\n",
    "            'unique_to_hyp': pred_coverage['unique_to_hyp'],\n",
    "            'unique_to_ref': pred_coverage['unique_to_ref']\n",
    "        }\n",
    "        \n",
    "        # Store BnF metrics\n",
    "        result['bnf'][level] = {\n",
    "            'word_precision': bnf_coverage['precision'],\n",
    "            'word_recall': bnf_coverage['recall'],\n",
    "            'word_f1': bnf_coverage['f1'],\n",
    "            'shared_words': bnf_coverage['shared_words'],\n",
    "            'unique_to_hyp': bnf_coverage['unique_to_hyp'],\n",
    "            'unique_to_ref': bnf_coverage['unique_to_ref']\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_all_pages() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate all matching page triplets across all magazines.\n",
    "    \n",
    "    Returns:\n",
    "        List of page evaluation results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for mag_name, gold_dir, pred_dir, bnf_dir, _ in magazine_triplets:\n",
    "        print(f\"Processing {mag_name}...\")\n",
    "        \n",
    "        # Find matching files\n",
    "        gold_files = {f.stem: f for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.stem: f for f in pred_dir.glob(\"*.json\")}\n",
    "        bnf_files = {f.stem: f for f in bnf_dir.glob(\"*.txt\")}\n",
    "        \n",
    "        matching_stems = sorted(set(gold_files.keys()) & set(pred_files.keys()) & set(bnf_files.keys()))\n",
    "        \n",
    "        for stem in matching_stems:\n",
    "            result = evaluate_page(gold_files[stem], pred_files[stem], bnf_files[stem])\n",
    "            result['magazine'] = mag_name\n",
    "            all_results.append(result)\n",
    "        \n",
    "        print(f\"  ✓ Evaluated {len(matching_stems)} pages\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating All Pages\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "all_pages = evaluate_all_pages()\n",
    "\n",
    "print(f\"\\n✓ Total pages evaluated: {len(all_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391507b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Aggregate Statistics\n",
      "============================================================\n",
      "\n",
      "\n",
      "STRICT Normalization:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Word Coverage (Bag-of-Words, order-agnostic):\n",
      "  F1 Score:\n",
      "    Mistral:  0.946 (±0.072)\n",
      "    BnF:      0.817 (±0.023)\n",
      "    Δ Mistral advantage: +12.8 percentage points\n",
      "\n",
      "  Precision (% of extracted words that are in gold):\n",
      "    Mistral:  0.973 (±0.023)\n",
      "    BnF:      0.790 (±0.036)\n",
      "\n",
      "  Recall (% of gold words that were extracted):\n",
      "    Mistral:  0.927 (±0.118)\n",
      "    BnF:      0.848 (±0.021)\n",
      "\n",
      "  Average word counts per page:\n",
      "    Shared (both found):     298 vs 263\n",
      "    Unique to hypothesis:    7 vs 68\n",
      "    Missing from hypothesis: 11 vs 46\n",
      "\n",
      "STANDARD Normalization:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Word Coverage (Bag-of-Words, order-agnostic):\n",
      "  F1 Score:\n",
      "    Mistral:  0.946 (±0.072)\n",
      "    BnF:      0.817 (±0.023)\n",
      "    Δ Mistral advantage: +12.8 percentage points\n",
      "\n",
      "  Precision (% of extracted words that are in gold):\n",
      "    Mistral:  0.973 (±0.023)\n",
      "    BnF:      0.790 (±0.036)\n",
      "\n",
      "  Recall (% of gold words that were extracted):\n",
      "    Mistral:  0.927 (±0.118)\n",
      "    BnF:      0.848 (±0.021)\n",
      "\n",
      "  Average word counts per page:\n",
      "    Shared (both found):     298 vs 263\n",
      "    Unique to hypothesis:    7 vs 68\n",
      "    Missing from hypothesis: 11 vs 46\n",
      "\n",
      "\n",
      "============================================================\n",
      "Length Statistics\n",
      "============================================================\n",
      "Average page length:\n",
      "  Gold Standard:       3006 chars, 485 words\n",
      "  Mistral Predictions: 2980 chars, 480 words\n",
      "  BnF OCR:             3028 chars, 508 words\n",
      "\n",
      "  Mistral/Gold ratio:  0.99\n",
      "  BnF/Gold ratio:      1.01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aggregate Statistics Across All Pages\n",
    "\n",
    "Calculate means, medians, and standard deviations for all metrics.\n",
    "Compare Mistral vs BnF performance.\n",
    "\"\"\"\n",
    "\n",
    "def compute_aggregate_stats(all_pages: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute aggregate statistics across all evaluated pages.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with statistics for each metric at each normalization level\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'mistral': {},\n",
    "        'bnf': {}\n",
    "    }\n",
    "    \n",
    "    for system in ['mistral', 'bnf']:\n",
    "        for level in ['strict', 'standard', 'letters_only']:\n",
    "            level_metrics = {\n",
    "                'word_precision': [],\n",
    "                'word_recall': [],\n",
    "                'word_f1': [],\n",
    "                'shared_words': [],\n",
    "                'unique_to_hyp': [],\n",
    "                'unique_to_ref': []\n",
    "            }\n",
    "            \n",
    "            for page in all_pages:\n",
    "                # Skip letters_only if not present in page data\n",
    "                if level not in page[system]:\n",
    "                    continue\n",
    "                \n",
    "                metrics = page[system][level]\n",
    "                for key in level_metrics.keys():\n",
    "                    value = metrics.get(key, 0)\n",
    "                    # Filter out inf/nan values\n",
    "                    if value != float('inf') and not np.isnan(value):\n",
    "                        level_metrics[key].append(value)\n",
    "            \n",
    "            # Compute statistics\n",
    "            stats[system][level] = {}\n",
    "            for metric, values in level_metrics.items():\n",
    "                if values:\n",
    "                    stats[system][level][metric] = {\n",
    "                        'mean': np.mean(values),\n",
    "                        'median': np.median(values),\n",
    "                        'std': np.std(values),\n",
    "                        'min': np.min(values),\n",
    "                        'max': np.max(values),\n",
    "                        'n': len(values)\n",
    "                    }\n",
    "                else:\n",
    "                    stats[system][level][metric] = {\n",
    "                        'mean': 0, 'median': 0, 'std': 0,\n",
    "                        'min': 0, 'max': 0, 'n': 0\n",
    "                    }\n",
    "    \n",
    "    # Length statistics\n",
    "    stats['length'] = {\n",
    "        'gold_chars': np.mean([p['gold_chars'] for p in all_pages]),\n",
    "        'pred_chars': np.mean([p['pred_chars'] for p in all_pages]),\n",
    "        'bnf_chars': np.mean([p['bnf_chars'] for p in all_pages]),\n",
    "        'gold_words': np.mean([p['gold_words'] for p in all_pages]),\n",
    "        'pred_words': np.mean([p['pred_words'] for p in all_pages]),\n",
    "        'bnf_words': np.mean([p['bnf_words'] for p in all_pages]),\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Compute statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Aggregate Statistics\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "aggregate_stats = compute_aggregate_stats(all_pages)\n",
    "\n",
    "# Display statistics by normalization level\n",
    "for level in ['strict', 'standard']:\n",
    "    print(f\"\\n{level.upper().replace('_', ' ')} Normalization:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mistral_stats = aggregate_stats['mistral'][level]\n",
    "    bnf_stats = aggregate_stats['bnf'][level]\n",
    "    \n",
    "    print(f\"\\nWord Coverage (Bag-of-Words, order-agnostic):\")\n",
    "    \n",
    "    print(f\"  F1 Score:\")\n",
    "    print(f\"    Mistral:  {mistral_stats['word_f1']['mean']:.3f} (±{mistral_stats['word_f1']['std']:.3f})\")\n",
    "    print(f\"    BnF:      {bnf_stats['word_f1']['mean']:.3f} (±{bnf_stats['word_f1']['std']:.3f})\")\n",
    "    print(f\"    Δ Mistral advantage: {(mistral_stats['word_f1']['mean'] - bnf_stats['word_f1']['mean'])*100:+.1f} percentage points\")\n",
    "    \n",
    "    print(f\"\\n  Precision (% of extracted words that are in gold):\")\n",
    "    print(f\"    Mistral:  {mistral_stats['word_precision']['mean']:.3f} (±{mistral_stats['word_precision']['std']:.3f})\")\n",
    "    print(f\"    BnF:      {bnf_stats['word_precision']['mean']:.3f} (±{bnf_stats['word_precision']['std']:.3f})\")\n",
    "    \n",
    "    print(f\"\\n  Recall (% of gold words that were extracted):\")\n",
    "    print(f\"    Mistral:  {mistral_stats['word_recall']['mean']:.3f} (±{mistral_stats['word_recall']['std']:.3f})\")\n",
    "    print(f\"    BnF:      {bnf_stats['word_recall']['mean']:.3f} (±{bnf_stats['word_recall']['std']:.3f})\")\n",
    "    \n",
    "    print(f\"\\n  Average word counts per page:\")\n",
    "    print(f\"    Shared (both found):     {mistral_stats['shared_words']['mean']:.0f} vs {bnf_stats['shared_words']['mean']:.0f}\")\n",
    "    print(f\"    Unique to hypothesis:    {mistral_stats['unique_to_hyp']['mean']:.0f} vs {bnf_stats['unique_to_hyp']['mean']:.0f}\")\n",
    "    print(f\"    Missing from hypothesis: {mistral_stats['unique_to_ref']['mean']:.0f} vs {bnf_stats['unique_to_ref']['mean']:.0f}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"Length Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average page length:\")\n",
    "print(f\"  Gold Standard:       {aggregate_stats['length']['gold_chars']:.0f} chars, {aggregate_stats['length']['gold_words']:.0f} words\")\n",
    "print(f\"  Mistral Predictions: {aggregate_stats['length']['pred_chars']:.0f} chars, {aggregate_stats['length']['pred_words']:.0f} words\")\n",
    "print(f\"  BnF OCR:             {aggregate_stats['length']['bnf_chars']:.0f} chars, {aggregate_stats['length']['bnf_words']:.0f} words\")\n",
    "print(f\"\\n  Mistral/Gold ratio:  {aggregate_stats['length']['pred_chars']/aggregate_stats['length']['gold_chars']:.2f}\")\n",
    "print(f\"  BnF/Gold ratio:      {aggregate_stats['length']['bnf_chars']/aggregate_stats['length']['gold_chars']:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e782dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1 BnF COMPARATIVE EVALUATION - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Normalization    Metric Mistral   BnF Difference\n",
      "       Strict  F1 Score   0.946 0.817    +12.8pp\n",
      "       Strict Precision   0.973 0.790    +18.4pp\n",
      "       Strict    Recall   0.927 0.848     +7.9pp\n",
      "     Standard  F1 Score   0.946 0.817    +12.8pp\n",
      "     Standard Precision   0.973 0.790    +18.4pp\n",
      "     Standard    Recall   0.927 0.848     +7.9pp\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "1. OVERALL WORD COVERAGE (F1 Score):\n",
      "   - Mistral: 94.6%\n",
      "   - BnF:     81.7%\n",
      "   - Difference: +12.8 percentage points in Mistral's favor\n",
      "\n",
      "2. PRECISION (% of extracted words that are correct):\n",
      "   - Mistral: 97.3%\n",
      "   - BnF:     79.0%\n",
      "   - Interpretation: Mistral has fewer false positives\n",
      "\n",
      "3. RECALL (% of gold words that were extracted):\n",
      "   - Mistral: 92.7%\n",
      "   - BnF:     84.8%\n",
      "   - Interpretation: Mistral misses fewer words\n",
      "\n",
      "4. AVERAGE WORDS PER PAGE:\n",
      "   - Mistral: 298 correct, 11 missed\n",
      "   - BnF:     263 correct, 46 missed\n",
      "\n",
      "NOTE: These are order-agnostic metrics. Word extraction order differences\n",
      "      (e.g., extracting columns in different sequence) do not affect scores.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Final Summary Table\n",
    "\n",
    "Synthesize all findings into a comprehensive comparison table.\n",
    "\"\"\"\n",
    "\n",
    "def create_summary_table() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive summary table comparing Mistral vs BnF.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all key metrics\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    stats = aggregate_stats\n",
    "    \n",
    "    for level in ['strict', 'standard']:\n",
    "        mistral_stats = stats['mistral'][level]\n",
    "        bnf_stats = stats['bnf'][level]\n",
    "        \n",
    "        # F1 Score\n",
    "        mistral_f1 = mistral_stats['word_f1']['mean']\n",
    "        bnf_f1 = bnf_stats['word_f1']['mean']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Normalization': level.replace('_', ' ').title(),\n",
    "            'Metric': 'F1 Score',\n",
    "            'Mistral': f\"{mistral_f1:.3f}\",\n",
    "            'BnF': f\"{bnf_f1:.3f}\",\n",
    "            'Difference': f\"{(mistral_f1 - bnf_f1)*100:+.1f}pp\"\n",
    "        })\n",
    "        \n",
    "        # Precision\n",
    "        mistral_prec = mistral_stats['word_precision']['mean']\n",
    "        bnf_prec = bnf_stats['word_precision']['mean']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Normalization': level.replace('_', ' ').title(),\n",
    "            'Metric': 'Precision',\n",
    "            'Mistral': f\"{mistral_prec:.3f}\",\n",
    "            'BnF': f\"{bnf_prec:.3f}\",\n",
    "            'Difference': f\"{(mistral_prec - bnf_prec)*100:+.1f}pp\"\n",
    "        })\n",
    "        \n",
    "        # Recall\n",
    "        mistral_rec = mistral_stats['word_recall']['mean']\n",
    "        bnf_rec = bnf_stats['word_recall']['mean']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Normalization': level.replace('_', ' ').title(),\n",
    "            'Metric': 'Recall',\n",
    "            'Mistral': f\"{mistral_rec:.3f}\",\n",
    "            'BnF': f\"{bnf_rec:.3f}\",\n",
    "            'Difference': f\"{(mistral_rec - bnf_rec)*100:+.1f}pp\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "# Generate and display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STAGE 1 BnF COMPARATIVE EVALUATION - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "summary_df = create_summary_table()\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mistral_f1 = aggregate_stats['mistral']['standard']['word_f1']['mean']\n",
    "bnf_f1 = aggregate_stats['bnf']['standard']['word_f1']['mean']\n",
    "mistral_prec = aggregate_stats['mistral']['standard']['word_precision']['mean']\n",
    "bnf_prec = aggregate_stats['bnf']['standard']['word_precision']['mean']\n",
    "mistral_rec = aggregate_stats['mistral']['standard']['word_recall']['mean']\n",
    "bnf_rec = aggregate_stats['bnf']['standard']['word_recall']['mean']\n",
    "\n",
    "mistral_shared = aggregate_stats['mistral']['standard']['shared_words']['mean']\n",
    "bnf_shared = aggregate_stats['bnf']['standard']['shared_words']['mean']\n",
    "mistral_missing = aggregate_stats['mistral']['standard']['unique_to_ref']['mean']\n",
    "bnf_missing = aggregate_stats['bnf']['standard']['unique_to_ref']['mean']\n",
    "\n",
    "print(f\"\"\"\n",
    "1. OVERALL WORD COVERAGE (F1 Score):\n",
    "   - Mistral: {mistral_f1:.1%}\n",
    "   - BnF:     {bnf_f1:.1%}\n",
    "   - Difference: {(mistral_f1 - bnf_f1)*100:+.1f} percentage points in {\"Mistral's\" if mistral_f1 > bnf_f1 else \"BnF's\"} favor\n",
    "\n",
    "2. PRECISION (% of extracted words that are correct):\n",
    "   - Mistral: {mistral_prec:.1%}\n",
    "   - BnF:     {bnf_prec:.1%}\n",
    "   - Interpretation: {\"Mistral has fewer false positives\" if mistral_prec > bnf_prec else \"BnF has fewer false positives\"}\n",
    "\n",
    "3. RECALL (% of gold words that were extracted):\n",
    "   - Mistral: {mistral_rec:.1%}\n",
    "   - BnF:     {bnf_rec:.1%}\n",
    "   - Interpretation: {\"Mistral misses fewer words\" if mistral_rec > bnf_rec else \"BnF misses fewer words\"}\n",
    "\n",
    "4. AVERAGE WORDS PER PAGE:\n",
    "   - Mistral: {mistral_shared:.0f} correct, {mistral_missing:.0f} missed\n",
    "   - BnF:     {bnf_shared:.0f} correct, {bnf_missing:.0f} missed\n",
    "\n",
    "NOTE: These are order-agnostic metrics. Word extraction order differences\n",
    "      (e.g., extracting columns in different sequence) do not affect scores.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
