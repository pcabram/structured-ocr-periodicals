{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c08446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 OCR Extraction\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian-ramirez/Documents/These/Code/magazine_graphs/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 OCR Extraction - Mistral Document AI\n",
    "\n",
    "Extracts structured page-level data from photographs of historical French literary magazines.\n",
    "\n",
    "Input:  PDF files in data/raw/\n",
    "Output: JSON files per page in data/interim_pages/\n",
    "Schema: schemas/stage1_page.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "from mistralai import Mistral\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, **kwargs: x\n",
    "\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"extraction\")\n",
    "\n",
    "# Project root detection\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "print(\"Stage 1 OCR Extraction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73eb107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  Source directory: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/raw\n",
      "  Output directory: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/interim_pages\n",
      "  Model: mistral-ocr-latest\n",
      "  Overwrite existing: False\n",
      "  API key: ✓ Configured\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Path Setup\n",
    "\"\"\"\n",
    "\n",
    "# Input/Output directories\n",
    "SRC_ROOT = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DST_PAGES = PROJECT_ROOT / \"data\" / \"interim_pages\"   # <-- where page JSONs will be written\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in (SRC_ROOT, DST_PAGES):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extraction parameters\n",
    "CONFIG = {\n",
    "    \"model_name\": \"mistral-ocr-latest\",\n",
    "    \"overwrite\": False,  # Skip already-extracted pages\n",
    "    \"zero_pad\": 3,  # Page number padding (001, 002, ...)\n",
    "    \"max_retries\": 3,  # API retry attempts\n",
    "    \"base_delay\": 1.0,  # Initial retry delay (seconds)\n",
    "    \"max_delay\": 8.0,  # Maximum retry delay (seconds)\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Source directory: {SRC_ROOT}\")\n",
    "print(f\"  Output directory: {DST_PAGES}\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"  Overwrite existing: {CONFIG['overwrite']}\")\n",
    "\n",
    "# API key setup\n",
    "def read_api_key(\n",
    "    env_var: str = \"MISTRAL_API_KEY\",\n",
    "    fallback_file: Path = PROJECT_ROOT / \"api_key\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Read Mistral API key from environment variable or fallback file.\n",
    "    \n",
    "    Args:\n",
    "        env_var: Environment variable name\n",
    "        fallback_file: Path to file containing API key\n",
    "        \n",
    "    Returns:\n",
    "        API key string\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If API key not found\n",
    "    \"\"\"\n",
    "    key = os.environ.get(env_var)\n",
    "    if key:\n",
    "        return key.strip()\n",
    "    \n",
    "    if fallback_file.exists():\n",
    "        return fallback_file.read_text(encoding=\"utf-8\").strip()\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        f\"{env_var} not set and fallback file '{fallback_file}' not found. \"\n",
    "        \"Please set MISTRAL_API_KEY environment variable or create api_key file.\"\n",
    "    )\n",
    "\n",
    "def get_mistral_client() -> Mistral:\n",
    "    \"\"\"Initialize Mistral client with API key.\"\"\"\n",
    "    return Mistral(api_key=read_api_key())\n",
    "\n",
    "print(\"  API key: Configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31d488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "  Loaded: Stage1PageModel\n",
      "  Item classes: typing.Literal['prose', 'verse', 'ad', 'paratext', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load Stage 1 Schema\n",
    "\"\"\"\n",
    "\n",
    "# Add schemas directory to Python path\n",
    "SCHEMAS_DIR = PROJECT_ROOT / \"schemas\"\n",
    "if str(SCHEMAS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCHEMAS_DIR))\n",
    "\n",
    "# Import schema\n",
    "from stage1_page import Stage1PageModel, Stage1Item, ITEM_CLASS\n",
    "\n",
    "# Generate response format for Mistral API\n",
    "DOC_ANNOT_FMT = response_format_from_pydantic_model(Stage1PageModel)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "print(f\"  Loaded: {Stage1PageModel.__name__}\")\n",
    "print(f\"  Item classes: {ITEM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PDF Processing Utilities\n",
    "\"\"\"\n",
    "\n",
    "def count_pages(pdf_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Count number of pages in a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        \n",
    "    Returns:\n",
    "        Number of pages (0 if file cannot be read)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdf_path.open(\"rb\") as fh:\n",
    "            try:\n",
    "                reader = PdfReader(fh, strict=False)\n",
    "            except TypeError:\n",
    "                reader = PdfReader(fh)  # fallback if 'strict' arg unsupported, because I'm unsure\n",
    "            if getattr(reader, \"is_encrypted\", False) and reader.decrypt(\"\") == 0:\n",
    "                logger.warning(f\"Encrypted PDF (cannot decrypt): {pdf_path.name}\")\n",
    "                return 0\n",
    "            return len(reader.pages)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read {pdf_path.name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def encode_file_to_data_url(path: Path, mime: str = \"application/pdf\") -> str:\n",
    "    \"\"\"\n",
    "    Encode file as base64 data URL for Mistral API.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to file\n",
    "        mime: MIME type\n",
    "        \n",
    "    Returns:\n",
    "        Data URL string (data:<mime>;base64,<encoded_content>)\n",
    "    \"\"\"\n",
    "    b64 = base64.b64encode(path.read_bytes()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n",
    "\n",
    "def chunks(seq, size):\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i:i+size]\n",
    "\n",
    "def parse_annotation_response(resp) -> dict:\n",
    "    \"\"\"\n",
    "    Extract annotation dict from Mistral OCR response.\n",
    "    \n",
    "    Handles different response formats:\n",
    "    - resp.document_annotation (string or dict)\n",
    "    - resp.pages[0].document_annotation (fallback)\n",
    "    \n",
    "    Args:\n",
    "        resp: Mistral OCR API response object\n",
    "        \n",
    "    Returns:\n",
    "        Annotation dict (empty dict if parsing fails)\n",
    "    \"\"\"\n",
    "    # Try top-level document_annotation first\n",
    "    ann = getattr(resp, \"document_annotation\", None)\n",
    "    \n",
    "    if isinstance(ann, str):\n",
    "        try:\n",
    "            return json.loads(ann)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    elif isinstance(ann, dict):\n",
    "        return ann or {}\n",
    "    \n",
    "    # Fall back to pages array\n",
    "    pages = getattr(resp, \"pages\", None) or []\n",
    "    if pages:\n",
    "        page_ann = getattr(pages[0], \"document_annotation\", None)\n",
    "        \n",
    "        if isinstance(page_ann, str):\n",
    "            try:\n",
    "                return json.loads(page_ann)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "        elif isinstance(page_ann, dict):\n",
    "            return page_ann or {}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def call_with_retry(fn, *, retries: int = 3, base_delay: float = 1.0, max_delay: float = 8.0):\n",
    "    \"\"\"\n",
    "    Call function with exponential backoff retry logic.\n",
    "    \n",
    "    Args:\n",
    "        fn: Function to call (no arguments)\n",
    "        retries: Maximum number of retry attempts\n",
    "        base_delay: Initial delay between retries (seconds)\n",
    "        max_delay: Maximum delay between retries (seconds)\n",
    "        \n",
    "    Returns:\n",
    "        Function result\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If all retries fail\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            \n",
    "            delay = min(max_delay, base_delay * (2 ** attempt))\n",
    "            jitter = delay * (1 + 0.25 * random.random())\n",
    "            \n",
    "            logger.warning(f\"API call failed ({e}). Retrying in {jitter:.1f}s...\")\n",
    "            time.sleep(jitter)\n",
    "\n",
    "def validate_extraction(annot: dict, page_number: int, pdf_name: str) -> tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate extracted annotation for common issues.\n",
    "    \n",
    "    Args:\n",
    "        annot: Annotation dictionary\n",
    "        page_number: Page number (1-indexed)\n",
    "        pdf_name: PDF filename for logging\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, list_of_warnings)\n",
    "    \"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Check if items exist\n",
    "    if \"items\" not in annot:\n",
    "        warnings.append(f\"Missing 'items' field\")\n",
    "        return False, warnings\n",
    "    \n",
    "    items = annot[\"items\"]\n",
    "    \n",
    "    # Check for empty pages (valid but worth noting)\n",
    "    if len(items) == 0:\n",
    "        warnings.append(f\"Zero items extracted (possibly blank page)\")\n",
    "    \n",
    "    # Check for suspiciously short items\n",
    "    for idx, item in enumerate(items):\n",
    "        text = item.get(\"item_text_raw\", \"\")\n",
    "        if len(text) < 3:\n",
    "            warnings.append(f\"Item {idx} has very short text ({len(text)} chars)\")\n",
    "    \n",
    "    # Schema validation with Pydantic\n",
    "    try:\n",
    "        Stage1PageModel(**annot)\n",
    "    except ValidationError as e:\n",
    "        warnings.append(f\"Schema validation failed: {e}\")\n",
    "        return False, warnings\n",
    "    \n",
    "    return True, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f241343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # TEST CELL: Batch Response Structure Analysis\n",
    "# # Run this once to understand what Mistral returns for multi-page batches\n",
    "\n",
    "# def test_batch_response_structure(\n",
    "#     pdf_path: Path,\n",
    "#     test_pages: list[int] = [0, 1],  # Test with first 2 pages\n",
    "#     model_name: str = \"mistral-ocr-latest\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Test what Mistral returns when requesting multiple pages at once.\n",
    "#     This helps determine if batching is viable for your workflow.\n",
    "#     \"\"\"\n",
    "#     logger.info(\"=\"*60)\n",
    "#     logger.info(\"BATCH RESPONSE TEST\")\n",
    "#     logger.info(\"=\"*60)\n",
    "#     logger.info(f\"Testing with: {pdf_path.name}\")\n",
    "#     logger.info(f\"Pages requested: {test_pages}\")\n",
    "    \n",
    "#     # Encode PDF\n",
    "#     data_url = encode_file_to_data_url(pdf_path)\n",
    "#     client = get_mistral_client()\n",
    "    \n",
    "#     # Make batch request\n",
    "#     try:\n",
    "#         resp = client.ocr.process(\n",
    "#             model=model_name,\n",
    "#             document={\"type\": \"document_url\", \"document_url\": data_url},\n",
    "#             pages=test_pages,  # Request multiple pages\n",
    "#             document_annotation_format=DOC_ANNOT_FMT,\n",
    "#             include_image_base64=False,\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Batch request failed: {e}\")\n",
    "#         return None\n",
    "    \n",
    "#     # Analyze the response structure\n",
    "#     logger.info(\"\\n\" + \"=\"*60)\n",
    "#     logger.info(\"RESPONSE STRUCTURE ANALYSIS\")\n",
    "#     logger.info(\"=\"*60)\n",
    "    \n",
    "#     # Check top-level structure\n",
    "#     logger.info(f\"\\nResponse type: {type(resp)}\")\n",
    "#     logger.info(f\"Response attributes: {dir(resp)}\")\n",
    "    \n",
    "#     # Check document_annotation\n",
    "#     doc_annot = getattr(resp, \"document_annotation\", None)\n",
    "#     logger.info(f\"\\n--- document_annotation ---\")\n",
    "#     logger.info(f\"Type: {type(doc_annot)}\")\n",
    "    \n",
    "#     if isinstance(doc_annot, str):\n",
    "#         try:\n",
    "#             parsed = json.loads(doc_annot)\n",
    "#             logger.info(f\"Parsed type: {type(parsed)}\")\n",
    "#             logger.info(f\"Parsed keys: {parsed.keys() if isinstance(parsed, dict) else 'N/A'}\")\n",
    "            \n",
    "#             # Pretty print first 500 chars\n",
    "#             preview = json.dumps(parsed, indent=2, ensure_ascii=False)[:500]\n",
    "#             logger.info(f\"\\nFirst 500 chars:\\n{preview}...\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             logger.warning(f\"Could not parse as JSON: {e}\")\n",
    "#             logger.info(f\"Raw value (first 200 chars): {doc_annot[:200]}...\")\n",
    "#     elif isinstance(doc_annot, dict):\n",
    "#         logger.info(f\"Keys: {doc_annot.keys()}\")\n",
    "#         preview = json.dumps(doc_annot, indent=2, ensure_ascii=False)[:500]\n",
    "#         logger.info(f\"\\nFirst 500 chars:\\n{preview}...\")\n",
    "#     else:\n",
    "#         logger.info(f\"Value: {doc_annot}\")\n",
    "    \n",
    "#     # Check pages array\n",
    "#     pages_list = getattr(resp, \"pages\", None)\n",
    "#     logger.info(f\"\\n--- pages array ---\")\n",
    "#     logger.info(f\"Type: {type(pages_list)}\")\n",
    "    \n",
    "#     if pages_list:\n",
    "#         logger.info(f\"Length: {len(pages_list)}\")\n",
    "        \n",
    "#         # Check first page structure\n",
    "#         if len(pages_list) > 0:\n",
    "#             first_page = pages_list[0]\n",
    "#             logger.info(f\"\\nFirst page type: {type(first_page)}\")\n",
    "#             logger.info(f\"First page attributes: {dir(first_page)}\")\n",
    "            \n",
    "#             page_doc_annot = getattr(first_page, \"document_annotation\", None)\n",
    "#             logger.info(f\"\\nFirst page document_annotation type: {type(page_doc_annot)}\")\n",
    "            \n",
    "#             if page_doc_annot:\n",
    "#                 if isinstance(page_doc_annot, str):\n",
    "#                     try:\n",
    "#                         parsed_page = json.loads(page_doc_annot)\n",
    "#                         logger.info(f\"First page parsed keys: {parsed_page.keys() if isinstance(parsed_page, dict) else 'N/A'}\")\n",
    "#                     except:\n",
    "#                         pass\n",
    "#                 elif isinstance(page_doc_annot, dict):\n",
    "#                     logger.info(f\"First page keys: {page_doc_annot.keys()}\")\n",
    "    \n",
    "#     # Save full response for manual inspection\n",
    "#     output_test_file = PROJECT_ROOT / \"test_batch_response.json\"\n",
    "#     try:\n",
    "#         # Try to convert response to dict for saving\n",
    "#         if hasattr(resp, 'model_dump'):\n",
    "#             resp_dict = resp.model_dump()\n",
    "#         elif hasattr(resp, 'dict'):\n",
    "#             resp_dict = resp.dict()\n",
    "#         else:\n",
    "#             resp_dict = {\"raw\": str(resp)}\n",
    "        \n",
    "#         output_test_file.write_text(\n",
    "#             json.dumps(resp_dict, indent=2, ensure_ascii=False),\n",
    "#             encoding=\"utf-8\"\n",
    "#         )\n",
    "#         logger.info(f\"\\n✓ Full response saved to: {output_test_file}\")\n",
    "#         logger.info(f\"  Review this file to understand the exact structure\")\n",
    "#     except Exception as e:\n",
    "#         logger.warning(f\"Could not save response: {e}\")\n",
    "    \n",
    "#     logger.info(\"\\n\" + \"=\"*60)\n",
    "#     logger.info(\"KEY QUESTIONS TO ANSWER:\")\n",
    "#     logger.info(\"=\"*60)\n",
    "#     logger.info(\"1. Is document_annotation ONE merged annotation or ARRAY of annotations?\")\n",
    "#     logger.info(\"2. Does resp.pages contain separate annotations per page?\")\n",
    "#     logger.info(\"3. Are page boundaries preserved or merged?\")\n",
    "#     logger.info(\"4. Review the saved JSON file for the complete picture\")\n",
    "#     logger.info(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "#     return resp\n",
    "\n",
    "# # %%\n",
    "# # RUN THE TEST\n",
    "# # Pick any PDF from your data directory\n",
    "# test_pdf = next(SRC_ROOT.rglob(\"*.pdf\"))  # Gets first PDF found\n",
    "\n",
    "# logger.info(f\"Running batch test on: {test_pdf.name}\\n\")\n",
    "# test_response = test_batch_response_structure(\n",
    "#     pdf_path=test_pdf,\n",
    "#     test_pages=[0, 1]  # Test with first 2 pages\n",
    "# )\n",
    "\n",
    "# # %%\n",
    "# # Optional: Test with more pages to see scaling behavior\n",
    "# if test_response:\n",
    "#     logger.info(\"\\n\\nTesting with 4 pages...\\n\")\n",
    "#     test_response_4 = test_batch_response_structure(\n",
    "#         pdf_path=test_pdf,\n",
    "#         test_pages=[0, 1, 2, 3]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4b2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_annotation(resp) -> dict:\n",
    "    \"\"\"\n",
    "    Extract one page's annotation from a single-page request response.\n",
    "    Prefer resp.document_annotation; fall back to resp.pages[0].document_annotation.\n",
    "    \"\"\"\n",
    "    ann = getattr(resp, \"document_annotation\", None)\n",
    "    if isinstance(ann, str):\n",
    "        try:\n",
    "            return json.loads(ann)\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif isinstance(ann, dict):\n",
    "        return ann or {}\n",
    "\n",
    "    pages = getattr(resp, \"pages\", None) or []\n",
    "    if pages:\n",
    "        raw = getattr(pages[0], \"document_annotation\", None)\n",
    "        if isinstance(raw, str):\n",
    "            try:\n",
    "                return json.loads(raw)\n",
    "            except Exception:\n",
    "                return {}\n",
    "        elif isinstance(raw, dict):\n",
    "            return raw or {}\n",
    "    return {}\n",
    "\n",
    "def call_with_retry(fn, *, retries: int = 3, base_delay: float = 1.0, max_delay: float = 8.0):\n",
    "    \"\"\"Simple exponential backoff with jitter for transient API errors.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            delay = min(max_delay, base_delay * (2 ** attempt)) * (1 + 0.25 * random.random())\n",
    "            logger.warning(\"Call failed (%s). Retrying in %.1fs...\", e, delay)\n",
    "            time.sleep(delay)\n",
    "\n",
    "def _prune_empty_fields(d: dict) -> dict:\n",
    "    \"\"\"Remove keys with None/empty values, but keep 'items' even if empty list.\"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, str) and v.strip() == \"\":\n",
    "            continue\n",
    "        if isinstance(v, list):\n",
    "            out[k] = v  # keep list, even empty\n",
    "        elif isinstance(v, dict):\n",
    "            pruned = _prune_empty_fields(v)\n",
    "            if pruned:\n",
    "                out[k] = pruned\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def annotate_pdf_per_page(\n",
    "    pdf_path: Path,\n",
    "    out_root: Path = DST_PAGES,\n",
    "    model_name: str = \"mistral-ocr-latest\",\n",
    "    overwrite: bool = OVERWRITE,\n",
    ") -> int:\n",
    "    \"\"\"Call Document Annotation once per page and write one JSON per page.\n",
    "    Only keep fields/text that the model explicitly extracts from the page.\n",
    "    \"\"\"\n",
    "    n_pages = count_pages(pdf_path)\n",
    "    if n_pages == 0:\n",
    "        return 0\n",
    "\n",
    "    rel_no_ext = pdf_path.relative_to(SRC_ROOT).with_suffix(\"\")\n",
    "    out_dir = out_root / rel_no_ext\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data_url = encode_file_to_data_url(pdf_path)\n",
    "    client = get_mistral_client()\n",
    "\n",
    "    written = 0\n",
    "    for page_idx in tqdm(range(n_pages), desc=f\"Annotating (per-page) {pdf_path.name}\", leave=False):\n",
    "        out_json = out_dir / f\"{pdf_path.stem}__page-{page_idx+1:0{ZERO_PAD}d}.json\"\n",
    "        if out_json.exists() and not overwrite:\n",
    "            continue\n",
    "\n",
    "        def _call():\n",
    "            return client.ocr.process(\n",
    "                model=model_name,\n",
    "                document={\"type\": \"document_url\", \"document_url\": data_url},\n",
    "                pages=[page_idx],\n",
    "                document_annotation_format=DOC_ANNOT_FMT,\n",
    "                include_image_base64=False,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            resp = call_with_retry(_call)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Page %d of %s failed after retries: %s\", page_idx + 1, pdf_path.name, e)\n",
    "            continue\n",
    "\n",
    "        annot = parse_single_annotation(resp) or {}\n",
    "        # Ensure items key exists, but don't fabricate other fields\n",
    "        if \"items\" not in annot:\n",
    "            annot[\"items\"] = []\n",
    "\n",
    "        # Drop synthetic/default fields; keep only what's present\n",
    "        annot = _prune_empty_fields(annot)\n",
    "\n",
    "        try:\n",
    "            out_json.write_text(json.dumps(annot, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "            written += 1\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to write %s: %s\", out_json, e)\n",
    "\n",
    "    logger.info(\"Per-page annotated %s → %d/%d JSONs\", pdf_path.name, written, n_pages)\n",
    "    return written\n",
    "\n",
    "def annotate_all_pdfs_per_page(src_root: Path = SRC_ROOT) -> int:\n",
    "    total = 0\n",
    "    for pdf in tqdm([p for p in src_root.rglob(\"*.pdf\") if p.is_file()], desc=\"Annotating PDFs (per-page)\"):\n",
    "        total += annotate_pdf_per_page(pdf)\n",
    "    logger.info(\"Total per-page annotated JSONs: %d\", total)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff57818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating PDFs (per-page):   0%|          | 0/1 [00:00<?, ?it/s]2025-10-16 20:30:01,412 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:30:27,009 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:31:02,848 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:31:47,287 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:32:14,115 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:32:44,629 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:33:18,420 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:33:45,248 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:34:28,869 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:34:59,178 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:35:37,680 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:36:11,881 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:36:56,937 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:37:31,137 | INFO | HTTP Request: POST https://api.mistral.ai/v1/ocr \"HTTP/1.1 200 OK\"\n",
      "2025-10-16 20:37:31,153 | INFO | Per-page annotated La_Plume_bpt6k1185893k_1_10_1889.pdf → 14/14 JSONs\n",
      "Annotating PDFs (per-page): 100%|██████████| 1/1 [07:55<00:00, 475.99s/it]\n",
      "2025-10-16 20:37:31,158 | INFO | Total per-page annotated JSONs: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run (set OVERWRITE=True first if you need to replace existing files)\n",
    "# OVERWRITE = True\n",
    "total_per_page = annotate_all_pdfs_per_page()\n",
    "total_per_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c204499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # =============================================================================\n",
    "# # BATCH API IMPLEMENTATION (Alternative to per-page synchronous calls)\n",
    "# # =============================================================================\n",
    "# # This section uses Mistral's Batch API for 50% cost reduction\n",
    "# # It processes all pages from all PDFs as a single asynchronous job\n",
    "\n",
    "# import time\n",
    "# from typing import List, Tuple\n",
    "\n",
    "# def create_batch_requests_file(\n",
    "#     src_root: Path = SRC_ROOT,\n",
    "#     output_file: Path = PROJECT_ROOT / \"batch_requests.jsonl\",\n",
    "#     overwrite: bool = False,\n",
    "# ) -> Tuple[Path, int]:\n",
    "#     \"\"\"\n",
    "#     Create a JSONL file with one OCR request per page for all PDFs.\n",
    "    \n",
    "#     Returns:\n",
    "#         Tuple of (jsonl_path, total_requests)\n",
    "#     \"\"\"\n",
    "#     if output_file.exists() and not overwrite:\n",
    "#         logger.info(\"Batch file already exists: %s\", output_file)\n",
    "#         # Count lines to return request count\n",
    "#         with output_file.open('r') as f:\n",
    "#             count = sum(1 for _ in f)\n",
    "#         return output_file, count\n",
    "    \n",
    "#     logger.info(\"Creating batch requests file...\")\n",
    "    \n",
    "#     # Generate proper JSON schema from Pydantic model\n",
    "#     # This is what the Batch API expects\n",
    "#     doc_annot_format = {\n",
    "#         \"type\": \"json_schema\",\n",
    "#         \"json_schema\": {\n",
    "#             \"name\": Stage1PageModel.__name__,\n",
    "#             \"schema\": Stage1PageModel.model_json_schema(),\n",
    "#             \"strict\": True\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     requests = []\n",
    "    \n",
    "#     for pdf in tqdm([p for p in src_root.rglob(\"*.pdf\") if p.is_file()], desc=\"Preparing batch requests\"):\n",
    "#         n_pages = count_pages(pdf)\n",
    "#         if n_pages == 0:\n",
    "#             continue\n",
    "            \n",
    "#         data_url = encode_file_to_data_url(pdf)\n",
    "        \n",
    "#         for page_idx in range(n_pages):\n",
    "#             # Create unique ID: pdf_name__page-XXX\n",
    "#             rel_no_ext = pdf.relative_to(src_root).with_suffix(\"\")\n",
    "#             custom_id = f\"{rel_no_ext}__page-{page_idx+1:0{ZERO_PAD}d}\".replace(\"/\", \"__\")\n",
    "            \n",
    "#             request = {\n",
    "#                 \"custom_id\": custom_id,\n",
    "#                 \"body\": {\n",
    "#                     \"document\": {\n",
    "#                         \"type\": \"document_url\",\n",
    "#                         \"document_url\": data_url\n",
    "#                     },\n",
    "#                     \"pages\": [page_idx],\n",
    "#                     \"document_annotation_format\": doc_annot_format,\n",
    "#                     \"include_image_base64\": False\n",
    "#                 }\n",
    "#             }\n",
    "#             requests.append(request)\n",
    "    \n",
    "#     # Write JSONL file\n",
    "#     with output_file.open('w', encoding='utf-8') as f:\n",
    "#         for req in requests:\n",
    "#             f.write(json.dumps(req, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "#     logger.info(\"Created batch file with %d requests: %s\", len(requests), output_file)\n",
    "#     return output_file, len(requests)\n",
    "\n",
    "\n",
    "# def submit_batch_job(\n",
    "#     batch_file: Path,\n",
    "#     model_name: str = \"mistral-ocr-latest\",\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Upload batch file and create batch job.\n",
    "    \n",
    "#     Returns:\n",
    "#         job_id\n",
    "#     \"\"\"\n",
    "#     client = get_mistral_client()\n",
    "    \n",
    "#     logger.info(\"Uploading batch file: %s\", batch_file)\n",
    "#     batch_data = client.files.upload(\n",
    "#         file={\n",
    "#             \"file_name\": batch_file.name,\n",
    "#             \"content\": batch_file.open(\"rb\")\n",
    "#         },\n",
    "#         purpose=\"batch\"\n",
    "#     )\n",
    "#     logger.info(\"File uploaded with ID: %s\", batch_data.id)\n",
    "    \n",
    "#     logger.info(\"Creating batch job...\")\n",
    "#     created_job = client.batch.jobs.create(\n",
    "#         input_files=[batch_data.id],\n",
    "#         model=model_name,\n",
    "#         endpoint=\"/v1/ocr\",\n",
    "#         metadata={\"job_type\": \"stage1_ocr\", \"source\": \"notebook\"}\n",
    "#     )\n",
    "    \n",
    "#     logger.info(\"Batch job created!\")\n",
    "#     logger.info(\"  Job ID: %s\", created_job.id)\n",
    "#     logger.info(\"  Status: %s\", created_job.status)\n",
    "#     logger.info(\"  Total requests: %s\", created_job.total_requests)\n",
    "    \n",
    "#     return created_job.id\n",
    "\n",
    "\n",
    "# def monitor_batch_job(\n",
    "#     job_id: str,\n",
    "#     poll_interval: int = 10,\n",
    "#     max_wait_minutes: int = 120,\n",
    "# ) -> dict:\n",
    "#     \"\"\"\n",
    "#     Monitor batch job until completion.\n",
    "    \n",
    "#     Returns:\n",
    "#         Final job info dict\n",
    "#     \"\"\"\n",
    "#     client = get_mistral_client()\n",
    "#     start_time = time.time()\n",
    "#     max_wait_seconds = max_wait_minutes * 60\n",
    "    \n",
    "#     logger.info(\"Monitoring batch job: %s\", job_id)\n",
    "#     logger.info(\"Will check every %d seconds (max wait: %d minutes)\", poll_interval, max_wait_minutes)\n",
    "    \n",
    "#     while True:\n",
    "#         elapsed = time.time() - start_time\n",
    "#         if elapsed > max_wait_seconds:\n",
    "#             raise TimeoutError(f\"Job did not complete within {max_wait_minutes} minutes\")\n",
    "        \n",
    "#         retrieved_job = client.batch.jobs.get(job_id=job_id)\n",
    "        \n",
    "#         status = retrieved_job.status\n",
    "#         total = retrieved_job.total_requests or 0\n",
    "#         succeeded = retrieved_job.succeeded_requests or 0\n",
    "#         failed = retrieved_job.failed_requests or 0\n",
    "        \n",
    "#         if total > 0:\n",
    "#             percent = round((succeeded + failed) / total * 100, 2)\n",
    "#         else:\n",
    "#             percent = 0\n",
    "        \n",
    "#         logger.info(\n",
    "#             \"Status: %s | Total: %d | Success: %d | Failed: %d | Complete: %s%%\",\n",
    "#             status, total, succeeded, failed, percent\n",
    "#         )\n",
    "        \n",
    "#         if status in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
    "#             logger.info(\"Job finished with status: %s\", status)\n",
    "#             return {\n",
    "#                 \"job_id\": job_id,\n",
    "#                 \"status\": status,\n",
    "#                 \"total_requests\": total,\n",
    "#                 \"succeeded_requests\": succeeded,\n",
    "#                 \"failed_requests\": failed,\n",
    "#                 \"output_file\": retrieved_job.output_file,\n",
    "#                 \"error_file\": getattr(retrieved_job, \"error_file\", None)\n",
    "#             }\n",
    "        \n",
    "#         time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "# def download_and_parse_batch_results(\n",
    "#     job_info: dict,\n",
    "#     out_root: Path = DST_PAGES,\n",
    "#     results_file: Path = PROJECT_ROOT / \"batch_results.jsonl\",\n",
    "# ) -> int:\n",
    "#     \"\"\"\n",
    "#     Download batch results and save as individual page JSONs.\n",
    "    \n",
    "#     Returns:\n",
    "#         Number of pages written\n",
    "#     \"\"\"\n",
    "#     client = get_mistral_client()\n",
    "    \n",
    "#     output_file_id = job_info.get(\"output_file\")\n",
    "#     if not output_file_id:\n",
    "#         logger.error(\"No output file ID in job info\")\n",
    "#         return 0\n",
    "    \n",
    "#     logger.info(\"Downloading results from file ID: %s\", output_file_id)\n",
    "#     result_content = client.files.download(file_id=output_file_id)\n",
    "    \n",
    "#     # Save raw results for backup\n",
    "#     results_file.write_text(result_content, encoding='utf-8')\n",
    "#     logger.info(\"Raw results saved to: %s\", results_file)\n",
    "    \n",
    "#     # Parse and save individual page JSONs\n",
    "#     written = 0\n",
    "#     failed = 0\n",
    "    \n",
    "#     for line in tqdm(result_content.strip().split('\\n'), desc=\"Processing batch results\"):\n",
    "#         if not line.strip():\n",
    "#             continue\n",
    "            \n",
    "#         try:\n",
    "#             result = json.loads(line)\n",
    "#             custom_id = result.get(\"custom_id\", \"\")\n",
    "#             response_body = result.get(\"response\", {}).get(\"body\", {})\n",
    "            \n",
    "#             # Parse the custom_id to get output path\n",
    "#             # Format: path__to__pdf__page-001\n",
    "#             parts = custom_id.rsplit(\"__page-\", 1)\n",
    "#             if len(parts) != 2:\n",
    "#                 logger.warning(\"Could not parse custom_id: %s\", custom_id)\n",
    "#                 failed += 1\n",
    "#                 continue\n",
    "            \n",
    "#             pdf_rel_path = parts[0].replace(\"__\", \"/\")\n",
    "#             page_num_str = parts[1]\n",
    "            \n",
    "#             # Create output directory\n",
    "#             out_dir = out_root / pdf_rel_path\n",
    "#             out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "#             # Create output file path\n",
    "#             out_json = out_dir / f\"{Path(pdf_rel_path).name}__page-{page_num_str}.json\"\n",
    "            \n",
    "#             # Extract annotation from response\n",
    "#             # The response structure should match what we get from sync API\n",
    "#             annot = {}\n",
    "#             if \"document_annotation\" in response_body:\n",
    "#                 doc_annot = response_body[\"document_annotation\"]\n",
    "#                 if isinstance(doc_annot, str):\n",
    "#                     annot = json.loads(doc_annot)\n",
    "#                 elif isinstance(doc_annot, dict):\n",
    "#                     annot = doc_annot\n",
    "            \n",
    "#             # Ensure items key exists\n",
    "#             if \"items\" not in annot:\n",
    "#                 annot[\"items\"] = []\n",
    "            \n",
    "#             # Prune empty fields\n",
    "#             annot = _prune_empty_fields(annot)\n",
    "            \n",
    "#             # Write JSON file\n",
    "#             out_json.write_text(json.dumps(annot, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "#             written += 1\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             logger.warning(\"Failed to process result line: %s\", e)\n",
    "#             failed += 1\n",
    "    \n",
    "#     logger.info(\"Batch results processed: %d written, %d failed\", written, failed)\n",
    "#     return written\n",
    "\n",
    "\n",
    "# def run_batch_ocr_pipeline(\n",
    "#     src_root: Path = SRC_ROOT,\n",
    "#     out_root: Path = DST_PAGES,\n",
    "#     overwrite_batch_file: bool = False,\n",
    "#     poll_interval: int = 10,\n",
    "# ) -> int:\n",
    "#     \"\"\"\n",
    "#     Complete pipeline: Create batch file, submit job, monitor, download results.\n",
    "    \n",
    "#     Returns:\n",
    "#         Number of page JSONs written\n",
    "#     \"\"\"\n",
    "#     logger.info(\"=\"*60)\n",
    "#     logger.info(\"STARTING BATCH OCR PIPELINE\")\n",
    "#     logger.info(\"=\"*60)\n",
    "    \n",
    "#     # Step 1: Create batch requests file\n",
    "#     batch_file, total_requests = create_batch_requests_file(\n",
    "#         src_root=src_root,\n",
    "#         overwrite=overwrite_batch_file\n",
    "#     )\n",
    "#     logger.info(\"Batch file ready with %d requests\", total_requests)\n",
    "    \n",
    "#     # Step 2: Submit batch job\n",
    "#     job_id = submit_batch_job(batch_file)\n",
    "    \n",
    "#     # Step 3: Monitor until completion\n",
    "#     job_info = monitor_batch_job(job_id, poll_interval=poll_interval)\n",
    "    \n",
    "#     # Step 4: Download and parse results\n",
    "#     if job_info[\"status\"] == \"SUCCEEDED\":\n",
    "#         written = download_and_parse_batch_results(job_info, out_root=out_root)\n",
    "#         logger.info(\"=\"*60)\n",
    "#         logger.info(\"BATCH OCR PIPELINE COMPLETE\")\n",
    "#         logger.info(\"Total pages written: %d\", written)\n",
    "#         logger.info(\"=\"*60)\n",
    "#         return written\n",
    "#     else:\n",
    "#         logger.error(\"Batch job failed with status: %s\", job_info[\"status\"])\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # =============================================================================\n",
    "# # RUN BATCH OCR PIPELINE\n",
    "# # =============================================================================\n",
    "# # Uncomment the lines below to run the batch pipeline\n",
    "# # This will process ALL PDFs in SRC_ROOT using the Batch API\n",
    "\n",
    "# total_batch = run_batch_ocr_pipeline(\n",
    "#     src_root=SRC_ROOT,\n",
    "#     out_root=DST_PAGES,\n",
    "#     overwrite_batch_file=False,  # Set True to regenerate batch file\n",
    "#     poll_interval=10  # Check status every 10 seconds\n",
    "# )\n",
    "# total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # =============================================================================\n",
    "# # TEST BATCH WITH 4 PAGES ONLY\n",
    "# # =============================================================================\n",
    "\n",
    "# def create_small_test_batch(\n",
    "#     pdf_path: Path,\n",
    "#     num_pages: int = 2,\n",
    "#     output_file: Path = PROJECT_ROOT / \"batch_requests_test.jsonl\",\n",
    "# ) -> Tuple[Path, int]:\n",
    "#     \"\"\"\n",
    "#     Create a test batch file with just a few pages.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Creating test batch with {num_pages} pages from {pdf_path.name}\")\n",
    "    \n",
    "#     # Generate proper JSON schema from Pydantic model\n",
    "#     doc_annot_format = {\n",
    "#         \"type\": \"json_schema\",\n",
    "#         \"json_schema\": {\n",
    "#             \"name\": Stage1PageModel.__name__,\n",
    "#             \"schema\": Stage1PageModel.model_json_schema(),\n",
    "#             \"strict\": True\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     n_pages = count_pages(pdf_path)\n",
    "#     if n_pages == 0:\n",
    "#         logger.error(\"PDF has no pages!\")\n",
    "#         return output_file, 0\n",
    "    \n",
    "#     # Limit to available pages\n",
    "#     num_pages = min(num_pages, n_pages)\n",
    "#     data_url = encode_file_to_data_url(pdf_path)\n",
    "    \n",
    "#     requests = []\n",
    "#     for page_idx in range(num_pages):\n",
    "#         rel_no_ext = pdf_path.relative_to(SRC_ROOT).with_suffix(\"\")\n",
    "#         custom_id = f\"{rel_no_ext}__page-{page_idx+1:0{ZERO_PAD}d}\".replace(\"/\", \"__\")\n",
    "        \n",
    "#         request = {\n",
    "#             \"custom_id\": custom_id,\n",
    "#             \"body\": {\n",
    "#                 \"document\": {\n",
    "#                     \"type\": \"document_url\",\n",
    "#                     \"document_url\": data_url\n",
    "#                 },\n",
    "#                 \"pages\": [page_idx],\n",
    "#                 \"document_annotation_format\": doc_annot_format,\n",
    "#                 \"include_image_base64\": False\n",
    "#             }\n",
    "#         }\n",
    "#         requests.append(request)\n",
    "    \n",
    "#     # Write JSONL file\n",
    "#     with output_file.open('w', encoding='utf-8') as f:\n",
    "#         for req in requests:\n",
    "#             f.write(json.dumps(req, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "#     logger.info(f\"Created test batch file with {len(requests)} requests: {output_file}\")\n",
    "#     return output_file, len(requests)\n",
    "\n",
    "\n",
    "# def run_test_batch(\n",
    "#     pdf_path: Path,\n",
    "#     num_pages: int = 2,\n",
    "#     out_root: Path = DST_PAGES / \"batch_test\",\n",
    "#     poll_interval: int = 5,\n",
    "# ) -> int:\n",
    "#     \"\"\"\n",
    "#     Test batch pipeline with just a few pages.\n",
    "#     \"\"\"\n",
    "#     logger.info(\"=\"*60)\n",
    "#     logger.info(\"STARTING TEST BATCH (2 PAGES)\")\n",
    "#     logger.info(\"=\"*60)\n",
    "    \n",
    "#     # Create test batch file\n",
    "#     batch_file, total_requests = create_small_test_batch(\n",
    "#         pdf_path=pdf_path,\n",
    "#         num_pages=num_pages\n",
    "#     )\n",
    "#     logger.info(f\"Test batch ready with {total_requests} requests\")\n",
    "    \n",
    "#     # Submit batch job\n",
    "#     try:\n",
    "#         job_id = submit_batch_job(batch_file)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Failed to submit batch job: {e}\")\n",
    "#         return 0\n",
    "    \n",
    "#     # Monitor until completion\n",
    "#     job_info = monitor_batch_job(job_id, poll_interval=poll_interval)\n",
    "    \n",
    "#     # Download and parse results\n",
    "#     if job_info[\"status\"] == \"SUCCEEDED\":\n",
    "#         written = download_and_parse_batch_results(job_info, out_root=out_root)\n",
    "#         logger.info(\"=\"*60)\n",
    "#         logger.info(\"TEST BATCH COMPLETE\")\n",
    "#         logger.info(f\"Total pages written: {written}\")\n",
    "#         logger.info(f\"Results saved to: {out_root}\")\n",
    "#         logger.info(\"=\"*60)\n",
    "#         return written\n",
    "#     else:\n",
    "#         logger.error(f\"Batch job failed with status: {job_info['status']}\")\n",
    "#         return 0\n",
    "\n",
    "# # %%\n",
    "# # RUN THE TEST\n",
    "# test_pdf = next(SRC_ROOT.rglob(\"*.pdf\"))  # Your La Plume PDF\n",
    "\n",
    "# logger.info(f\"Testing batch with first 2 pages of: {test_pdf.name}\\n\")\n",
    "# test_result = run_test_batch(\n",
    "#     pdf_path=test_pdf,\n",
    "#     num_pages=4,\n",
    "#     poll_interval=5  # Check every 5 seconds\n",
    "# )\n",
    "# test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
