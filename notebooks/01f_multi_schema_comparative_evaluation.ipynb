{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Multi-Schema Evaluation\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "\n",
      "Directories:\n",
      "  Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "  Predictions:   /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 Multi-Schema Evaluation\n",
    "Compares multiple schema variants to identify the best performing schema.\n",
    "\n",
    "Schema Families:\n",
    "- WITH continuations: Has is_continuation + continues_on_next_page fields\n",
    "- WITHOUT continuations: Simpler structure without cross-page tracking\n",
    "\n",
    "Evaluation Strategy:\n",
    "1. Auto-detect schema families based on field presence\n",
    "2. Within-family comparison: Find best schema in each family\n",
    "3. Cross-family comparison: Compare winners on common dimensions only\n",
    "4. Final recommendation: Which schema to use and why\n",
    "\n",
    "Input:  data/predictions/{schema_version}/{magazine_name}/\n",
    "        data/gold_standard/cleaned/{magazine_name}/\n",
    "Output: Comparative metrics and recommendation\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "import inspect\n",
    "\n",
    "# Project imports\n",
    "from utils.paths import PROJECT_ROOT, PREDICTIONS, GOLD_CLEAN\n",
    "from utils.config import EVALUATION_CONFIG\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only,\n",
    "    token_sort_text\n",
    ")\n",
    "from utils.ocr_metrics import character_error_rate, word_error_rate\n",
    "from utils.evaluation import (\n",
    "    match_items,\n",
    "    load_and_match_page,\n",
    "    filter_matches_by_class,\n",
    "    get_matched_pairs,\n",
    "    evaluate_order_agnostic,\n",
    "    evaluate_structure_aware,\n",
    "    evaluate_classification,\n",
    "    evaluate_metadata_field,\n",
    "    evaluate_continuation_all_items\n",
    ")\n",
    "\n",
    "# Paths\n",
    "GOLD_ROOT = GOLD_CLEAN\n",
    "PRED_ROOT = PREDICTIONS\n",
    "\n",
    "print(\"Stage 1 Multi-Schema Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Gold standard: {GOLD_ROOT}\")\n",
    "print(f\"  Predictions:   {PRED_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c9f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Detecting Schema Families (from schema definitions)\n",
      "============================================================\n",
      "\n",
      "Found 3 schema version(s):\n",
      "\n",
      "  - stage1_page\n",
      "  - stage1_page_v2\n",
      "  - stage1_page_v2_medium\n",
      "\n",
      "Checking schema definitions in /home/fabian-ramirez/Documents/These/Code/magazine_graphs/schemas/...\n",
      "\n",
      "  stage1_page                    -> with_continuations\n",
      "  stage1_page_v2                 -> with_continuations\n",
      "  stage1_page_v2_medium          -> with_continuations\n",
      "\n",
      "------------------------------------------------------------\n",
      "Family Summary:\n",
      "  WITH continuations:    3 schema(s)\n",
      "    - stage1_page\n",
      "    - stage1_page_v2\n",
      "    - stage1_page_v2_medium\n",
      "\n",
      "  WITHOUT continuations: 0 schema(s)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schema Family Detection\n",
    "Determine schema families by inspecting schema definition files in schemas/ folder.\n",
    "This is the source of truth - not the prediction outputs.\n",
    "\"\"\"\n",
    "\n",
    "def detect_schema_family_from_definition(schema_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Detect schema family by checking if continuation fields are defined in the schema class.\n",
    "    \n",
    "    Args:\n",
    "        schema_name: Name of schema directory (e.g., 'stage1_page', 'stage1_page_v2')\n",
    "        \n",
    "    Returns:\n",
    "        'with_continuations' or 'without_continuations' or None if cannot determine\n",
    "    \"\"\"\n",
    "    # Map schema directory name to schema file\n",
    "    # Convention: stage1_page -> stage1_page.py, stage1_page_v2 -> stage1_page_v2.py\n",
    "    schema_file = PROJECT_ROOT / 'schemas' / f'{schema_name}.py'\n",
    "    \n",
    "    if not schema_file.exists():\n",
    "        print(f\"  WARNING: Schema file not found: {schema_file}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load the schema module\n",
    "        spec = importlib.util.spec_from_file_location(schema_name, schema_file)\n",
    "        if spec is None or spec.loader is None:\n",
    "            return None\n",
    "            \n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        \n",
    "        # Find the Item class (Stage1Item or similar)\n",
    "        item_class = None\n",
    "        for name, obj in inspect.getmembers(module):\n",
    "            if inspect.isclass(obj) and 'Item' in name and name != 'BaseModel':\n",
    "                item_class = obj\n",
    "                break\n",
    "        \n",
    "        if item_class is None:\n",
    "            print(f\"  WARNING: Could not find Item class in {schema_file.name}\")\n",
    "            return None\n",
    "        \n",
    "        # Check if continuation fields are defined in the model\n",
    "        model_fields = item_class.model_fields if hasattr(item_class, 'model_fields') else {}\n",
    "        \n",
    "        has_is_continuation = 'is_continuation' in model_fields\n",
    "        has_continues = 'continues_on_next_page' in model_fields\n",
    "        \n",
    "        if has_is_continuation and has_continues:\n",
    "            return 'with_continuations'\n",
    "        elif not has_is_continuation and not has_continues:\n",
    "            return 'without_continuations'\n",
    "        else:\n",
    "            # Has one but not the other - unusual\n",
    "            print(f\"  WARNING: {schema_name} has only one continuation field\")\n",
    "            return 'with_continuations' if (has_is_continuation or has_continues) else 'without_continuations'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading schema {schema_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Schema versions are in predictions/schema_evaluations/\n",
    "SCHEMA_ROOT = PRED_ROOT / 'schema_evaluations'\n",
    "\n",
    "if not SCHEMA_ROOT.exists():\n",
    "    print(f\"ERROR: Schema evaluations directory not found at {SCHEMA_ROOT}\")\n",
    "    print(\"Expected structure: predictions/schema_evaluations/{{schema_version}}/{{magazine_name}}/\")\n",
    "    available_schemas = []\n",
    "else:\n",
    "    # Find all schema directories\n",
    "    available_schemas = sorted([d for d in SCHEMA_ROOT.iterdir() if d.is_dir()])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Detecting Schema Families (from schema definitions)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if not available_schemas:\n",
    "    print(\"ERROR: No schema directories found in predictions/schema_evaluations/\")\n",
    "    print(\"Expected structure: predictions/schema_evaluations/{{schema_version}}/{{magazine_name}}/\")\n",
    "else:\n",
    "    print(f\"Found {len(available_schemas)} schema version(s):\\n\")\n",
    "    for schema_dir in available_schemas:\n",
    "        print(f\"  - {schema_dir.name}\")\n",
    "    \n",
    "    print(f\"\\nChecking schema definitions in {PROJECT_ROOT / 'schemas'}/...\\n\")\n",
    "    \n",
    "    # Group schemas by family\n",
    "    families = {\n",
    "        'with_continuations': [],\n",
    "        'without_continuations': []\n",
    "    }\n",
    "    \n",
    "    for schema_dir in available_schemas:\n",
    "        schema_name = schema_dir.name\n",
    "        family = detect_schema_family_from_definition(schema_name)\n",
    "        \n",
    "        if family:\n",
    "            families[family].append(schema_name)\n",
    "            print(f\"  {schema_name:<30} -> {family}\")\n",
    "        else:\n",
    "            print(f\"  {schema_name:<30} -> UNKNOWN (could not detect from schema file)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Family Summary:\")\n",
    "    print(f\"  WITH continuations:    {len(families['with_continuations'])} schema(s)\")\n",
    "    for schema in families['with_continuations']:\n",
    "        print(f\"    - {schema}\")\n",
    "    print(f\"\\n  WITHOUT continuations: {len(families['without_continuations'])} schema(s)\")\n",
    "    for schema in families['without_continuations']:\n",
    "        print(f\"    - {schema}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab6ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Finding Magazine Pairs for Each Schema\n",
      "============================================================\n",
      "\n",
      "stage1_page:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2_medium:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "VERIFICATION: All schemas have identical test sets\n",
      "Common magazines (2): La_Plume_bpt6k1185893k_1_10_1889, La_Plume_bpt6k1212187t_15-11-1893\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find Magazine Pairs for Each Schema\n",
    "For each schema version, find magazines with matching gold standard data.\n",
    "Ensures fair comparison by verifying all schemas have same test set.\n",
    "\"\"\"\n",
    "\n",
    "def find_magazine_pairs_for_schema(schema_name: str) -> List[Tuple[str, Path, Path, int]]:\n",
    "    \"\"\"\n",
    "    Find magazines with both gold standard and predictions for a given schema.\n",
    "    \n",
    "    Args:\n",
    "        schema_name: Name of schema directory\n",
    "        \n",
    "    Returns:\n",
    "        List of (magazine_name, gold_dir, pred_dir, num_matching_files) tuples\n",
    "    \"\"\"\n",
    "    schema_path = SCHEMA_ROOT / schema_name\n",
    "    \n",
    "    # Get all gold standard magazines\n",
    "    gold_magazines = {d.name: d for d in GOLD_ROOT.iterdir() if d.is_dir()}\n",
    "    \n",
    "    # Get all prediction magazines for this schema\n",
    "    pred_magazines = {d.name: d for d in schema_path.iterdir() if d.is_dir()}\n",
    "    \n",
    "    # Find magazines that exist in both\n",
    "    common_magazines = set(gold_magazines.keys()) & set(pred_magazines.keys())\n",
    "    \n",
    "    pairs = []\n",
    "    for mag_name in sorted(common_magazines):\n",
    "        gold_dir = gold_magazines[mag_name]\n",
    "        pred_dir = pred_magazines[mag_name]\n",
    "        \n",
    "        # Find matching page files (same filename in both directories)\n",
    "        gold_files = {f.name for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.name for f in pred_dir.glob(\"*.json\")}\n",
    "        \n",
    "        matching_files = gold_files & pred_files\n",
    "        \n",
    "        if matching_files:\n",
    "            pairs.append((mag_name, gold_dir, pred_dir, len(matching_files)))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Find pairs for all schemas\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Finding Magazine Pairs for Each Schema\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "schema_magazine_pairs = {}\n",
    "\n",
    "for schema_dir in available_schemas:\n",
    "    schema_name = schema_dir.name\n",
    "    pairs = find_magazine_pairs_for_schema(schema_name)\n",
    "    schema_magazine_pairs[schema_name] = pairs\n",
    "    \n",
    "    print(f\"{schema_name}:\")\n",
    "    if not pairs:\n",
    "        print(\"  No matching magazines found\")\n",
    "    else:\n",
    "        for mag_name, gold_dir, pred_dir, num_files in pairs:\n",
    "            print(f\"  {mag_name}: {num_files} matching pages\")\n",
    "    print()\n",
    "\n",
    "# Verify all schemas have same test set (CRITICAL for fair comparison)\n",
    "all_magazine_sets = [\n",
    "    set(mag_name for mag_name, _, _, _ in pairs)\n",
    "    for pairs in schema_magazine_pairs.values()\n",
    "]\n",
    "\n",
    "if len(set(map(frozenset, all_magazine_sets))) == 1:\n",
    "    print(\"VERIFICATION: All schemas have identical test sets\")\n",
    "    common_magazines = all_magazine_sets[0]\n",
    "    print(f\"Common magazines ({len(common_magazines)}): {', '.join(sorted(common_magazines))}\")\n",
    "else:\n",
    "    print(\"WARNING: Schemas have different test sets - comparison may not be fair\")\n",
    "    for schema_name, pairs in schema_magazine_pairs.items():\n",
    "        mags = set(mag_name for mag_name, _, _, _ in pairs)\n",
    "        print(f\"  {schema_name}: {mags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item matching functions loaded\n",
      "Similarity threshold: 0.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Item Matching Functions\n",
    "Reused from 01c - match gold items to predicted items using text similarity.\n",
    "These functions determine which predicted item corresponds to which gold item.\n",
    "\"\"\"\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for similarity comparison.\n",
    "    - Lowercase\n",
    "    - Remove punctuation\n",
    "    - Normalize whitespace to single spaces\n",
    "    - Strip leading/trailing whitespace\n",
    "    \n",
    "    This makes the similarity matching more robust to minor OCR variations.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity ratio between two texts using SequenceMatcher.\n",
    "    \n",
    "    Returns:\n",
    "        Float between 0.0 (completely different) and 1.0 (identical)\n",
    "    \"\"\"\n",
    "    t1 = normalize_text(text1)\n",
    "    t2 = normalize_text(text2)\n",
    "    \n",
    "    if not t1 and not t2:\n",
    "        return 1.0  # Both empty\n",
    "    if not t1 or not t2:\n",
    "        return 0.0  # One empty, one not\n",
    "    \n",
    "    return SequenceMatcher(None, t1, t2).ratio()\n",
    "\n",
    "def match_items(\n",
    "    gold_items: List[Dict], \n",
    "    pred_items: List[Dict],\n",
    "    similarity_threshold: float = EVALUATION_CONFIG.similarity_threshold\n",
    ") -> Tuple[List[Tuple[int, int, float]], Set[int], Set[int]]:\n",
    "    \"\"\"\n",
    "    Match gold items to prediction items using greedy best-match algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "        For each gold item:\n",
    "        1. Compare its text against all unmatched predicted items\n",
    "        2. Find the best match (highest similarity score)\n",
    "        3. If score >= threshold, accept the match\n",
    "        4. Mark that predicted item as matched (can't be reused)\n",
    "        5. Move to next gold item\n",
    "    \n",
    "    This ensures each predicted item matches at most one gold item.\n",
    "    \n",
    "    Args:\n",
    "        gold_items: List of gold standard items\n",
    "        pred_items: List of predicted items\n",
    "        similarity_threshold: Minimum similarity score to consider a match\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - matches: List of (gold_idx, pred_idx, similarity_score)\n",
    "        - unmatched_gold: Set of gold indices with no match\n",
    "        - unmatched_pred: Set of pred indices with no match\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    matched_pred_indices = set()\n",
    "    unmatched_gold = set()\n",
    "    \n",
    "    for gold_idx, gold_item in enumerate(gold_items):\n",
    "        gold_text = gold_item.get('item_text_raw', '')\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_pred_idx = None\n",
    "        \n",
    "        # Find best matching predicted item\n",
    "        for pred_idx, pred_item in enumerate(pred_items):\n",
    "            if pred_idx in matched_pred_indices:\n",
    "                continue  # Already matched to another gold item\n",
    "            \n",
    "            pred_text = pred_item.get('item_text_raw', '')\n",
    "            score = text_similarity(gold_text, pred_text)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred_idx = pred_idx\n",
    "        \n",
    "        # Accept match if above threshold\n",
    "        if best_score >= similarity_threshold and best_pred_idx is not None:\n",
    "            matches.append((gold_idx, best_pred_idx, best_score))\n",
    "            matched_pred_indices.add(best_pred_idx)\n",
    "        else:\n",
    "            unmatched_gold.add(gold_idx)\n",
    "    \n",
    "    # Find predicted items that never got matched\n",
    "    unmatched_pred = set(range(len(pred_items))) - matched_pred_indices\n",
    "    \n",
    "    return matches, unmatched_gold, unmatched_pred\n",
    "\n",
    "def load_and_match_page(\n",
    "    gold_path: Path, \n",
    "    pred_path: Path,\n",
    "    similarity_threshold: float = EVALUATION_CONFIG.similarity_threshold\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Load a page pair (gold + prediction) and match their items.\n",
    "    \n",
    "    Args:\n",
    "        gold_path: Path to gold standard JSON\n",
    "        pred_path: Path to prediction JSON\n",
    "        similarity_threshold: Minimum similarity for matching\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - gold_items: All gold items\n",
    "        - pred_items: All pred items\n",
    "        - matches: List of (gold_idx, pred_idx, score) tuples\n",
    "        - unmatched_gold: Set of unmatched gold indices\n",
    "        - unmatched_pred: Set of unmatched pred indices\n",
    "        - page_name: Filename\n",
    "    \"\"\"\n",
    "    # Load and validate gold standard\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        gold_data = json.load(f)\n",
    "    gold_page = Stage1PageModel.model_validate(gold_data)\n",
    "    gold_data = gold_page.model_dump()\n",
    "    \n",
    "    # Load and validate predictions\n",
    "    with open(pred_path, 'r', encoding='utf-8') as f:\n",
    "        pred_data = json.load(f)\n",
    "    pred_page = Stage1PageModel.model_validate(pred_data)\n",
    "    pred_data = pred_page.model_dump()\n",
    "    \n",
    "    gold_items = gold_data.get('items', [])\n",
    "    pred_items = pred_data.get('items', [])\n",
    "    \n",
    "    # Match items\n",
    "    matches, unmatched_gold, unmatched_pred = match_items(\n",
    "        gold_items, pred_items, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gold_items': gold_items,\n",
    "        'pred_items': pred_items,\n",
    "        'matches': matches,\n",
    "        'unmatched_gold': unmatched_gold,\n",
    "        'unmatched_pred': unmatched_pred,\n",
    "        'page_name': gold_path.name\n",
    "    }\n",
    "\n",
    "print(\"Item matching functions loaded\")\n",
    "print(f\"Similarity threshold: {EVALUATION_CONFIG.similarity_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2281e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
