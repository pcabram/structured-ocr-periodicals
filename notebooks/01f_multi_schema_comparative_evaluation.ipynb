{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e368f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Multi-Schema Evaluation\n",
      "============================================================\n",
      "Project root: /home/fabian-ramirez/Documents/These/Code/magazine_graphs\n",
      "\n",
      "Directories:\n",
      "  Gold standard: /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/gold_standard/cleaned\n",
      "  Predictions:   /home/fabian-ramirez/Documents/These/Code/magazine_graphs/data/predictions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1 Multi-Schema Evaluation\n",
    "Compares multiple schema variants to identify the best performing schema.\n",
    "\n",
    "Schema Families:\n",
    "- WITH continuations: Has is_continuation + continues_on_next_page fields\n",
    "- WITHOUT continuations: Simpler structure without cross-page tracking\n",
    "\n",
    "Evaluation Strategy:\n",
    "1. Auto-detect schema families based on field presence\n",
    "2. Within-family comparison: Find best schema in each family\n",
    "3. Cross-family comparison: Compare winners on common dimensions only\n",
    "4. Final recommendation: Which schema to use and why\n",
    "\n",
    "Input:  data/predictions/{schema_version}/{magazine_name}/\n",
    "        data/gold_standard/cleaned/{magazine_name}/\n",
    "Output: Comparative metrics and recommendation\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "import inspect\n",
    "\n",
    "# Project imports\n",
    "from utils.paths import PROJECT_ROOT, PREDICTIONS, GOLD_CLEAN\n",
    "from utils.config import EVALUATION_CONFIG\n",
    "from schemas.stage1_page import Stage1PageModel\n",
    "from utils.text_processing import (\n",
    "    normalize_text_strict,\n",
    "    normalize_text_standard,\n",
    "    normalize_text_letters_only,\n",
    "    token_sort_text\n",
    ")\n",
    "from utils.ocr_metrics import character_error_rate, word_error_rate\n",
    "from utils.evaluation import (\n",
    "    match_items,\n",
    "    load_and_match_page,\n",
    "    filter_matches_by_class,\n",
    "    get_matched_pairs,\n",
    "    evaluate_order_agnostic,\n",
    "    evaluate_structure_aware,\n",
    "    evaluate_classification,\n",
    "    evaluate_metadata_field,\n",
    "    evaluate_continuation_all_items\n",
    ")\n",
    "\n",
    "# Paths\n",
    "GOLD_ROOT = GOLD_CLEAN\n",
    "PRED_ROOT = PREDICTIONS\n",
    "\n",
    "print(\"Stage 1 Multi-Schema Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"\\nDirectories:\")\n",
    "print(f\"  Gold standard: {GOLD_ROOT}\")\n",
    "print(f\"  Predictions:   {PRED_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c9f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Detecting Schema Families (from schema definitions)\n",
      "============================================================\n",
      "\n",
      "Found 7 schema version(s):\n",
      "\n",
      "  - stage1_page\n",
      "  - stage1_page_v2\n",
      "  - stage1_page_v2_medium\n",
      "  - stage1_page_v2_medium_pure\n",
      "  - stage1_page_v2_pure\n",
      "  - stage1_page_v2_small\n",
      "  - stage1_page_v2_small_pure\n",
      "\n",
      "Checking schema definitions in /home/fabian-ramirez/Documents/These/Code/magazine_graphs/schemas/...\n",
      "\n",
      "  stage1_page                    -> with_continuations\n",
      "  stage1_page_v2                 -> with_continuations\n",
      "  stage1_page_v2_medium          -> with_continuations\n",
      "  stage1_page_v2_medium_pure     -> without_continuations\n",
      "  stage1_page_v2_pure            -> without_continuations\n",
      "  stage1_page_v2_small           -> with_continuations\n",
      "  stage1_page_v2_small_pure      -> without_continuations\n",
      "\n",
      "------------------------------------------------------------\n",
      "Family Summary:\n",
      "  WITH continuations:    4 schema(s)\n",
      "    - stage1_page\n",
      "    - stage1_page_v2\n",
      "    - stage1_page_v2_medium\n",
      "    - stage1_page_v2_small\n",
      "\n",
      "  WITHOUT continuations: 3 schema(s)\n",
      "    - stage1_page_v2_medium_pure\n",
      "    - stage1_page_v2_pure\n",
      "    - stage1_page_v2_small_pure\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schema Family Detection\n",
    "Determine schema families by inspecting schema definition files in schemas/ folder.\n",
    "This is the source of truth - not the prediction outputs.\n",
    "\"\"\"\n",
    "\n",
    "def detect_schema_family_from_definition(schema_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Detect schema family by checking if continuation fields are defined in the schema class.\n",
    "    \n",
    "    Args:\n",
    "        schema_name: Name of schema directory (e.g., 'stage1_page', 'stage1_page_v2')\n",
    "        \n",
    "    Returns:\n",
    "        'with_continuations' or 'without_continuations' or None if cannot determine\n",
    "    \"\"\"\n",
    "    # Map schema directory name to schema file\n",
    "    # Convention: stage1_page -> stage1_page.py, stage1_page_v2 -> stage1_page_v2.py\n",
    "    schema_file = PROJECT_ROOT / 'schemas' / f'{schema_name}.py'\n",
    "    \n",
    "    if not schema_file.exists():\n",
    "        print(f\"  WARNING: Schema file not found: {schema_file}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load the schema module\n",
    "        spec = importlib.util.spec_from_file_location(schema_name, schema_file)\n",
    "        if spec is None or spec.loader is None:\n",
    "            return None\n",
    "            \n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        \n",
    "        # Find the Item class (Stage1Item or similar)\n",
    "        item_class = None\n",
    "        for name, obj in inspect.getmembers(module):\n",
    "            if inspect.isclass(obj) and 'Item' in name and name != 'BaseModel':\n",
    "                item_class = obj\n",
    "                break\n",
    "        \n",
    "        if item_class is None:\n",
    "            print(f\"  WARNING: Could not find Item class in {schema_file.name}\")\n",
    "            return None\n",
    "        \n",
    "        # Check if continuation fields are defined in the model\n",
    "        model_fields = item_class.model_fields if hasattr(item_class, 'model_fields') else {}\n",
    "        \n",
    "        has_is_continuation = 'is_continuation' in model_fields\n",
    "        has_continues = 'continues_on_next_page' in model_fields\n",
    "        \n",
    "        if has_is_continuation and has_continues:\n",
    "            return 'with_continuations'\n",
    "        elif not has_is_continuation and not has_continues:\n",
    "            return 'without_continuations'\n",
    "        else:\n",
    "            # Has one but not the other - unusual\n",
    "            print(f\"  WARNING: {schema_name} has only one continuation field\")\n",
    "            return 'with_continuations' if (has_is_continuation or has_continues) else 'without_continuations'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading schema {schema_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Schema versions are in predictions/schema_evaluations/\n",
    "SCHEMA_ROOT = PRED_ROOT / 'schema_evaluations'\n",
    "\n",
    "if not SCHEMA_ROOT.exists():\n",
    "    print(f\"ERROR: Schema evaluations directory not found at {SCHEMA_ROOT}\")\n",
    "    print(\"Expected structure: predictions/schema_evaluations/{{schema_version}}/{{magazine_name}}/\")\n",
    "    available_schemas = []\n",
    "else:\n",
    "    # Find all schema directories\n",
    "    available_schemas = sorted([d for d in SCHEMA_ROOT.iterdir() if d.is_dir()])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Detecting Schema Families (from schema definitions)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if not available_schemas:\n",
    "    print(\"ERROR: No schema directories found in predictions/schema_evaluations/\")\n",
    "    print(\"Expected structure: predictions/schema_evaluations/{{schema_version}}/{{magazine_name}}/\")\n",
    "else:\n",
    "    print(f\"Found {len(available_schemas)} schema version(s):\\n\")\n",
    "    for schema_dir in available_schemas:\n",
    "        print(f\"  - {schema_dir.name}\")\n",
    "    \n",
    "    print(f\"\\nChecking schema definitions in {PROJECT_ROOT / 'schemas'}/...\\n\")\n",
    "    \n",
    "    # Group schemas by family\n",
    "    families = {\n",
    "        'with_continuations': [],\n",
    "        'without_continuations': []\n",
    "    }\n",
    "    \n",
    "    for schema_dir in available_schemas:\n",
    "        schema_name = schema_dir.name\n",
    "        family = detect_schema_family_from_definition(schema_name)\n",
    "        \n",
    "        if family:\n",
    "            families[family].append(schema_name)\n",
    "            print(f\"  {schema_name:<30} -> {family}\")\n",
    "        else:\n",
    "            print(f\"  {schema_name:<30} -> UNKNOWN (could not detect from schema file)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Family Summary:\")\n",
    "    print(f\"  WITH continuations:    {len(families['with_continuations'])} schema(s)\")\n",
    "    for schema in families['with_continuations']:\n",
    "        print(f\"    - {schema}\")\n",
    "    print(f\"\\n  WITHOUT continuations: {len(families['without_continuations'])} schema(s)\")\n",
    "    for schema in families['without_continuations']:\n",
    "        print(f\"    - {schema}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab6ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Finding Magazine Pairs for Each Schema\n",
      "============================================================\n",
      "\n",
      "stage1_page:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2_medium:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2_medium_pure:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2_pure:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2_small:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "stage1_page_v2_small_pure:\n",
      "  La_Plume_bpt6k1185893k_1_10_1889: 14 matching pages\n",
      "  La_Plume_bpt6k1212187t_15-11-1893: 34 matching pages\n",
      "\n",
      "VERIFICATION: All schemas have identical test sets\n",
      "Common magazines (2): La_Plume_bpt6k1185893k_1_10_1889, La_Plume_bpt6k1212187t_15-11-1893\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find Magazine Pairs for Each Schema\n",
    "For each schema version, find magazines with matching gold standard data.\n",
    "Ensures fair comparison by verifying all schemas have same test set.\n",
    "\"\"\"\n",
    "\n",
    "def find_magazine_pairs_for_schema(schema_name: str) -> List[Tuple[str, Path, Path, int]]:\n",
    "    \"\"\"\n",
    "    Find magazines with both gold standard and predictions for a given schema.\n",
    "    \n",
    "    Args:\n",
    "        schema_name: Name of schema directory\n",
    "        \n",
    "    Returns:\n",
    "        List of (magazine_name, gold_dir, pred_dir, num_matching_files) tuples\n",
    "    \"\"\"\n",
    "    schema_path = SCHEMA_ROOT / schema_name\n",
    "    \n",
    "    # Get all gold standard magazines\n",
    "    gold_magazines = {d.name: d for d in GOLD_ROOT.iterdir() if d.is_dir()}\n",
    "    \n",
    "    # Get all prediction magazines for this schema\n",
    "    pred_magazines = {d.name: d for d in schema_path.iterdir() if d.is_dir()}\n",
    "    \n",
    "    # Find magazines that exist in both\n",
    "    common_magazines = set(gold_magazines.keys()) & set(pred_magazines.keys())\n",
    "    \n",
    "    pairs = []\n",
    "    for mag_name in sorted(common_magazines):\n",
    "        gold_dir = gold_magazines[mag_name]\n",
    "        pred_dir = pred_magazines[mag_name]\n",
    "        \n",
    "        # Find matching page files (same filename in both directories)\n",
    "        gold_files = {f.name for f in gold_dir.glob(\"*.json\")}\n",
    "        pred_files = {f.name for f in pred_dir.glob(\"*.json\")}\n",
    "        \n",
    "        matching_files = gold_files & pred_files\n",
    "        \n",
    "        if matching_files:\n",
    "            pairs.append((mag_name, gold_dir, pred_dir, len(matching_files)))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Find pairs for all schemas\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Finding Magazine Pairs for Each Schema\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "schema_magazine_pairs = {}\n",
    "\n",
    "for schema_dir in available_schemas:\n",
    "    schema_name = schema_dir.name\n",
    "    pairs = find_magazine_pairs_for_schema(schema_name)\n",
    "    schema_magazine_pairs[schema_name] = pairs\n",
    "    \n",
    "    print(f\"{schema_name}:\")\n",
    "    if not pairs:\n",
    "        print(\"  No matching magazines found\")\n",
    "    else:\n",
    "        for mag_name, gold_dir, pred_dir, num_files in pairs:\n",
    "            print(f\"  {mag_name}: {num_files} matching pages\")\n",
    "    print()\n",
    "\n",
    "# Verify all schemas have same test set (CRITICAL for fair comparison)\n",
    "all_magazine_sets = [\n",
    "    set(mag_name for mag_name, _, _, _ in pairs)\n",
    "    for pairs in schema_magazine_pairs.values()\n",
    "]\n",
    "\n",
    "if len(set(map(frozenset, all_magazine_sets))) == 1:\n",
    "    print(\"VERIFICATION: All schemas have identical test sets\")\n",
    "    common_magazines = all_magazine_sets[0]\n",
    "    print(f\"Common magazines ({len(common_magazines)}): {', '.join(sorted(common_magazines))}\")\n",
    "else:\n",
    "    print(\"WARNING: Schemas have different test sets - comparison may not be fair\")\n",
    "    for schema_name, pairs in schema_magazine_pairs.items():\n",
    "        mags = set(mag_name for mag_name, _, _, _ in pairs)\n",
    "        print(f\"  {schema_name}: {mags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014b154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions loaded from utils/evaluation.py\n",
      "Similarity threshold: 0.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation functions imported from utils\n",
    "All evaluation logic is in utils/evaluation.py\n",
    "\"\"\"\n",
    "print(\"Evaluation functions loaded from utils/evaluation.py\")\n",
    "print(f\"Similarity threshold: {EVALUATION_CONFIG.similarity_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c2281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full schema evaluation pipeline ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full Schema Evaluation Pipeline\n",
    "Evaluates a single schema across all its magazine pairs.\n",
    "Runs the complete 01c evaluation (all 5 dimensions) and aggregates results.\n",
    "\"\"\"\n",
    "def evaluate_schema_full(\n",
    "    schema_name: str,\n",
    "    magazine_pairs: List[Tuple[str, Path, Path, int]],\n",
    "    schema_family: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a schema across all its magazine pairs.\n",
    "    \n",
    "    Args:\n",
    "        schema_name: Name of schema to evaluate\n",
    "        magazine_pairs: List of (mag_name, gold_dir, pred_dir, n_files) tuples\n",
    "        schema_family: 'with_continuations' or 'without_continuations'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with aggregated metrics across all pages\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {schema_name}...\")\n",
    "    print(f\"Family: {schema_family}\")\n",
    "    print(f\"Magazines: {len(magazine_pairs)}\")\n",
    "    \n",
    "    # Accumulators for aggregation\n",
    "    all_gold_items = []\n",
    "    all_pred_items = []\n",
    "    all_matches = []\n",
    "    all_unmatched_gold = set()\n",
    "    all_unmatched_pred = set()\n",
    "    total_pages = 0\n",
    "    \n",
    "    # Track item offsets for continuation evaluation\n",
    "    gold_offset = 0\n",
    "    pred_offset = 0\n",
    "    \n",
    "    # Process all pages across all magazines\n",
    "    for mag_name, gold_dir, pred_dir, n_files in magazine_pairs:\n",
    "        gold_files = sorted(gold_dir.glob(\"*.json\"))\n",
    "        \n",
    "        for gold_file in gold_files:\n",
    "            pred_file = pred_dir / gold_file.name\n",
    "            if not pred_file.exists():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load and match page\n",
    "                page_data = load_and_match_page(gold_file, pred_file)\n",
    "                \n",
    "                gold_items = page_data['gold_items']\n",
    "                pred_items = page_data['pred_items']\n",
    "                matches = page_data['matches']\n",
    "                unmatched_gold = page_data['unmatched_gold']\n",
    "                unmatched_pred = page_data['unmatched_pred']\n",
    "                \n",
    "                # Accumulate with offset adjustment for continuation evaluation\n",
    "                all_gold_items.extend(gold_items)\n",
    "                all_pred_items.extend(pred_items)\n",
    "                \n",
    "                # Adjust match indices to account for accumulated items\n",
    "                adjusted_matches = [\n",
    "                    (g_idx + gold_offset, p_idx + pred_offset, score)\n",
    "                    for g_idx, p_idx, score in matches\n",
    "                ]\n",
    "                all_matches.extend(adjusted_matches)\n",
    "                \n",
    "                # Adjust unmatched indices\n",
    "                all_unmatched_gold.update(idx + gold_offset for idx in unmatched_gold)\n",
    "                all_unmatched_pred.update(idx + pred_offset for idx in unmatched_pred)\n",
    "                \n",
    "                # Update offsets\n",
    "                gold_offset += len(gold_items)\n",
    "                pred_offset += len(pred_items)\n",
    "                \n",
    "                total_pages += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR processing {gold_file.name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    print(f\"  Processed {total_pages} pages\")\n",
    "    print(f\"  Total gold items: {len(all_gold_items)}\")\n",
    "    print(f\"  Total pred items: {len(all_pred_items)}\")\n",
    "    print(f\"  Total matches: {len(all_matches)}\")\n",
    "    \n",
    "    # 1. Structure Detection\n",
    "    total_gold = len(all_gold_items)\n",
    "    total_pred = len(all_pred_items)\n",
    "    total_matched = len(all_matches)\n",
    "    \n",
    "    precision = total_matched / total_pred if total_pred > 0 else 0.0\n",
    "    recall = total_matched / total_gold if total_gold > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    structure_metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'match_rate': recall,  # match_rate is same as recall\n",
    "        'total_gold_items': total_gold,\n",
    "        'total_pred_items': total_pred,\n",
    "        'total_matched': total_matched\n",
    "    }\n",
    "    \n",
    "    # 2. Text Quality (using structure-aware evaluation)\n",
    "    text_metrics = evaluate_structure_aware(all_gold_items, all_pred_items, all_matches)\n",
    "    \n",
    "    # 3. Classification\n",
    "    classification_metrics = evaluate_classification(all_gold_items, all_pred_items, all_matches)\n",
    "    \n",
    "    # 4. Metadata - evaluate across all matched items\n",
    "    metadata_metrics = {}\n",
    "    for field_name in ['item_title', 'item_author']:\n",
    "        field_results = evaluate_metadata_field(\n",
    "            all_gold_items, \n",
    "            all_pred_items, \n",
    "            all_matches, \n",
    "            field_name\n",
    "        )\n",
    "        # Store with simplified key name\n",
    "        simple_name = field_name.replace('item_', '')\n",
    "        metadata_metrics[simple_name] = field_results\n",
    "    \n",
    "    # 5. Continuation (only if applicable)\n",
    "    continuation_metrics = None\n",
    "    if schema_family == 'with_continuations':\n",
    "        cont_results = evaluate_continuation_all_items(\n",
    "            all_gold_items,\n",
    "            all_pred_items,\n",
    "            all_matches,\n",
    "            all_unmatched_gold,\n",
    "            all_unmatched_pred\n",
    "        )\n",
    "        \n",
    "        # Extract combined metrics\n",
    "        is_cont_f1 = cont_results['is_continuation']['f1']\n",
    "        continues_f1 = cont_results['continues_on_next_page']['f1']\n",
    "        combined_f1 = (is_cont_f1 + continues_f1) / 2\n",
    "        \n",
    "        continuation_metrics = {\n",
    "            'is_continuation_f1': is_cont_f1,\n",
    "            'continues_f1': continues_f1,\n",
    "            'combined_f1': combined_f1,\n",
    "            'is_continuation': cont_results['is_continuation'],\n",
    "            'continues_on_next_page': cont_results['continues_on_next_page']\n",
    "        }\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'schema_name': schema_name,\n",
    "        'schema_family': schema_family,\n",
    "        'n_pages': total_pages,\n",
    "        'structure_detection': structure_metrics,\n",
    "        'text_quality': {\n",
    "            'structure_aware': {\n",
    "                'all_items': text_metrics\n",
    "            }\n",
    "        },\n",
    "        'classification': {\n",
    "            'overall': classification_metrics\n",
    "        },\n",
    "        'metadata_extraction': metadata_metrics\n",
    "    }\n",
    "    \n",
    "    if continuation_metrics is not None:\n",
    "        results['continuation'] = continuation_metrics\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Full schema evaluation pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c69e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running Full Evaluation for All Schemas\n",
      "============================================================\n",
      "\n",
      "Evaluating stage1_page...\n",
      "Family: with_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 178\n",
      "  Total matches: 99\n",
      "\n",
      "  Results for stage1_page:\n",
      "    Structure Match Rate: 0.460\n",
      "    Text CER (SA all): 0.048\n",
      "    Classification Acc: 0.949\n",
      "    Continuation F1: 0.151\n",
      "\n",
      "Evaluating stage1_page_v2...\n",
      "Family: with_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 221\n",
      "  Total matches: 135\n",
      "\n",
      "  Results for stage1_page_v2:\n",
      "    Structure Match Rate: 0.628\n",
      "    Text CER (SA all): 0.057\n",
      "    Classification Acc: 0.881\n",
      "    Continuation F1: 0.143\n",
      "\n",
      "Evaluating stage1_page_v2_medium...\n",
      "Family: with_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 207\n",
      "  Total matches: 131\n",
      "\n",
      "  Results for stage1_page_v2_medium:\n",
      "    Structure Match Rate: 0.609\n",
      "    Text CER (SA all): 0.094\n",
      "    Classification Acc: 0.847\n",
      "    Continuation F1: 0.146\n",
      "\n",
      "Evaluating stage1_page_v2_medium_pure...\n",
      "Family: without_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 184\n",
      "  Total matches: 124\n",
      "\n",
      "  Results for stage1_page_v2_medium_pure:\n",
      "    Structure Match Rate: 0.577\n",
      "    Text CER (SA all): 0.075\n",
      "    Classification Acc: 0.855\n",
      "\n",
      "Evaluating stage1_page_v2_pure...\n",
      "Family: without_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 258\n",
      "  Total matches: 161\n",
      "\n",
      "  Results for stage1_page_v2_pure:\n",
      "    Structure Match Rate: 0.749\n",
      "    Text CER (SA all): 0.087\n",
      "    Classification Acc: 0.944\n",
      "\n",
      "Evaluating stage1_page_v2_small...\n",
      "Family: with_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 259\n",
      "  Total matches: 79\n",
      "\n",
      "  Results for stage1_page_v2_small:\n",
      "    Structure Match Rate: 0.367\n",
      "    Text CER (SA all): 0.108\n",
      "    Classification Acc: 0.772\n",
      "    Continuation F1: 0.043\n",
      "\n",
      "Evaluating stage1_page_v2_small_pure...\n",
      "Family: without_continuations\n",
      "Magazines: 2\n",
      "  Processed 48 pages\n",
      "  Total gold items: 215\n",
      "  Total pred items: 280\n",
      "  Total matches: 80\n",
      "\n",
      "  Results for stage1_page_v2_small_pure:\n",
      "    Structure Match Rate: 0.372\n",
      "    Text CER (SA all): 0.103\n",
      "    Classification Acc: 0.750\n",
      "\n",
      "============================================================\n",
      "Evaluation complete. Processed 7 schema(s)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute Full Evaluation for All Schemas\n",
    "Run the comprehensive evaluation pipeline for each schema and collect results.\n",
    "\"\"\"\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Running Full Evaluation for All Schemas\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for schema_dir in available_schemas:\n",
    "    schema_name = schema_dir.name\n",
    "    \n",
    "    # Get schema family\n",
    "    family = detect_schema_family_from_definition(schema_name)\n",
    "    if family is None:\n",
    "        print(f\"\\nSkipping {schema_name} - could not determine family\")\n",
    "        continue\n",
    "    \n",
    "    # Get magazine pairs\n",
    "    magazine_pairs = schema_magazine_pairs.get(schema_name, [])\n",
    "    if not magazine_pairs:\n",
    "        print(f\"\\nSkipping {schema_name} - no magazine pairs found\")\n",
    "        continue\n",
    "    \n",
    "    # Run evaluation\n",
    "    try:\n",
    "        results = evaluate_schema_full(schema_name, magazine_pairs, family)\n",
    "\n",
    "        # Validate results structure\n",
    "        required_keys = ['schema_name', 'schema_family', 'n_pages', 'structure_detection', \n",
    "                        'text_quality', 'classification', 'metadata_extraction']\n",
    "        missing_keys = [k for k in required_keys if k not in results]\n",
    "        if missing_keys:\n",
    "            print(f\"  WARNING: Results missing keys: {missing_keys}\")\n",
    "            continue\n",
    "\n",
    "        all_results[schema_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n  Results for {schema_name}:\")\n",
    "        print(f\"    Structure Match Rate: {results['structure_detection']['match_rate']:.3f}\")\n",
    "        print(f\"    Text CER (SA all): {results['text_quality']['structure_aware']['all_items']['cer_standard']:.3f}\")\n",
    "        print(f\"    Classification Acc: {results['classification']['overall']['accuracy']:.3f}\")\n",
    "        \n",
    "        if 'continuation' in results:\n",
    "            print(f\"    Continuation F1: {results['continuation']['combined_f1']:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ERROR evaluating {schema_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Evaluation complete. Processed {len(all_results)} schema(s)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0b4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Family: with_continuations\n",
      "==========================================================================================\n",
      "Schema                         Match Rate   CER        Class Acc    Meta F1   \n",
      "------------------------------------------------------------------------------------------\n",
      "stage1_page                    0.460        0.048      0.949        0.727     \n",
      "  └─ Continuation F1           0.151\n",
      "stage1_page_v2                 0.628        0.057      0.881        0.634     \n",
      "  └─ Continuation F1           0.143\n",
      "stage1_page_v2_medium          0.609        0.094      0.847        0.708     \n",
      "  └─ Continuation F1           0.146\n",
      "stage1_page_v2_small           0.367        0.108      0.772        0.675     \n",
      "  └─ Continuation F1           0.043\n",
      "\n",
      "Family: without_continuations\n",
      "==========================================================================================\n",
      "Schema                         Match Rate   CER        Class Acc    Meta F1   \n",
      "------------------------------------------------------------------------------------------\n",
      "stage1_page_v2_medium_pure     0.577        0.075      0.855        0.752     \n",
      "stage1_page_v2_pure            0.749        0.087      0.944        0.648     \n",
      "stage1_page_v2_small_pure      0.372        0.103      0.750        0.597     \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Within-Family Evaluation\n",
    "Compare schemas within each family on all metrics.\n",
    "\"\"\"\n",
    "# Group results by family\n",
    "family_results = defaultdict(list)\n",
    "for schema_name, results in all_results.items():\n",
    "    family = results['schema_family']\n",
    "    family_results[family].append((schema_name, results))\n",
    "\n",
    "# Compare schemas within each family\n",
    "for family, schemas in family_results.items():\n",
    "    print(f\"\\nFamily: {family}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Sort schemas alphabetically for consistent display\n",
    "    schemas_sorted = sorted(schemas, key=lambda x: x[0])\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Schema':<30} {'Match Rate':<12} {'CER':<10} {'Class Acc':<12} {'Meta F1':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Print each schema\n",
    "    for schema_name, results in schemas_sorted:\n",
    "        match_rate = results['structure_detection']['match_rate']\n",
    "        cer = results['text_quality']['structure_aware']['all_items']['cer_standard']\n",
    "        acc = results['classification']['overall']['accuracy']\n",
    "        \n",
    "        # Calculate average metadata F1\n",
    "        metadata_scores = [field['f1'] for field in results['metadata_extraction'].values()]\n",
    "        meta_f1 = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0.0\n",
    "        \n",
    "        print(f\"{schema_name:<30} {match_rate:<12.3f} {cer:<10.3f} {acc:<12.3f} {meta_f1:<10.3f}\")\n",
    "        \n",
    "        # Show continuation metrics if available\n",
    "        if 'continuation' in results:\n",
    "            cont_f1 = results['continuation']['combined_f1']\n",
    "            print(f\"{'  └─ Continuation F1':<30} {cont_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89dc3cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  with_continuations: stage1_page (score: 0.772)\n",
      "  without_continuations: stage1_page_v2_pure (score: 0.813)\n",
      "\n",
      "Cross-Family Comparison\n",
      "================================================================================\n",
      "Dimension                 with_continuations   without_continuations\n",
      "--------------------------------------------------------------------------------\n",
      "structure_detection       0.460                  0.749 *                \n",
      "text_quality              0.952 *                0.913                  \n",
      "classification            0.949 *                0.944                  \n",
      "metadata                  0.727 *                0.648                  \n",
      "\n",
      "* = Best in dimension\n",
      "\n",
      "Summary:\n",
      "  with_continuations: 3/4 dimensions won\n",
      "  without_continuations: 1/4 dimensions won\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cross-Family Comparison\n",
    "Compare best-performing schema from each family on common dimensions.\n",
    "\"\"\"\n",
    "\n",
    "if len(family_results) < 2:\n",
    "    print(\"\\nCross-Family Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Only one family present - cross-family comparison not applicable\")\n",
    "else:\n",
    "    family_representatives = {}\n",
    "    for family, schemas in family_results.items():\n",
    "        # Calculate composite score\n",
    "        best_schema = None\n",
    "        best_score = -1.0\n",
    "\n",
    "        for schema_name, results in schemas:\n",
    "            # Composite score: average of match rate, (1 - CER), classification accuracy, metadata F1\n",
    "            # Extract 4 core dimensions\n",
    "            match_rate = results['structure_detection']['match_rate']\n",
    "            text_quality = 1 - results['text_quality']['structure_aware']['all_items']['cer_standard']\n",
    "            class_acc = results['classification']['overall']['accuracy']\n",
    "\n",
    "            # Calculate metadata F1\n",
    "            metadata_scores = [field['f1'] for field in results['metadata_extraction'].values()]\n",
    "            meta_f1 = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0.0\n",
    "            \n",
    "            composite = (match_rate + text_quality + class_acc + meta_f1) / 4\n",
    "            \n",
    "            if composite > best_score:\n",
    "                best_score = composite\n",
    "                best_schema = (schema_name, results)\n",
    "        family_representatives[family] = best_schema\n",
    "        print(f\"  {family}: {best_schema[0]} (score: {best_score:.3f})\")\n",
    "\n",
    "    # Define common dimensions for comparison\n",
    "    common_dimensions = ['structure_detection', 'text_quality', 'classification', 'metadata']\n",
    "\n",
    "    # Compare family representatives on each dimension\n",
    "    print(\"\\nCross-Family Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Dimension':<25} {' '.join(f'{fam:<20}' for fam in family_representatives.keys())}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    dimension_scores = defaultdict(dict)\n",
    "\n",
    "    # FIRST: Collect all scores\n",
    "    for dimension in common_dimensions:\n",
    "        for family, (schema_name, results) in family_representatives.items():\n",
    "            # Extract the relevant metric for the dimension\n",
    "            if dimension == 'structure_detection':\n",
    "                score = results[dimension]['match_rate']\n",
    "            elif dimension == 'text_quality':\n",
    "                score = 1 - results[dimension]['structure_aware']['all_items']['cer_standard']\n",
    "            elif dimension == 'classification':\n",
    "                score = results[dimension]['overall']['accuracy']\n",
    "            elif dimension == 'metadata':\n",
    "                metadata_scores = [field['f1'] for field in results['metadata_extraction'].values()]\n",
    "                score = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0.0\n",
    "            \n",
    "            dimension_scores[dimension][family] = score\n",
    "\n",
    "    # THEN: Print with correct asterisks\n",
    "    for dimension in common_dimensions:\n",
    "        print(f\"{dimension:<25}\", end=\" \")\n",
    "        \n",
    "        best_score_in_dim = max(dimension_scores[dimension].values())\n",
    "        \n",
    "        for family in family_representatives.keys():\n",
    "            score = dimension_scores[dimension][family]\n",
    "            is_best = (score == best_score_in_dim)\n",
    "            marker = \" *\" if is_best else \"  \"\n",
    "            print(f\"{score:.3f}{marker:<17}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "    print(\"\\n* = Best in dimension\")\n",
    "    print(\"\\nSummary:\")\n",
    "    for family in family_representatives.keys():\n",
    "        wins = sum(1 for dim_scores in dimension_scores.values() \n",
    "                   if dim_scores[family] == max(dim_scores.values()))\n",
    "        print(f\"  {family}: {wins}/{len(common_dimensions)} dimensions won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8336756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive Comparison Table\n",
      "====================================================================================================\n",
      "               Family                     Schema  Match Rate      CER      WER  Class Acc  Meta F1  Cont F1\n",
      "   with_continuations                stage1_page    0.460465 0.048353 0.107027   0.949495 0.727358 0.150943\n",
      "   with_continuations             stage1_page_v2    0.627907 0.057294 0.111283   0.881481 0.634259 0.142857\n",
      "   with_continuations      stage1_page_v2_medium    0.609302 0.094469 0.146861   0.847328 0.708333 0.146075\n",
      "   with_continuations       stage1_page_v2_small    0.367442 0.107978 0.161254   0.772152 0.674797 0.042553\n",
      "without_continuations stage1_page_v2_medium_pure    0.576744 0.075431 0.134812   0.854839 0.751938      NaN\n",
      "without_continuations        stage1_page_v2_pure    0.748837 0.086966 0.132047   0.944099 0.647692      NaN\n",
      "without_continuations  stage1_page_v2_small_pure    0.372093 0.103073 0.166978   0.750000 0.597297      NaN\n",
      "\n",
      "Notes:\n",
      "  - Lower CER/WER is better (error rates)\n",
      "  - Higher Match Rate/Accuracy/F1 is better\n",
      "  - Cont F1 only applicable to schemas with continuation tracking\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Detailed Comparison Table\n",
    "Comprehensive summary table showing all metrics for all schemas across both families.\n",
    "\"\"\"\n",
    "# Prepare data for the table\n",
    "comparison_data = []\n",
    "for family, schemas in family_results.items():\n",
    "    for schema_name, results in schemas:\n",
    "        # Extract continuation F1 if available\n",
    "        continuation_f1 = None\n",
    "        if 'continuation' in results:\n",
    "            continuation_f1 = results['continuation']['combined_f1']\n",
    "        \n",
    "        # Calculate average metadata F1\n",
    "        metadata_scores = [field['f1'] for field in results['metadata_extraction'].values()]\n",
    "        meta_f1 = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0.0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Family': family,\n",
    "            'Schema': schema_name,\n",
    "            'Match Rate': results['structure_detection']['match_rate'],\n",
    "            'CER': results['text_quality']['structure_aware']['all_items']['cer_standard'],\n",
    "            'WER': results['text_quality']['structure_aware']['all_items']['wer_standard'],\n",
    "            'Class Acc': results['classification']['overall']['accuracy'],\n",
    "            'Meta F1': meta_f1,\n",
    "            'Cont F1': continuation_f1\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sort by family and schema name for consistent display\n",
    "df_comparison = df_comparison.sort_values(by=['Family', 'Schema'])\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nComprehensive Comparison Table\")\n",
    "print(\"=\" * 100)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"\\nNotes:\")\n",
    "print(\"  - Lower CER/WER is better (error rates)\")\n",
    "print(\"  - Higher Match Rate/Accuracy/F1 is better\")\n",
    "print(\"  - Cont F1 only applicable to schemas with continuation tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473365b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERALL WINNER\n",
      "================================================================================\n",
      "\n",
      " WINNER: stage1_page_v2_pure\n",
      "   Family: without_continuations\n",
      "   Composite Score: 0.813\n",
      "\n",
      "Key Metrics:\n",
      "  Match Rate:      0.749\n",
      "  CER:             0.087\n",
      "  Classification:  0.944\n",
      "  Metadata F1:     0.648\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Final Recommendation: Overall Winner\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL WINNER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find best schema across ALL\n",
    "best_overall = None\n",
    "best_composite = -1\n",
    "\n",
    "for schema_name, results in all_results.items():\n",
    "    match_rate = results['structure_detection']['match_rate']\n",
    "    text_quality = 1 - results['text_quality']['structure_aware']['all_items']['cer_standard']\n",
    "    class_acc = results['classification']['overall']['accuracy']\n",
    "    \n",
    "    metadata_scores = [field['f1'] for field in results['metadata_extraction'].values()]\n",
    "    meta_f1 = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0.0\n",
    "    \n",
    "    composite = (match_rate + text_quality + class_acc + meta_f1) / 4\n",
    "    \n",
    "    if composite > best_composite:\n",
    "        best_composite = composite\n",
    "        best_overall = (schema_name, results)\n",
    "\n",
    "winner_name, winner_results = best_overall\n",
    "\n",
    "print(f\"\\n WINNER: {winner_name}\")\n",
    "print(f\"   Family: {winner_results['schema_family']}\")\n",
    "print(f\"   Composite Score: {best_composite:.3f}\\n\")\n",
    "print(\"Key Metrics:\")\n",
    "print(f\"  Match Rate:      {winner_results['structure_detection']['match_rate']:.3f}\")\n",
    "print(f\"  CER:             {winner_results['text_quality']['structure_aware']['all_items']['cer_standard']:.3f}\")\n",
    "print(f\"  Classification:  {winner_results['classification']['overall']['accuracy']:.3f}\")\n",
    "\n",
    "metadata_scores = [field['f1'] for field in winner_results['metadata_extraction'].values()]\n",
    "meta_f1 = sum(metadata_scores) / len(metadata_scores)\n",
    "print(f\"  Metadata F1:     {meta_f1:.3f}\")\n",
    "\n",
    "if 'continuation' in winner_results:\n",
    "    print(f\"  Continuation F1: {winner_results['continuation']['combined_f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968362fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPLETE RANKINGS (by composite score)\n",
      "====================================================================================================\n",
      "Rank   Schema                         Score    Match    CER      Class    Meta     Family\n",
      "====================================================================================================\n",
      "1      stage1_page_v2_pure            0.813    0.749  0.087  0.944  0.648  no cont WINNER\n",
      "2      stage1_page_v2_medium_pure     0.777    0.577  0.075  0.855  0.752  no cont\n",
      "3      stage1_page                    0.772    0.460  0.048  0.949  0.727  w/cont\n",
      "4      stage1_page_v2                 0.772    0.628  0.057  0.881  0.634  w/cont\n",
      "5      stage1_page_v2_medium          0.768    0.609  0.094  0.847  0.708  w/cont\n",
      "6      stage1_page_v2_small           0.677    0.367  0.108  0.772  0.675  w/cont\n",
      "7      stage1_page_v2_small_pure      0.654    0.372  0.103  0.750  0.597  no cont\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Rankings\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPLETE RANKINGS (by composite score)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "rankings = []\n",
    "for schema_name, results in all_results.items():\n",
    "    match_rate = results['structure_detection']['match_rate']\n",
    "    text_quality = 1 - results['text_quality']['structure_aware']['all_items']['cer_standard']\n",
    "    class_acc = results['classification']['overall']['accuracy']\n",
    "    \n",
    "    metadata_scores = [field['f1'] for field in results['metadata_extraction'].values()]\n",
    "    meta_f1 = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0.0\n",
    "    \n",
    "    composite = (match_rate + text_quality + class_acc + meta_f1) / 4\n",
    "    \n",
    "    cer = results['text_quality']['structure_aware']['all_items']['cer_standard']\n",
    "    \n",
    "    rankings.append((\n",
    "        schema_name, \n",
    "        composite, \n",
    "        match_rate, \n",
    "        cer,\n",
    "        class_acc, \n",
    "        meta_f1,\n",
    "        results['schema_family']\n",
    "    ))\n",
    "\n",
    "rankings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Schema':<30} {'Score':<8} {'Match':<8} {'CER':<8} {'Class':<8} {'Meta':<8} {'Family'}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, (name, score, match, cer, cls, meta, family) in enumerate(rankings, 1):\n",
    "    marker = \" WINNER\" if i == 1 else \"\"\n",
    "    family_short = 'w/cont' if family == 'with_continuations' else 'no cont'\n",
    "    print(f\"{i:<6} {name:<30} {score:.3f}    {match:.3f}  {cer:.3f}  {cls:.3f}  {meta:.3f}  {family_short}{marker}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magazine-graphs-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
